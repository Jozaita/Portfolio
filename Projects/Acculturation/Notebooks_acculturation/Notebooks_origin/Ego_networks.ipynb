{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ego networks \n",
    "\n",
    "In this notebook, we will build the ego networks using the two dataframes of clean data obtained in the past notebook _clean.ipynb_. In order to achieve this: \n",
    "\n",
    "* We will use the library _networkx_ to transform the dataframe of relationships between the alteri into the different ego networks. \n",
    "* We will extract some measures of the structure of this ego networks and create with them a dataframe, as our goal is to use them as predictors of the nationality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the .csv files \n",
    "\n",
    "We import the different libraries, the usual numpy, pandas, matplotlib and networkx. The methods library that can be seen contains the algorithms to calculate the Dunbar estructure of an individual given an ego network. We will also incorporate this measure in our analysis in order to provide a more complete view. Then we load our .csv files and delete the unformatted columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Import libraries\n",
    "from numpy import nan\n",
    "import pandas as pd \n",
    "import networkx as nx\n",
    "from methods import *\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "###Load .csv files\n",
    "contactos=pd.read_csv(r'/home/juan/Python/Acculturation/Contactos.csv',low_memory=False)\n",
    "df=pd.read_csv(r'/home/juan/Python/Acculturation/all_data_clean.csv',low_memory=False)\n",
    "\n",
    "###Delete useless columns\n",
    "del contactos['Unnamed: 0']\n",
    "del df['Unnamed: 0']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out the total number of subjects\n",
    "\n",
    "In order to begin the calculations, we calculate the total number of subjets. We also transform the datatype of the columns that are going to play a role in the dessign of the ego networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of subjects is 473\n"
     ]
    }
   ],
   "source": [
    "###Change datatypes\n",
    "contactos['Alter']=contactos['Alter'].astype(int)\n",
    "contactos['Alter2']=contactos['Alter2'].astype(int)\n",
    "\n",
    "#Find out the total number of subjects \n",
    "sujetos = len(contactos['sub/num'].unique())\n",
    "print(\"The total number of subjects is {}\".format(sujetos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the ego networks and measuring their properties \n",
    "\n",
    "At this point, we build the ego networks. Coming from the contacts dataframe, we build a _networkx_ graph and we go through all the subjects, one by one, introducing the ties between alteris as links between nodes. And the intensity of their relationships determines the weight associated with the link. \n",
    "<br> <br>\n",
    "Once we have these graphs, we compute different properties: _Average degree, betweenness, closeness, clustering, load centrality, size of the largest component, number of components.._ Apart from these structural measures, we also compute the average intensity with people from their origin country and people from their residence country and their presence (the total number in the ego network). We store all these values in different lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def include_ego(grafica):\n",
    "    edges_ego = []\n",
    "    for node in grafica.nodes():\n",
    "        edges_ego.append((0,node,{\"Value\":1}))\n",
    "    grafica.add_edges_from(edges_ego)\n",
    "    return grafica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/.local/lib/python3.8/site-packages/networkx/algorithms/assortativity/correlation.py:282: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return (xy * (M - ab)).sum() / np.sqrt(vara * varb)\n"
     ]
    }
   ],
   "source": [
    "###We create the list in order to store the ego networks and their different properties. \n",
    "graficas=[0]*sujetos\n",
    "avdeg=[0]*sujetos\n",
    "betw=[0]*sujetos\n",
    "closs=[0]*sujetos\n",
    "assort=[0]*sujetos\n",
    "clustering=[0]*sujetos\n",
    "load=[0]*sujetos\n",
    "size=[0]*sujetos\n",
    "comp=[0]*sujetos\n",
    "ori=[0]*sujetos\n",
    "num=[0]*sujetos\n",
    "res=[0]*sujetos\n",
    "#closnac=[0]*sujetos\n",
    "#closnonac=[0]*sujetos\n",
    "#numnac=[0]*sujetos\n",
    "#mu=[0]*sujetos\n",
    "#numnonac=[0]*sujetos\n",
    "#vect=[[0]*5 for i in range(sujetos)]\n",
    "\n",
    "###We compute these ego networks and their attributes\n",
    "\n",
    "for k,j in enumerate(contactos[\"sub/num\"].unique()):\n",
    "    graficas[k]=nx.Graph()\n",
    "    ###We select the data that corresponds to each subject\n",
    "    data=contactos[contactos['sub/num']==j]\n",
    "    if (len(data)>0):\n",
    "        ###We compute the different number of subjects and their intensity. \n",
    "        ###This will be used to calculate the Dunbar parameters. \n",
    "        #datamu=df['Clos'][df['sub/num']==j].value_counts()\n",
    "        #for m in range(0,len(datamu)):\n",
    "        #    vect[k][datamu.index[m]-1]= datamu[datamu.index[m]]\n",
    "        ###Building the ego networks\n",
    "        edges=list(zip(data['Alter'],data['Alter2'],data['Value']))\n",
    "        graficas[k].add_weighted_edges_from(edges,'Value')\n",
    "        #graficas[k] = include_ego(graficas[k])\n",
    "        ###Measuring the networks\n",
    "        degrees=[val for (node, val) in graficas[k].degree()]\n",
    "        avdeg[k]=sum(degrees)/len(degrees)\n",
    "        betw[k]=sum(nx.betweenness_centrality(graficas[k],weight='Value').values())/len(nx.betweenness_centrality(graficas[k]).values())\n",
    "        closs[k]=sum(nx.closeness_centrality(graficas[k]).values())/len(nx.closeness_centrality(graficas[k]).values())\n",
    "        load[k]=sum(nx.load_centrality(graficas[k],weight='Value').values())/len(nx.load_centrality(graficas[k]).values())\n",
    "        assort[k]=nx.degree_assortativity_coefficient(graficas[k],weight='Value')\n",
    "        size[k]= len(max(nx.connected_components(graficas[k]), key=len))/len(graficas[k].nodes())\n",
    "        clustering[k]=nx.average_clustering(graficas[k],weight='Value')\n",
    "        \n",
    "        comp[k]=nx.number_connected_components(graficas[k])\n",
    "        ego_origin = df['sub/origin'][df['sub/num'] == j].unique()[0]\n",
    "        ego_residence = df['sub/residence'][df['sub/num'] == j].unique()[0]\n",
    "        #closnac[k]=df['Clos'][(df['sub/num'] == j) & (df['alter/origin'] == ego_origin)].mean() \n",
    "        #numnac[k]=df['Clos'][(df['sub/num'] == j) & (df['alter/origin'] == ego_origin)].count()\n",
    "        #closnonac[k]=df['Clos'][(df['sub/num'] == j) & (df['alter/origin'] == ego_residence)].mean()\n",
    "        #numnonac[k]=df['Clos'][(df['sub/num'] == j) & (df['alter/origin'] == ego_residence)].count()\n",
    "        ori[k]=ego_origin\n",
    "        num[k]=j\n",
    "        res[k]=ego_residence\n",
    "    else: print(j)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the ego networks dataframe \n",
    "\n",
    "Coming from the previous lists, we create the networks dataframe, that contains the ego networks and their attributes. At this point, we also compute the Dunbar's parameter $\\mu$ and add it to our dataframe, with its confidence interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Creation of the dataframe\n",
    "#redes=pd.DataFrame(data={'Subject origin':ori,'Subject residence':res,'Subject ID':num,'Vect':vect,'Graphs':graficas,\n",
    "#                         'Average degree':avdeg,'Betweenness':betw,'Closeness':closs,'Load centrality':load,\n",
    "#                         'Assortativity':assort,'Clustering':clustering,\n",
    "#                         'Number components':comp,'Size largest component':size,\n",
    "#                         'Closeness residence':closnonac,'Closeness origin':closnac,'Number residence':numnonac,\n",
    "#                         'Number origin':numnac})\n",
    "redes=pd.DataFrame(data={'Subject origin':ori,'Subject residence':res,'Subject ID':num,'Graphs':graficas,\n",
    "                         'Average degree':avdeg,'Betweenness':betw,'Closeness':closs,'Load centrality':load,\n",
    "                         'Assortativity':assort,'Clustering':clustering,\n",
    "                         'Number components':comp,'Size largest component':size})\n",
    "\n",
    "df.isnull().sum()\n",
    "redes.fillna(0,inplace=True)\n",
    "###Calculating the Dunbar's parameters \n",
    "#redes['Fitted']=redes['Vect'].apply(lambda x:Individual(x).fit_model())\n",
    "#redes[['Mu','Mu-','Mu+']] = pd.DataFrame(redes.Fitted.values.tolist(), index= redes.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some changes in the dataframe \n",
    "\n",
    "At this point, we make some changes in the ego networks dataframe. We introduce a function that substitutes the confidence interval of the $\\mu$ by an string that determines the regime. Then we delete the columns associated with the confidence interval, fill the _NaN_ and rename some columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Creating function to make clearer the regime\n",
    "def rex(x,y):\n",
    "    if ((x < 0) & (y < 0)): z='Inverted'\n",
    "    elif ((x > 0) & (y > 0)): z='Standard'\n",
    "    else : z='Unclear'\n",
    "    return z\n",
    "\n",
    "###Introducing the former function in our dataframe\n",
    "#regime=[0]*sujetos\n",
    "#for i in range(len(redes)):\n",
    "#    regime[i]=rex(redes['Mu-'][i],redes['Mu+'][i])\n",
    "#redes['Regime']=regime\n",
    "#del redes['Fitted']\n",
    "#del redes['Mu-']\n",
    "#del redes['Mu+']\n",
    "\n",
    "###Filling NaNs\n",
    "#redes.fillna(0,inplace=True)\n",
    "\n",
    "###Renaming columns and getting rid of the networkx graph\n",
    "#redes=redes[['Subject ID','Subject origin','Subject residence','Mu','Regime','Average degree','Betweenness',\n",
    "#             'Closeness','Load centrality','Assortativity','Clustering',\n",
    "#             'Number components','Size largest component',\n",
    "#             'Closeness residence','Number residence','Closeness origin','Number origin','Graphs']]\n",
    "redes=redes[['Subject ID','Subject origin','Subject residence','Average degree','Betweenness',\n",
    "             'Closeness','Load centrality','Assortativity','Clustering',\n",
    "             'Number components','Size largest component','Graphs']]\n",
    "\n",
    "del redes['Graphs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More changes in the networks dataframe and merging \n",
    "\n",
    "Now we are going to merge the network measures dataframe with the original one _df_, that contains information about egos and alteris. We want to preserve the egos attributes, so we delete all the information about the alteris. We will also introduce the variable _FMIG2_ that measures the number of years these migrants have spent in their residence countries. Once we have made all these changes, some typos need to be corrected, and that is what we do at the end of this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Deleting the alteri column in df and merging it with the networks dataframe\n",
    "df.drop([\"alter/origin\",\"alter/residence\",\"alter/num\"],axis=1,inplace=True)\n",
    "df.drop([col for col in df.columns if col[0] == \"A\"],axis=1,inplace=True)\n",
    "df.rename(columns={'sub/num':'Subject ID','sub/origin':'Subject origin','sub/residence':'Subject residence'},inplace=True)\n",
    "df = df.drop_duplicates(['Subject origin','Subject ID'])\n",
    "\n",
    "redes2= pd.merge(redes, df,how='left',on=['Subject ID','Subject origin','Subject residence'])\n",
    "redes2.drop(\"Subject ID\",axis=1,inplace=True)\n",
    "\n",
    "###Calculating FMIG2\n",
    "import datetime\n",
    "FMIG2=[0]*len(redes2)\n",
    "for i in range(len(redes2)):\n",
    "    if (redes2['FMIG'].iloc[i]<0.5): \n",
    "        FMIG2[i]=0\n",
    "    elif (redes2['FMIG'].iloc[i]>1500):\n",
    "        FMIG2[i] = 2005 - redes2['FMIG'].iloc[i]\n",
    "redes2['FMIG2']=FMIG2\n",
    "redes2.drop(\"FMIG\",axis=1,inplace=True)\n",
    "\n",
    "###Renaming columns and changing some typos\n",
    "redes2.rename(columns={'Subject origin':'Subject_origin','Subject residence':'Subject_residence'},inplace=True)\n",
    "redes2.columns = redes2.columns.str.replace(' ', '_')\n",
    "redes2.dropna(axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the final results and display a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "redes2.to_csv(\"Redes_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_origin</th>\n",
       "      <th>Subject_residence</th>\n",
       "      <th>Average_degree</th>\n",
       "      <th>Betweenness</th>\n",
       "      <th>Closeness</th>\n",
       "      <th>Load_centrality</th>\n",
       "      <th>Assortativity</th>\n",
       "      <th>Clustering</th>\n",
       "      <th>Number_components</th>\n",
       "      <th>Size_largest_component</th>\n",
       "      <th>...</th>\n",
       "      <th>TIEDEN</th>\n",
       "      <th>TIEDC</th>\n",
       "      <th>TIECC</th>\n",
       "      <th>TIEBC</th>\n",
       "      <th>TIECND</th>\n",
       "      <th>TIECCZ</th>\n",
       "      <th>DSET</th>\n",
       "      <th>sub/language</th>\n",
       "      <th>alter_language</th>\n",
       "      <th>FMIG2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>pu</td>\n",
       "      <td>usa</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>-0.022727</td>\n",
       "      <td>0.443452</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18990</td>\n",
       "      <td>78.66287</td>\n",
       "      <td>2332.70721</td>\n",
       "      <td>7.66692</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>rc1f1pre</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>se</td>\n",
       "      <td>sp</td>\n",
       "      <td>7.955556</td>\n",
       "      <td>0.027030</td>\n",
       "      <td>0.413990</td>\n",
       "      <td>0.027028</td>\n",
       "      <td>0.177088</td>\n",
       "      <td>0.603074</td>\n",
       "      <td>2</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12929</td>\n",
       "      <td>83.24420</td>\n",
       "      <td>39.04754</td>\n",
       "      <td>30.20888</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.81</td>\n",
       "      <td>l3bs</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>ar</td>\n",
       "      <td>sp</td>\n",
       "      <td>9.644444</td>\n",
       "      <td>0.029058</td>\n",
       "      <td>0.454121</td>\n",
       "      <td>0.029058</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.643503</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.21919</td>\n",
       "      <td>81.71247</td>\n",
       "      <td>29.39773</td>\n",
       "      <td>28.89688</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.69</td>\n",
       "      <td>l5ca</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>do</td>\n",
       "      <td>usa</td>\n",
       "      <td>15.466667</td>\n",
       "      <td>0.015520</td>\n",
       "      <td>0.544001</td>\n",
       "      <td>0.015518</td>\n",
       "      <td>-0.008601</td>\n",
       "      <td>0.592477</td>\n",
       "      <td>2</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.17677</td>\n",
       "      <td>85.38206</td>\n",
       "      <td>8419.10629</td>\n",
       "      <td>34.55963</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>rc2ds</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>se</td>\n",
       "      <td>sp</td>\n",
       "      <td>22.488889</td>\n",
       "      <td>0.011443</td>\n",
       "      <td>0.680561</td>\n",
       "      <td>0.011435</td>\n",
       "      <td>-0.138651</td>\n",
       "      <td>0.658373</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45051</td>\n",
       "      <td>55.37099</td>\n",
       "      <td>63.79310</td>\n",
       "      <td>8.60800</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>l2bs</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>do</td>\n",
       "      <td>usa</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002607</td>\n",
       "      <td>-0.022727</td>\n",
       "      <td>0.390453</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11212</td>\n",
       "      <td>84.07258</td>\n",
       "      <td>5171.17767</td>\n",
       "      <td>1.72071</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>rc2de</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>ar</td>\n",
       "      <td>sp</td>\n",
       "      <td>12.933333</td>\n",
       "      <td>0.016420</td>\n",
       "      <td>0.594448</td>\n",
       "      <td>0.016420</td>\n",
       "      <td>-0.177886</td>\n",
       "      <td>0.771905</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.29394</td>\n",
       "      <td>73.89007</td>\n",
       "      <td>83.91839</td>\n",
       "      <td>45.84123</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>l5da</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>ar</td>\n",
       "      <td>sp</td>\n",
       "      <td>9.555556</td>\n",
       "      <td>0.018205</td>\n",
       "      <td>0.569890</td>\n",
       "      <td>0.018205</td>\n",
       "      <td>-0.374783</td>\n",
       "      <td>0.773919</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.21717</td>\n",
       "      <td>81.92389</td>\n",
       "      <td>89.00012</td>\n",
       "      <td>56.72185</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>l2aa</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>se</td>\n",
       "      <td>sp</td>\n",
       "      <td>22.977778</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>0.680042</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>-0.110167</td>\n",
       "      <td>0.529352</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>52.32558</td>\n",
       "      <td>39.81595</td>\n",
       "      <td>2.83102</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>l1s</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>do</td>\n",
       "      <td>usa</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.003270</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002161</td>\n",
       "      <td>-0.022727</td>\n",
       "      <td>0.391767</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09293</td>\n",
       "      <td>94.19280</td>\n",
       "      <td>3398.46306</td>\n",
       "      <td>7.04420</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>rc2f1de</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Subject_origin Subject_residence  Average_degree  Betweenness  Closeness  \\\n",
       "136             pu               usa       44.000000     0.005251   1.000000   \n",
       "440             se                sp        7.955556     0.027030   0.413990   \n",
       "216             ar                sp        9.644444     0.029058   0.454121   \n",
       "61              do               usa       15.466667     0.015520   0.544001   \n",
       "426             se                sp       22.488889     0.011443   0.680561   \n",
       "104             do               usa       44.000000     0.002744   1.000000   \n",
       "217             ar                sp       12.933333     0.016420   0.594448   \n",
       "265             ar                sp        9.555556     0.018205   0.569890   \n",
       "420             se                sp       22.977778     0.011113   0.680042   \n",
       "70              do               usa       44.000000     0.003270   1.000000   \n",
       "\n",
       "     Load_centrality  Assortativity  Clustering  Number_components  \\\n",
       "136         0.004416      -0.022727    0.443452                  1   \n",
       "440         0.027028       0.177088    0.603074                  2   \n",
       "216         0.029058       0.001705    0.643503                  1   \n",
       "61          0.015518      -0.008601    0.592477                  2   \n",
       "426         0.011435      -0.138651    0.658373                  1   \n",
       "104         0.002607      -0.022727    0.390453                  1   \n",
       "217         0.016420      -0.177886    0.771905                  1   \n",
       "265         0.018205      -0.374783    0.773919                  1   \n",
       "420         0.011113      -0.110167    0.529352                  1   \n",
       "70          0.002161      -0.022727    0.391767                  1   \n",
       "\n",
       "     Size_largest_component  ...   TIEDEN     TIEDC       TIECC     TIEBC  \\\n",
       "136                1.000000  ...  0.18990  78.66287  2332.70721   7.66692   \n",
       "440                0.955556  ...  0.12929  83.24420    39.04754  30.20888   \n",
       "216                1.000000  ...  0.21919  81.71247    29.39773  28.89688   \n",
       "61                 0.955556  ...  0.17677  85.38206  8419.10629  34.55963   \n",
       "426                1.000000  ...  0.45051  55.37099    63.79310   8.60800   \n",
       "104                1.000000  ...  0.11212  84.07258  5171.17767   1.72071   \n",
       "217                1.000000  ...  0.29394  73.89007    83.91839  45.84123   \n",
       "265                1.000000  ...  0.21717  81.92389    89.00012  56.72185   \n",
       "420                1.000000  ...  0.50000  52.32558    39.81595   2.83102   \n",
       "70                 1.000000  ...  0.09293  94.19280  3398.46306   7.04420   \n",
       "\n",
       "     TIECND  TIECCZ      DSET  sub/language  alter_language  FMIG2  \n",
       "136    11.0    0.90  rc1f1pre            en              en    0.0  \n",
       "440     4.0    0.81      l3bs            es              es    2.0  \n",
       "216     8.0    0.69      l5ca            es              es   -1.0  \n",
       "61      8.0    0.79     rc2ds            es              es    0.0  \n",
       "426    15.0    0.47      l2bs            es              es   11.0  \n",
       "104     7.0    0.91     rc2de            en              en    0.0  \n",
       "217    14.0    0.73      l5da            es              es   16.0  \n",
       "265     8.0    0.84      l2aa            es              es    4.0  \n",
       "420    22.0    0.34       l1s            es              es    0.0  \n",
       "70      4.0    0.87   rc2f1de            en              en    0.0  \n",
       "\n",
       "[10 rows x 100 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redes2.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
