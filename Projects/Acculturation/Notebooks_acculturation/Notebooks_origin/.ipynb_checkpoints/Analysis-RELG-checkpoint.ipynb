{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis \n",
    "\n",
    "In this notebook we will take the data from the _Ego networks_ notebook and make an analysis with the three different methods: a multinomial logistic model, a random forest method an an artificial neural network. \n",
    "<br><br>\n",
    "First, we will load the data, we will check for outliers and then we will prepare and format the predictors in order to apply each one of these methods. \n",
    "<br><br>\n",
    "The first step is loading the libraries, in this case we will use the standard numpy, pandas, matplotlib and seaborn for manipulating and plotting the data. In order to apply the different techniques of analysis, we will use sklearn, statsmodels and tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "# Sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Statsmodels\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.api import MNLogit\n",
    "\n",
    "\n",
    "\n",
    "# Just to print prettier. Uncomment to see all (not important) warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The next step is loading the .csv file from the previous notebook. Then we will select the columns we will use for the analysis, as the notebook contains a lot of information of the egos not related to the structural properties of their networks. Then we will map the categorical columns to a numerical encoding in the columns of : _Subject origin_, _Subject residence_, and _Regime_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "468    1\n",
       "469    1\n",
       "470    1\n",
       "471    1\n",
       "472    1\n",
       "Name: Subject_residence, Length: 473, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Read data\n",
    "df_2 = pd.read_csv('Redes_2.csv')\n",
    "### Drop Unnecessary Variables\n",
    "df_2.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "\n",
    "###Take the necessary ones\n",
    "df = df_2[df_2.columns[0:17]]\n",
    "df['EDUC'] = df_2['EDUC'].copy()\n",
    "df['FMIG2'] = df_2['FMIG2'].copy()\n",
    "df['SEX'] = df_2['SEX'].copy()\n",
    "df['RELG'] = df_2['RELG'].copy()\n",
    "\n",
    "### The numerical encoding\n",
    "#not_apply = ['Subject_origin','Subject_residence','Regime']\n",
    "not_apply = ['Subject_origin','Subject_residence']\n",
    "diccs = [0]*len(not_apply)\n",
    "i = 0\n",
    "for col in not_apply: \n",
    "        uniques = list(df[col].unique()) \n",
    "        diccs[i] = {uniques[j]:uniques.index(uniques[j]) for j in range(len(uniques)) }\n",
    "        df[col] = df[col].map(diccs[i])\n",
    "        i+=1\n",
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "### Reset the datatype of the columns\n",
    "df['Subject_origin'].astype('int64')\n",
    "df['Subject_residence'].astype('int64')\n",
    "#df['Regime'].astype('int64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and explore data\n",
    "\n",
    "We make an overview of the main statistics of the data and the properties we have generated in the past notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_num</th>\n",
       "      <th>Subject_origin</th>\n",
       "      <th>Subject_residence</th>\n",
       "      <th>Mu</th>\n",
       "      <th>Regime</th>\n",
       "      <th>Average_degree</th>\n",
       "      <th>Betweenness</th>\n",
       "      <th>Closeness</th>\n",
       "      <th>Load_centrality</th>\n",
       "      <th>Assortativity</th>\n",
       "      <th>...</th>\n",
       "      <th>Number_components</th>\n",
       "      <th>Size_largest_component</th>\n",
       "      <th>Closeness_residence</th>\n",
       "      <th>Number_residence</th>\n",
       "      <th>Closeness_origin</th>\n",
       "      <th>Number_origin</th>\n",
       "      <th>EDUC</th>\n",
       "      <th>FMIG2</th>\n",
       "      <th>SEX</th>\n",
       "      <th>RELG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unclear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>48.919662</td>\n",
       "      <td>4.997886</td>\n",
       "      <td>0.596195</td>\n",
       "      <td>-0.743170</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.910303</td>\n",
       "      <td>0.015374</td>\n",
       "      <td>0.687343</td>\n",
       "      <td>0.014964</td>\n",
       "      <td>-0.028397</td>\n",
       "      <td>...</td>\n",
       "      <td>1.162791</td>\n",
       "      <td>0.985243</td>\n",
       "      <td>2.766545</td>\n",
       "      <td>12.562368</td>\n",
       "      <td>2.495494</td>\n",
       "      <td>26.670190</td>\n",
       "      <td>3.509514</td>\n",
       "      <td>83.097252</td>\n",
       "      <td>1.448203</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>39.256840</td>\n",
       "      <td>2.781978</td>\n",
       "      <td>0.491179</td>\n",
       "      <td>13.524199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.574373</td>\n",
       "      <td>0.011329</td>\n",
       "      <td>0.216891</td>\n",
       "      <td>0.011674</td>\n",
       "      <td>0.213720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496737</td>\n",
       "      <td>0.053987</td>\n",
       "      <td>1.232411</td>\n",
       "      <td>10.960871</td>\n",
       "      <td>0.876297</td>\n",
       "      <td>13.596995</td>\n",
       "      <td>1.432231</td>\n",
       "      <td>395.983269</td>\n",
       "      <td>0.497836</td>\n",
       "      <td>11.438857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-294.081935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.628571</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.130665</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-0.695654</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.299994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.818182</td>\n",
       "      <td>0.006953</td>\n",
       "      <td>0.532497</td>\n",
       "      <td>0.005732</td>\n",
       "      <td>-0.139165</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.263158</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.111711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.066667</td>\n",
       "      <td>0.014024</td>\n",
       "      <td>0.632030</td>\n",
       "      <td>0.013907</td>\n",
       "      <td>-0.045455</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.560976</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>66.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100436</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.666667</td>\n",
       "      <td>0.020487</td>\n",
       "      <td>0.938077</td>\n",
       "      <td>0.020487</td>\n",
       "      <td>0.024567</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3.052632</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>161.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.302179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.955556</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.999012</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.974478</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>4.883721</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2018.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Subject_num  Subject_origin  Subject_residence          Mu   Regime  \\\n",
       "count    473.000000      473.000000         473.000000  473.000000      473   \n",
       "unique          NaN             NaN                NaN         NaN        3   \n",
       "top             NaN             NaN                NaN         NaN  Unclear   \n",
       "freq            NaN             NaN                NaN         NaN      222   \n",
       "mean      48.919662        4.997886           0.596195   -0.743170      NaN   \n",
       "std       39.256840        2.781978           0.491179   13.524199      NaN   \n",
       "min        1.000000        0.000000           0.000000 -294.081935      NaN   \n",
       "25%       17.000000        2.000000           0.000000   -0.299994      NaN   \n",
       "50%       42.000000        5.000000           1.000000   -0.111711      NaN   \n",
       "75%       66.000000        8.000000           1.000000    0.100436      NaN   \n",
       "max      161.000000        9.000000           1.000000    2.302179      NaN   \n",
       "\n",
       "        Average_degree  Betweenness   Closeness  Load_centrality  \\\n",
       "count       473.000000   473.000000  473.000000       473.000000   \n",
       "unique             NaN          NaN         NaN              NaN   \n",
       "top                NaN          NaN         NaN              NaN   \n",
       "freq               NaN          NaN         NaN              NaN   \n",
       "mean         23.910303     0.015374    0.687343         0.014964   \n",
       "std          13.574373     0.011329    0.216891         0.011674   \n",
       "min           2.628571     0.000023    0.130665         0.000023   \n",
       "25%          12.818182     0.006953    0.532497         0.005732   \n",
       "50%          19.066667     0.014024    0.632030         0.013907   \n",
       "75%          40.666667     0.020487    0.938077         0.020487   \n",
       "max          43.955556     0.078431    0.999012         0.078431   \n",
       "\n",
       "        Assortativity  ...  Number_components  Size_largest_component  \\\n",
       "count      473.000000  ...         473.000000              473.000000   \n",
       "unique            NaN  ...                NaN                     NaN   \n",
       "top               NaN  ...                NaN                     NaN   \n",
       "freq              NaN  ...                NaN                     NaN   \n",
       "mean        -0.028397  ...           1.162791                0.985243   \n",
       "std          0.213720  ...           0.496737                0.053987   \n",
       "min         -0.695654  ...           1.000000                0.500000   \n",
       "25%         -0.139165  ...           1.000000                1.000000   \n",
       "50%         -0.045455  ...           1.000000                1.000000   \n",
       "75%          0.024567  ...           1.000000                1.000000   \n",
       "max          0.974478  ...           6.000000                1.000000   \n",
       "\n",
       "        Closeness_residence  Number_residence  Closeness_origin  \\\n",
       "count            473.000000        473.000000        473.000000   \n",
       "unique                  NaN               NaN               NaN   \n",
       "top                     NaN               NaN               NaN   \n",
       "freq                    NaN               NaN               NaN   \n",
       "mean               2.766545         12.562368          2.495494   \n",
       "std                1.232411         10.960871          0.876297   \n",
       "min                0.000000          0.000000          0.000000   \n",
       "25%                2.263158          4.000000          2.062500   \n",
       "50%                3.000000         10.000000          2.560976   \n",
       "75%                3.600000         19.000000          3.052632   \n",
       "max                5.000000         66.000000          4.883721   \n",
       "\n",
       "        Number_origin        EDUC        FMIG2         SEX        RELG  \n",
       "count      473.000000  473.000000   473.000000  473.000000  473.000000  \n",
       "unique            NaN         NaN          NaN         NaN         NaN  \n",
       "top               NaN         NaN          NaN         NaN         NaN  \n",
       "freq              NaN         NaN          NaN         NaN         NaN  \n",
       "mean        26.670190    3.509514    83.097252    1.448203    1.000000  \n",
       "std         13.596995    1.432231   395.983269    0.497836   11.438857  \n",
       "min          0.000000    1.000000     0.000000    1.000000  -99.000000  \n",
       "25%         17.000000    2.000000     0.000000    1.000000    1.000000  \n",
       "50%         29.000000    4.000000     0.000000    1.000000    2.000000  \n",
       "75%         38.000000    4.000000     0.000000    2.000000    2.000000  \n",
       "max         72.000000    7.000000  2018.000000    2.000000    6.000000  \n",
       "\n",
       "[11 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some values of `mu` are way out of range (min = -294). This is clearly from divergences in the model. We mark observations greater than 10 (in absolute value) as `nan` and then drop `nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean estimates for mu\n",
    "#df['Mu'] = df['Mu'].apply(lambda x: np.nan if x < -100 else x)\n",
    "#df['Mu'] = df['Mu'].apply(lambda x: np.nan if x > 100 else x)\n",
    "### Check how many values have this problems\n",
    "#df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group some nationalities in `others` group\n",
    "\n",
    "We keep only classes with more than 50 observations. The rest of the classes will be considered as one called \"others\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are few data on several Origins\n",
    "count_origins = pd.get_dummies(df['Subject_origin']).sum()\n",
    "t = 50 # threshold\n",
    "df['Subject_origin'] = df['Subject_origin'].apply(lambda x: 10 if (count_origins[x] < t) else x)\n",
    "#pd.get_dummies(df['Subject_origin']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is just to translate the encoding to the first five integers\n",
    "dicc_traslation = {10:0,2:1,5:2,6:3,8:4,9:5}\n",
    "dicc_final = {0:\"Other\",1:\"Dominican\",2:\"PuertoRican\",3:\"Argentinean\",4:\"Moroccan\",5:\"Senegambian\"}\n",
    "df['Subject_origin'] = df['Subject_origin'].map(dicc_traslation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `predictors` for all the inference and prediction methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['Closeness','Clustering','Average_degree','Assortativity','Betweenness']\n",
    "target = \"Subject_origin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `train` and `test` split for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[predictors]       # independent variables\n",
    "y = df[target]\n",
    "\n",
    "test_size = 0.20 #maybe more is needed (20% is standard though)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 0)\n",
    "\n",
    "#Define dataframe as merge of X and y\n",
    "df_str = df[target].to_frame().merge(df[predictors], left_index=True, right_index=True)\n",
    "\n",
    "# Standar Scaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform (X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE\n",
    "\n",
    "At this point, we begin to include tools of inference, beginning by the multinomial logistic regression (MLN).\n",
    "The library used for this analysis is mainly _statsmodels_ and the main function can be checked in this link:\n",
    "https://stats.idre.ucla.edu/stata/dae/multinomiallogistic-regression/\n",
    "\n",
    "In this part of the notebook we will prepare the variables, execute the regression and save the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Multinomial Logistic Model\n",
    "\n",
    "https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.MNLogit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.419670\n",
      "         Iterations 8\n"
     ]
    }
   ],
   "source": [
    "### Uses the list 'predictors' as independent variables\n",
    "formula_predictors = ' + '.join(predictors)\n",
    "target_str = target +\" ~ {}\"\n",
    "model = MNLogit.from_formula(target_str.format(formula_predictors), df_str)\n",
    "results = model.fit(maxiter=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          MNLogit Regression Results                          \n",
      "==============================================================================\n",
      "Dep. Variable:         Subject_origin   No. Observations:                  473\n",
      "Model:                        MNLogit   Df Residuals:                      443\n",
      "Method:                           MLE   Df Model:                           25\n",
      "Date:                Tue, 21 Dec 2021   Pseudo R-squ.:                  0.1627\n",
      "Time:                        12:57:50   Log-Likelihood:                -671.50\n",
      "converged:                       True   LL-Null:                       -802.02\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.561e-41\n",
      "====================================================================================\n",
      "Subject_origin=1       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "Intercept           -0.1998      2.265     -0.088      0.930      -4.639       4.240\n",
      "Closeness           15.6155      5.478      2.850      0.004       4.878      26.353\n",
      "Clustering          -2.2555      1.192     -1.893      0.058      -4.591       0.080\n",
      "Average_degree      -0.2721      0.087     -3.119      0.002      -0.443      -0.101\n",
      "Assortativity        3.7376      1.342      2.784      0.005       1.107       6.369\n",
      "Betweenness        -58.5871     25.681     -2.281      0.023    -108.920      -8.254\n",
      "------------------------------------------------------------------------------------\n",
      "Subject_origin=2       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "Intercept           -1.5998      2.771     -0.577      0.564      -7.032       3.832\n",
      "Closeness           12.5818      7.109      1.770      0.077      -1.351      26.514\n",
      "Clustering          -4.0922      1.352     -3.027      0.002      -6.742      -1.443\n",
      "Average_degree      -0.1572      0.112     -1.402      0.161      -0.377       0.063\n",
      "Assortativity        3.2617      1.632      1.999      0.046       0.063       6.460\n",
      "Betweenness        -14.5503     28.057     -0.519      0.604     -69.541      40.441\n",
      "------------------------------------------------------------------------------------\n",
      "Subject_origin=3       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "Intercept           -3.9134      2.517     -1.555      0.120      -8.846       1.019\n",
      "Closeness           13.9670      5.386      2.593      0.010       3.410      24.524\n",
      "Clustering           7.5942      1.811      4.192      0.000       4.044      11.145\n",
      "Average_degree      -0.4374      0.095     -4.593      0.000      -0.624      -0.251\n",
      "Assortativity        1.5435      1.391      1.109      0.267      -1.183       4.270\n",
      "Betweenness        -51.6253     28.394     -1.818      0.069    -107.276       4.026\n",
      "------------------------------------------------------------------------------------\n",
      "Subject_origin=4       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "Intercept            0.7910      2.425      0.326      0.744      -3.962       5.544\n",
      "Closeness            9.5840      5.559      1.724      0.085      -1.311      20.479\n",
      "Clustering           1.9741      1.495      1.320      0.187      -0.957       4.905\n",
      "Average_degree      -0.2714      0.092     -2.961      0.003      -0.451      -0.092\n",
      "Assortativity        2.6681      1.404      1.900      0.057      -0.084       5.420\n",
      "Betweenness       -102.5844     32.412     -3.165      0.002    -166.111     -39.058\n",
      "------------------------------------------------------------------------------------\n",
      "Subject_origin=5       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "Intercept           -0.5633      2.554     -0.221      0.825      -5.569       4.443\n",
      "Closeness            9.1637      5.848      1.567      0.117      -2.299      20.626\n",
      "Clustering           2.6123      1.445      1.808      0.071      -0.219       5.444\n",
      "Average_degree      -0.2229      0.094     -2.363      0.018      -0.408      -0.038\n",
      "Assortativity        2.3300      1.423      1.638      0.101      -0.458       5.118\n",
      "Betweenness        -92.4066     34.120     -2.708      0.007    -159.280     -25.533\n",
      "====================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pseudo r-squared = 0.16\n"
     ]
    }
   ],
   "source": [
    "print('pseudo r-squared = {}'.format(np.round(results.prsquared,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.561483640849888e-41"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.llr_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTION\n",
    "\n",
    "We train and fit a powerful non-linear (and non-parametric) machine learnin classifier to the data; a Random Forest. There are many other alternatives, but tree based metods are very powerfull and there are new techniques to help identify relevant predictors.\n",
    "\n",
    "In this section, we want to test wether this model can outperform significantly other null (dummy) classifiers. If that is the case (which it is), it confirms the hypothesis that the predictors have relevant information about the nationalities of the subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test with MNL regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.382057\n",
      "         Iterations 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "formula_predictors = ' + '.join(predictors)\n",
    "model = MNLogit.from_formula(target_str.format(formula_predictors), df_str.loc[y_train.index])\n",
    "results_prediction = model.fit(maxiter=200)\n",
    "ypred = results_prediction.predict(df_str.loc[y_test.index])\n",
    "y_pred =list(map(np.argmax,np.array(ypred)))\n",
    "##Meter función accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35789473684210527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and tune the model using k-cross fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting Random Forest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scoring = 'accuracy' #'f1_macro' # This chooses the metric to optimise during training (there are others!)\n",
    "njobs=-1                         # This the number of cores used in your cpu (-1 means \"all of them\")\n",
    "cv=5                             # the k in k-cross-fold validation\n",
    "# RANDOM FOREST\n",
    "print('\\nFitting Random Forest\\n')\n",
    "\n",
    "rfc=RandomForestClassifier(random_state=0)\n",
    "# Parameter combinations to explore\n",
    "param_grid = { \n",
    "    'n_estimators': [75, 100,300,1000],\n",
    "    'max_features': ['auto', None],\n",
    "    'min_samples_split' :[2,6, 10, 14],\n",
    "    'max_depth' : [10, 15, 30, 50,None],\n",
    "    'max_samples' : [0.5 ,0.7, None],}\n",
    "\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring = scoring,\n",
    "                  verbose=0,\n",
    "                  n_jobs=njobs,\n",
    "                  cv= cv)\n",
    "CV_rfc.fit(X_train, y_train)\n",
    "\n",
    "print('\\nRandom Forest:')\n",
    "print('Best Score: ', CV_rfc.best_score_)\n",
    "print('Best Params: ', CV_rfc.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the algorithm performance in the test set (unseen data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = CV_rfc.predict(X_test)\n",
    "print('Confusion Matrix:\\n ', confusion_matrix(y_test,y_pred),'\\n')\n",
    "print(classification_report(y_test,y_pred),'\\n')\n",
    "print('Accuracy: {0:.2f}'.format(accuracy_score(y_test, y_pred),2))\n",
    "dicc_final = {0:\"Other\",1:\"Dominican\",2:\"PuertoRican\",3:\"Argentinean\",4:\"Moroccan\",5:\"Senegambian\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare this performance with  null models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  relative prevalence of each class\n",
    "rel_prev = (y.value_counts() / len(y))\n",
    "print(rel_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform Dummy Classifier (classifies randomly with p = 1/6)\n",
    "\n",
    "# If the classifier randomly guesses: \n",
    "print('Acurracy of uniform dummy classifier: ',(((1/6) * y.value_counts()) / len(y)).sum()) # = 1/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Dummy Classifier (classifies randomly with p ~ prevalence of each class)\n",
    "print('Acurracy of stratified dummy classifier: ',(rel_prev * y.value_counts()).sum() / len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent Dummy Classifier (classifies always in the most frequent class)\n",
    "print('Acurracy of Most freq dummy classifier: ',rel_prev.max() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLEARN versions of the dummy classifiers (to double check and for convinience methods)\n",
    "\n",
    "dummy = \"stratified\"# most_frequent, stratified, uniform\n",
    "dummy_clf = DummyClassifier(strategy=dummy,random_state=0) \n",
    "\n",
    " \n",
    "\n",
    "# Actual accuracy of the dummy in the same train-test split as the RF model\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_score = dummy_clf.score(X_test, y_test)\n",
    "print('Mean accuracy of null ' + dummy +' model: {0:.2f}'.format(dummy_score),'\\n')\n",
    "print('Mean accuracy (in test) of RF model: {0:.2f}'.format(CV_rfc.score(X_test, y_test)),'\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix and report of the selected dummy classifier\n",
    "\n",
    "y_pred_dummy = dummy_clf.predict(X_test)\n",
    "print('Confusion Matrix:\\n\\n ',confusion_matrix(y_test,y_pred_dummy),'\\n')\n",
    "print(classification_report(y_test,y_pred_dummy),'\\n')\n",
    "print('Accuracy: {0:.2f}'.format(accuracy_score(y_test, y_pred_dummy),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for reference, the results of the RF Model\n",
    "\n",
    "y_pred = CV_rfc.predict(X_test)\n",
    "print('Confusion Matrix:\\n\\n ', confusion_matrix(y_test,y_pred),'\\n')\n",
    "print(classification_report(y_test,y_pred),'\\n')\n",
    "print('Accuracy: {0:.2f}'.format(accuracy_score(y_test, y_pred),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_report = pd.DataFrame(classification_report(y_test,dummy_clf.predict(X_test), output_dict= True))\n",
    "\n",
    "rfc_report = pd.DataFrame(classification_report(y_test,CV_rfc.predict(X_test), output_dict= True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increase in prediction power (percentage with respect to null model)\n",
    "\n",
    "i.e. 100% means twice as good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table = ((rfc_report - dummy_report)*100 / dummy_report).drop('support').round(decimals=2)\n",
    "final_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This significant increases further support the claim that the predictors (based on ego-network properties) have useful information to predict the countries of origin of the individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shap Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "  Shap values are a tool to interpret our random forest model, in this case. They tell us some intuition about which part of the prediction belongs to each feature. \n",
    "</ul>\n",
    "<ul>\n",
    "A positive (negative) SHAP value indicates that the value (in this case, probability of belonging to a certain country) is reinforced (diminished) by the feature.  \n",
    "</ul>\n",
    "<ul>\n",
    "We will use 2 kind of plots at this moment. The first one one is a summary plot, a violin plot of the distribution of SHAP values. The colour indicates the value of the feature indicated at the left. This plot let us see the which features contribute the most (this is, they have high SHAP values). Features are ordered according to their contribution to the global prediction.\n",
    "</ul>\n",
    "<ul>\n",
    "The second kind of plot you will see several times after the summary plot is the dependence plot. They show the distribution of the SHAP values of a variable. The colormap plots another variable, the one the algorithm thinks it has more interaction with the current variable. It lets us distinguish between different regimes of the coloured variable. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the model's predictions using SHAP\n",
    "##Shap values\n",
    "import  shap\n",
    "\n",
    "shap.initjs()\n",
    "model = CV_rfc.best_estimator_\n",
    "explainer = shap.TreeExplainer(model,X_train,check_additivity=False)\n",
    "shap_values = explainer.shap_values(X_train,check_additivity=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of summary plot\n",
    "\n",
    "We extract the summary plots that summarizes the correlations for each nationality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>SHAP values for the dominican</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[1],X_train,feature_names = predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>SHAP values for the Puerto Rican</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[2],X_train,feature_names = predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>SHAP values for the argentinean</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[3],X_train,feature_names = predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>SHAP values for the moroccan</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[5],X_train,feature_names = predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>SHAP values for the control group</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[0],X_train,feature_names = predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "LIME (Local Interpretable Model-agnostic Explanations), is an algorithm that takes the decision function from the classifier (decision = f(features)). This function may be complex, but the algorithm makes a linear regression around a single prediction, weighting the importance of the coefficients with the distance to this local prediction.   \n",
    "</ul>\n",
    "<ul>\n",
    "This kind of algorithm helps us to explain single predictions.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Using LIME to interpret \n",
    "import lime\n",
    "import lime.lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=predictors, discretize_continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, X_test.shape[0])\n",
    "exp = explainer.explain_instance(X_test[i], CV_rfc.predict_proba, num_features=3, top_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True, show_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Artificial neural network\n",
    "\n",
    "As a complementary method, we train a simple ANN to provide a new method and give more strength to the previous results. In order to do that, we will preprocess the data, distinguishing the categorical and numerical predictors. Then we will split the dataset into the train and test parts and, finally, we will define the model and fit to obtain a final result for the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the package tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Define  a simple a ANN and fit our data\n",
    "stat_accul = []\n",
    "for i in range(10):\n",
    "    model_accul = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(70,activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(70,activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(6,activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    ###Compile the model \n",
    "    model_accul.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                   optimizer=tf.keras.optimizers.Adam(learning_rate=10e-4),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "    ### We fit the model 100 times and take notes of the accuracy on the test set\n",
    "\n",
    "    history_accul = model_accul.fit(X_train,\n",
    "                             np.array(y_train),\n",
    "                             epochs=100,\n",
    "                             verbose = 0)\n",
    "    stat_accul.append(model_accul.evaluate(X_test,np.array(y_test))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The final results for 10 training iterations is {np.average(stat_accul):.2f} with a std of {np.std(stat_accul):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
