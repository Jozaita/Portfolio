{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis \n",
    "\n",
    "In this notebook we will take the data from the _Ego networks_ notebook and make an analysis with the three different methods: a multinomial logistic model, a random forest method an an artificial neural network. \n",
    "<br><br>\n",
    "First, we will load the data, we will check for outliers and then we will prepare and format the predictors in order to apply each one of these methods. \n",
    "<br><br>\n",
    "The first step is loading the libraries, in this case we will use the standard numpy, pandas, matplotlib and seaborn for manipulating and plotting the data. In order to apply the different techniques of analysis, we will use sklearn, statsmodels and tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "# Sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Statsmodels\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.api import MNLogit\n",
    "\n",
    "\n",
    "\n",
    "# Just to print prettier. Uncomment to see all (not important) warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The next step is loading the .csv file from the previous notebook. Then we will select the columns we will use for the analysis, as the notebook contains a lot of information of the egos not related to the structural properties of their networks. Then we will map the categorical columns to a numerical encoding in the columns of : _Subject origin_, _Subject residence_, and _Regime_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Subject_residence'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Subject_residence'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m not_apply: \n\u001b[0;32m---> 20\u001b[0m         uniques \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munique()) \n\u001b[1;32m     21\u001b[0m         diccs[i] \u001b[38;5;241m=\u001b[39m {uniques[j]:uniques\u001b[38;5;241m.\u001b[39mindex(uniques[j]) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(uniques)) }\n\u001b[1;32m     22\u001b[0m         df[col] \u001b[38;5;241m=\u001b[39m df[col]\u001b[38;5;241m.\u001b[39mmap(diccs[i])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Subject_residence'"
     ]
    }
   ],
   "source": [
    "### Read data\n",
    "df_2 = pd.read_csv('Redes_2.csv')\n",
    "\n",
    "### Drop Unnecessary Variables\n",
    "df_2.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "\n",
    "###Take the necessary ones\n",
    "df = df_2[df_2.columns[0:17]]\n",
    "df['EDUC'] = df_2['EDUC'].copy()\n",
    "df['FMIG2'] = df_2['FMIG2'].copy()\n",
    "df['SEX'] = df_2['SEX'].copy()\n",
    "df['RELG'] = df_2['RELG'].copy()\n",
    "\n",
    "### The numerical encoding\n",
    "#not_apply = ['Subject_origin','Subject_residence','Regime']\n",
    "not_apply = ['Subject_origin','Subject_residence']\n",
    "diccs = [0]*len(not_apply)\n",
    "i = 0\n",
    "for col in not_apply: \n",
    "        uniques = list(df[col].unique()) \n",
    "        diccs[i] = {uniques[j]:uniques.index(uniques[j]) for j in range(len(uniques)) }\n",
    "        df[col] = df[col].map(diccs[i])\n",
    "        i+=1\n",
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "### Reset the datatype of the columns\n",
    "df['Subject_origin'].astype('int64')\n",
    "df['Subject_residence'].astype('int64')\n",
    "#df['Regime'].astype('int64')\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and explore data\n",
    "\n",
    "We make an overview of the main statistics of the data and the properties we have generated in the past notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some values of `mu` are way out of range (min = -294). This is clearly from divergences in the model. We mark observations greater than 10 (in absolute value) as `nan` and then drop `nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean estimates for mu\n",
    "df['Mu'] = df['Mu'].apply(lambda x: np.nan if x < -100 else x)\n",
    "df['Mu'] = df['Mu'].apply(lambda x: np.nan if x > 100 else x)\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group some nationalities in `others` group\n",
    "\n",
    "We keep only classes with more than 50 observations. The rest of the classes will be considered as one called \"others\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are few data on several Origins\n",
    "count_origins = pd.get_dummies(df['Subject_origin']).sum()\n",
    "t = 50 # threshold\n",
    "df['Subject_origin'] = df['Subject_origin'].apply(lambda x: 10 if (count_origins[x] < t) else x)\n",
    "#pd.get_dummies(df['Subject_origin']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is just to translate the encoding to the first five integers\n",
    "dicc_traslation = {10:0,2:1,5:2,6:3,8:4,9:5}\n",
    "dicc_final = {0:\"Other\",1:\"Dominican\",2:\"PuertoRican\",3:\"Argentinean\",4:\"Moroccan\",5:\"Senegambian\"}\n",
    "df['Subject_origin'] = df['Subject_origin'].map(dicc_traslation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `predictors` for all the inference and prediction methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictors = ['Closeness','Clustering','Average_degree','Assortativity','Betweenness',\n",
    "#             'Closeness_origin','Closeness_residence','Number_origin','Number_residence','Mu']\n",
    "predictors = ['Closeness','Clustering','Average_degree','Assortativity','Betweenness',\n",
    "             'Closeness_origin','Closeness_residence','Number_origin','Number_residence']\n",
    "target = \"Subject_origin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `train` and `test` split for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[predictors]       # independent variables\n",
    "y = df[target]\n",
    "\n",
    "test_size = 0.20 #maybe more is needed (20% is standard though)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 0)\n",
    "\n",
    "\n",
    "# Standard Scaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform (X_test)\n",
    "\n",
    "#Define dataframe as merge of X and y\n",
    "df_str = df[target].to_frame().merge(pd.DataFrame(sc.fit_transform(X),columns=predictors,index=df.index), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "log_test = LogisticRegression().fit(X_train,y_train)\n",
    "predictions = log_test.predict(X_test)\n",
    "print(classification_report(predictions,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = pd.DataFrame(data=log_test.coef_.T,index = df_str.drop(\"Subject_origin\",axis=1).columns, columns=dicc_final.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE\n",
    "\n",
    "At this point, we begin to include tools of inference, beginning by the multinomial logistic regression (MLN).\n",
    "The library used for this analysis is mainly _statsmodels_ and the main function can be checked in this link:\n",
    "https://stats.idre.ucla.edu/stata/dae/multinomiallogistic-regression/\n",
    "\n",
    "In this part of the notebook we will prepare the variables, execute the regression and save the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Multinomial Logistic Model\n",
    "\n",
    "https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.MNLogit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uses the list 'predictors' as independent variables\n",
    "formula_predictors = ' + '.join(predictors)\n",
    "target_str = target +\" ~ {}\"\n",
    "model = MNLogit.from_formula(target_str.format(formula_predictors), df_str)\n",
    "results = model.fit(maxiter=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('pseudo r-squared = {}'.format(np.round(results.prsquared,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.llr_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTION\n",
    "\n",
    "We train and fit a powerful non-linear (and non-parametric) machine learnin classifier to the data; a Random Forest. There are many other alternatives, but tree based metods are very powerfull and there are new techniques to help identify relevant predictors.\n",
    "\n",
    "In this section, we want to test wether this model can outperform significantly other null (dummy) classifiers. If that is the case (which it is), it confirms the hypothesis that the predictors have relevant information about the nationalities of the subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test with MNL regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "formula_predictors = ' + '.join(predictors)\n",
    "model = MNLogit.from_formula(target_str.format(formula_predictors), df_str.loc[y_train.index])\n",
    "results_prediction = model.fit(maxiter=200)\n",
    "ypred = results_prediction.predict(df_str.loc[y_test.index])\n",
    "y_pred =list(map(np.argmax,np.array(ypred)))\n",
    "##Meter funciÃ³n accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and tune the model using k-cross fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = 'accuracy' #'f1_macro' # This chooses the metric to optimise during training (there are others!)\n",
    "njobs=-1                         # This the number of cores used in your cpu (-1 means \"all of them\")\n",
    "cv=5                             # the k in k-cross-fold validation\n",
    "# RANDOM FOREST\n",
    "print('\\nFitting Random Forest\\n')\n",
    "\n",
    "rfc=RandomForestClassifier(random_state=0)\n",
    "# Parameter combinations to explore\n",
    "param_grid = { \n",
    "    'n_estimators': [75, 100,300,1000],\n",
    "    'max_features': ['auto', None],\n",
    "    'min_samples_split' :[2,6, 10, 14],\n",
    "    'max_depth' : [10, 15, 30, 50,None],\n",
    "    'max_samples' : [0.5 ,0.7, None],}\n",
    "\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring = scoring,\n",
    "                  verbose=0,\n",
    "                  n_jobs=njobs,\n",
    "                  cv= cv)\n",
    "CV_rfc.fit(X_train, y_train)\n",
    "\n",
    "print('\\nRandom Forest:')\n",
    "print('Best Score: ', CV_rfc.best_score_)\n",
    "print('Best Params: ', CV_rfc.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the algorithm performance in the test set (unseen data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = CV_rfc.predict(X_test)\n",
    "print('Confusion Matrix:\\n ', confusion_matrix(y_test,y_pred),'\\n')\n",
    "print(classification_report(y_test,y_pred),'\\n')\n",
    "print('Accuracy: {0:.2f}'.format(accuracy_score(y_test, y_pred),2))\n",
    "dicc_final = {0:\"Other\",1:\"Dominican\",2:\"PuertoRican\",3:\"Argentinean\",4:\"Moroccan\",5:\"Senegambian\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare this performance with  null models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Subject_origin\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  relative prevalence of each class\n",
    "rel_prev = (y.value_counts() / len(y))\n",
    "print(rel_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform Dummy Classifier (classifies randomly with p = 1/6)\n",
    "\n",
    "# If the classifier randomly guesses: \n",
    "print('Acurracy of uniform dummy classifier: ',(((1/6) * y.value_counts()) / len(y)).sum()) # = 1/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Dummy Classifier (classifies randomly with p ~ prevalence of each class)\n",
    "print('Acurracy of stratified dummy classifier: ',(rel_prev * y.value_counts()).sum() / len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent Dummy Classifier (classifies always in the most frequent class)\n",
    "print('Acurracy of Most freq dummy classifier: ',rel_prev.max() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLEARN versions of the dummy classifiers (to double check and for convinience methods)\n",
    "\n",
    "dummy = \"stratified\"# most_frequent, stratified, uniform\n",
    "dummy_clf = DummyClassifier(strategy=dummy,random_state=0) \n",
    "\n",
    " \n",
    "\n",
    "# Actual accuracy of the dummy in the same train-test split as the RF model\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_score = dummy_clf.score(X_test, y_test)\n",
    "print('Mean accuracy of null ' + dummy +' model: {0:.2f}'.format(dummy_score),'\\n')\n",
    "print('Mean accuracy (in test) of RF model: {0:.2f}'.format(CV_rfc.score(X_test, y_test)),'\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix and report of the selected dummy classifier\n",
    "\n",
    "y_pred_dummy = dummy_clf.predict(X_test)\n",
    "print('Confusion Matrix:\\n\\n ',confusion_matrix(y_test,y_pred_dummy),'\\n')\n",
    "print(classification_report(y_test,y_pred_dummy),'\\n')\n",
    "print('Accuracy: {0:.2f}'.format(accuracy_score(y_test, y_pred_dummy),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for reference, the results of the RF Model\n",
    "\n",
    "y_pred = CV_rfc.predict(X_test)\n",
    "print('Confusion Matrix:\\n\\n ', confusion_matrix(y_test,y_pred),'\\n')\n",
    "print(classification_report(y_test,y_pred),'\\n')\n",
    "print('Accuracy: {0:.2f}'.format(accuracy_score(y_test, y_pred),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_report = pd.DataFrame(classification_report(y_test,dummy_clf.predict(X_test), output_dict= True))\n",
    "\n",
    "rfc_report = pd.DataFrame(classification_report(y_test,CV_rfc.predict(X_test), output_dict= True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increase in prediction power (percentage with respect to null model)\n",
    "\n",
    "i.e. 100% means twice as good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table = ((rfc_report - dummy_report)*100 / dummy_report).drop('support').round(decimals=2)\n",
    "final_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This significant increases further support the claim that the predictors (based on ego-network properties) have useful information to predict the countries of origin of the individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shap Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "  Shap values are a tool to interpret our random forest model, in this case. They tell us some intuition about which part of the prediction belongs to each feature. \n",
    "</ul>\n",
    "<ul>\n",
    "A positive (negative) SHAP value indicates that the value (in this case, probability of belonging to a certain country) is reinforced (diminished) by the feature.  \n",
    "</ul>\n",
    "<ul>\n",
    "We will use 2 kind of plots at this moment. The first one one is a summary plot, a violin plot of the distribution of SHAP values. The colour indicates the value of the feature indicated at the left. This plot let us see the which features contribute the most (this is, they have high SHAP values). Features are ordered according to their contribution to the global prediction.\n",
    "</ul>\n",
    "<ul>\n",
    "The second kind of plot you will see several times after the summary plot is the dependence plot. They show the distribution of the SHAP values of a variable. The colormap plots another variable, the one the algorithm thinks it has more interaction with the current variable. It lets us distinguish between different regimes of the coloured variable. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the model's predictions using SHAP\n",
    "##Shap values\n",
    "import  shap\n",
    "\n",
    "shap.initjs()\n",
    "model = CV_rfc.best_estimator_\n",
    "explainer = shap.TreeExplainer(model,X_train,check_additivity=False)\n",
    "shap_values = explainer.shap_values(X_train,check_additivity=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of summary plot\n",
    "\n",
    "We extract the summary plots that summarizes the correlations for each nationality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>SHAP values for the dominican</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[1],X_train,feature_names = predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>SHAP values for the Puerto Rican</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[2],X_train,feature_names = predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>SHAP values for the argentinean</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[3],X_train,feature_names = predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>SHAP values for the moroccan</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[5],X_train,feature_names = predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>SHAP values for the control group</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[0],X_train,feature_names = predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "LIME (Local Interpretable Model-agnostic Explanations), is an algorithm that takes the decision function from the classifier (decision = f(features)). This function may be complex, but the algorithm makes a linear regression around a single prediction, weighting the importance of the coefficients with the distance to this local prediction.   \n",
    "</ul>\n",
    "<ul>\n",
    "This kind of algorithm helps us to explain single predictions.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Using LIME to interpret \n",
    "import lime\n",
    "import lime.lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=predictors, discretize_continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, X_test.shape[0])\n",
    "exp = explainer.explain_instance(X_test[i], CV_rfc.predict_proba, num_features=3, top_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True, show_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Artificial neural network\n",
    "\n",
    "As a complementary method, we train a simple ANN to provide a new method and give more strength to the previous results. In order to do that, we will preprocess the data, distinguishing the categorical and numerical predictors. Then we will split the dataset into the train and test parts and, finally, we will define the model and fit to obtain a final result for the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the package tensorflow\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Define  a simple a ANN and fit our data\n",
    "stat_accul = []\n",
    "model_accul = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(70,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(70,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(6,activation=\"softmax\")\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "###Compile the model \n",
    "model_accul.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "               optimizer=tf.keras.optimizers.Adam(learning_rate=10e-4),\n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "### We fit the model 100 times and take notes of the accuracy on the test set\n",
    "\n",
    "history_accul = model_accul.fit(X_train,\n",
    "                         np.array(y_train),\n",
    "                         epochs=100,\n",
    "                         verbose = 0)\n",
    "stat_accul.append(model_accul.evaluate(X_test,np.array(y_test))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The final results for a training iteration is {np.average(stat_accul):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radar plots for regressions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "df_fitted = sc.fit_transform(results.params)\n",
    "\n",
    "df_polar = pd.DataFrame(sc.fit_transform(results.params.transpose())).transpose()\n",
    "df_polar.columns = list(dicc_final.values())[1:]\n",
    "#df_polar.index = predictors.insert(0,\"Intercept\")\n",
    "df_polar = df_polar.drop(0,axis=0).reset_index().drop(\"index\",axis = 1)\n",
    "df_polar.index = predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook+pdf\" \n",
    "\n",
    "categories = predictors\n",
    "\n",
    "\n",
    "\n",
    "for col in df_polar.columns : \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r = df_polar[col].values,\n",
    "        theta = categories,\n",
    "        fill = \"toself\",\n",
    "        name = col\n",
    "    ))\n",
    "\n",
    "\n",
    "\n",
    "    fig.update_layout(\n",
    "      polar=dict(\n",
    "        radialaxis=dict(\n",
    "          visible=True,\n",
    "          range=[0, 1]\n",
    "        )),\n",
    "      showlegend=True,\n",
    "      font={\"size\":18}\n",
    "    )\n",
    "    \n",
    "    fig.write_image(\"Radar_\"+str(col)+\".jpg\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
