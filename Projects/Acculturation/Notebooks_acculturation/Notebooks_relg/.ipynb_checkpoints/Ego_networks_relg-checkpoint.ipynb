{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ego networks \n",
    "\n",
    "In this notebook, we will build the ego networks using the two dataframes of clean data obtained in the past notebook _clean.ipynb_. In order to achieve this: \n",
    "\n",
    "* We will use the library _networkx_ to transform the dataframe of relationships between the alteri into the different ego networks. \n",
    "* We will extract some measures of the structure of this ego networks and create with them a dataframe, as our goal is to use them as predictors of the nationality.\n",
    "\n",
    "<font color = \"blue\">\n",
    "The differences in the religion case are that we are creating a matrix of assortativities with respect to different attributes (<i>assort_atr</i>). These attributes are the ones defined in <i>cols_alteri</i> below.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the .csv files \n",
    "\n",
    "We import the different libraries, the usual numpy, pandas, matplotlib and networkx. The methods library that can be seen contains the algorithms to calculate the Dunbar estructure of an individual given an ego network. We will also incorporate this measure in our analysis in order to provide a more complete view. Then we load our .csv files and delete the unformatted columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/juan/Python/Acculturation/Contactos_relg.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_150077/3541788456.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m###Load .csv files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcontactos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/home/juan/Python/Acculturation/Contactos_relg.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/home/juan/Python/Acculturation/all_data_clean_relg.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/juan/Python/Acculturation/Contactos_relg.csv'"
     ]
    }
   ],
   "source": [
    "###Import libraries\n",
    "from numpy import nan\n",
    "import pandas as pd \n",
    "import networkx as nx\n",
    "from methods import *\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "###Load .csv files\n",
    "contactos=pd.read_csv(r'/home/juan/Python/Acculturation/Notebooks_relg/Contactos_relg.csv',low_memory=False)\n",
    "df=pd.read_csv(r'/home/juan/Python/Acculturation/Notebooks_relg/all_data_clean_relg.csv',low_memory=False)\n",
    "\n",
    "###Delete useless columns\n",
    "del contactos['Unnamed: 0']\n",
    "del df['Unnamed: 0']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out the total number of subjects\n",
    "\n",
    "In order to begin the calculations, we calculate the total number of subjets. We also transform the datatype of the columns that are going to play a role in the dessign of the ego networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Change datatypes\n",
    "contactos['Alter']=contactos['Alter'].astype(int)\n",
    "contactos['Alter2']=contactos['Alter2'].astype(int)\n",
    "\n",
    "#Find out the total number of subjects \n",
    "sujetos = len(contactos['sub/num'].unique())\n",
    "print(\"The total number of subjects is {}\".format(sujetos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_alteri = [\"Afrq\",\"Aol2\",\"Apro\",\"Arel\"]\n",
    "for col in cols_alteri:\n",
    "    dic_col = dict(zip(df[col].unique(),range(len(df[col].unique()))))\n",
    "    df[col].replace(dic_col,inplace=True)\n",
    "\n",
    "\n",
    "dic_smok ={'no':1,'exfumador':2,'ocasional':3,'diario':3,'dk':5,'si':3}\n",
    "dic_race = {'blanco/a':1,'moreno/a o mestizo/a':2,'negro/a':3,'otro/a':4,'dk':5}\n",
    "dic_sex = {'hombre':1,'mujer':2,'dk':3,'refused':3}\n",
    "\n",
    "\n",
    "df[\"Asmo\"].replace(dic_smok,inplace=True)\n",
    "df[\"Arac\"].replace(dic_race,inplace=True)\n",
    "df[\"Asex\"].replace(dic_sex,inplace=True)\n",
    "cols_alteri +=[\"alter/origin\",\"alter/residence\",\"Clos\",\"Asmo\",\"Arac\",\"Asex\",\"alter/num\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"blue\">\n",
    "The assortative attributes that have been added for this case are: frecuency of contact, age, proximity, type of relationship, origin of alteri, residence of alteri, closeness of alteri, smoke habit of alteri, race of alteri, sex of the alteri. The number is deleted below. We study the preferential attachment with these node attributes. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_alteri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the ego networks and measuring their properties \n",
    "\n",
    "At this point, we build the ego networks. Coming from the contacts dataframe, we build a _networkx_ graph and we go through all the subjects, one by one, introducing the ties between alteris as links between nodes. And the intensity of their relationships determines the weight associated with the link. \n",
    "<br> <br>\n",
    "Once we have these graphs, we compute different properties: _Average degree, betweenness, closeness, clustering, load centrality, size of the largest component, number of components.._ Apart from these structural measures, we also compute the average intensity with people from their origin country and people from their residence country and their presence (the total number in the ego network). We store all these values in different lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###We create the list in order to store the ego networks and their different properties. \n",
    "rel_proxy = [ 'pareja', 'sangre', 'matrimonio',\n",
    "        'centro de culto','juventud']\n",
    "\n",
    "graficas=[0]*sujetos\n",
    "avdeg=[0]*sujetos\n",
    "betw=[0]*sujetos\n",
    "closs=[0]*sujetos\n",
    "assort=[0]*sujetos\n",
    "clustering=[0]*sujetos\n",
    "load=[0]*sujetos\n",
    "size=[0]*sujetos\n",
    "comp=[0]*sujetos\n",
    "ori=[0]*sujetos\n",
    "num=[0]*sujetos\n",
    "res=[0]*sujetos\n",
    "closnac=[0]*sujetos\n",
    "closnonac=[0]*sujetos\n",
    "numnac=[0]*sujetos\n",
    "numnonac=[0]*sujetos\n",
    "\n",
    "assort_atr = np.empty((sujetos,len(cols_alteri)-1))\n",
    "\n",
    "\n",
    "mu=[0]*sujetos\n",
    "vect=[[0]*5 for i in range(sujetos)]\n",
    "\n",
    "###We compute these ego networks and their attributes\n",
    "\n",
    "for k,j in enumerate(contactos[\"sub/num\"].unique()):\n",
    "    graficas[k]=nx.Graph()\n",
    "    ###We select the data that corresponds to each subject\n",
    "    data=contactos[contactos['sub/num']==j]\n",
    "    if (len(data)>0):\n",
    "        ###We compute the different number of subjects and their intensity. \n",
    "        ###This will be used to calculate the Dunbar parameters. \n",
    "        data_sub =df[cols_alteri][df[\"sub/num\"]==j]\n",
    "        datamu=df['Clos'][df['sub/num']==j].value_counts()\n",
    "        for m in range(0,len(datamu)):\n",
    "            vect[k][datamu.index[m]-1]= datamu[datamu.index[m]]\n",
    "        ###Building the ego networks\n",
    "        edges=list(zip(data['Alter'],data['Alter2'],data['Value']))\n",
    "        graficas[k].add_weighted_edges_from(edges,'Value')\n",
    "        for col in cols_alteri:\n",
    "            nx.set_node_attributes(graficas[k],dict(zip(data_sub[\"alter/num\"],data_sub[col])),col)\n",
    "        ###Measuring the networks\n",
    "        degrees=[val for (node, val) in graficas[k].degree()]\n",
    "        avdeg[k]=sum(degrees)/len(degrees)\n",
    "        betw[k]=sum(nx.betweenness_centrality(graficas[k],weight='Value').values())/len(nx.betweenness_centrality(graficas[k]).values())\n",
    "        closs[k]=sum(nx.closeness_centrality(graficas[k]).values())/len(nx.closeness_centrality(graficas[k]).values())\n",
    "        load[k]=sum(nx.load_centrality(graficas[k],weight='Value').values())/len(nx.load_centrality(graficas[k]).values())\n",
    "        assort[k]=nx.degree_assortativity_coefficient(graficas[k],weight=\"Value\")\n",
    "        size[k]= len(max(nx.connected_components(graficas[k]), key=len))/len(graficas[k].nodes())\n",
    "        clustering[k]=nx.average_clustering(graficas[k],weight='Value')\n",
    "        \n",
    "        comp[k]=nx.number_connected_components(graficas[k])\n",
    "        ego_origin = df['sub/origin'][df['sub/num'] == j].unique()[0]\n",
    "        ego_residence = df['sub/residence'][df['sub/num'] == j].unique()[0]\n",
    "        closnac[k]=df['Clos'][(df['sub/num'] == j) & (df['alter/origin'] == ego_origin)].mean() \n",
    "        numnac[k]=df['Clos'][(df['sub/num'] == j) & (df['alter/origin'] == ego_origin)].count()\n",
    "        closnonac[k]=df['Clos'][(df['sub/num'] == j) & (df['alter/origin'] == ego_residence)].mean()\n",
    "        numnonac[k]=df['Clos'][(df['sub/num'] == j) & (df['alter/origin'] == ego_residence)].count()\n",
    "        for k1 in range(len(cols_alteri)-1):\n",
    "            assort_atr[k,k1] = nx.attribute_assortativity_coefficient(graficas[k],cols_alteri[k1])\n",
    "        ###\n",
    "        ori[k]=ego_origin\n",
    "        num[k]=j\n",
    "        res[k]=ego_residence\n",
    "    else: print(j)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the ego networks dataframe \n",
    "\n",
    "Coming from the previous lists, we create the networks dataframe, that contains the ego networks and their attributes. At this point, we also compute the Dunbar's parameter $\\mu$ and add it to our dataframe, with its confidence interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Creation of the dataframe\n",
    "redes=pd.DataFrame(data={'Subject origin':ori,'Subject residence':res,'Subject ID':num,'Vect':vect,'Graphs':graficas,\n",
    "                         'Average degree':avdeg,'Betweenness':betw,'Closeness':closs,'Load centrality':load,\n",
    "                         'Assortativity':assort,'Clustering':clustering,\n",
    "                         'Number components':comp,'Size largest component':size,\n",
    "                         'Closeness residence':closnonac,'Closeness origin':closnac,'Number residence':numnonac,\n",
    "                         'Number origin':numnac,\n",
    "                        })\n",
    "df.isnull().sum()\n",
    "redes.fillna(-2,inplace=True)\n",
    "###Calculating the Dunbar's parameters \n",
    "redes['Fitted']=redes['Vect'].apply(lambda x:Individual(x).fit_model())\n",
    "redes[['Mu','Mu-','Mu+']] = pd.DataFrame(redes.Fitted.values.tolist(), index= redes.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some changes in the dataframe \n",
    "\n",
    "At this point, we make some changes in the ego networks dataframe. We introduce a function that substitutes the confidence interval of the $\\mu$ by an string that determines the regime. Then we delete the columns associated with the confidence interval, fill the _NaN_ and rename some columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Creating function to make clearer the regime\n",
    "def rex(x,y):\n",
    "    if ((x < 0) & (y < 0)): z='Inverted'\n",
    "    elif ((x > 0) & (y > 0)): z='Standard'\n",
    "    else : z='Unclear'\n",
    "    return z\n",
    "\n",
    "###Introducing the former function in our dataframe\n",
    "regime=[0]*sujetos\n",
    "for i in range(len(redes)):\n",
    "    regime[i]=rex(redes['Mu-'][i],redes['Mu+'][i])\n",
    "redes['Regime']=regime\n",
    "del redes['Fitted']\n",
    "del redes['Mu-']\n",
    "del redes['Mu+']\n",
    "\n",
    "###Filling NaNs\n",
    "#redes.fillna(0,inplace=True)\n",
    "\n",
    "###Renaming columns and getting rid of the networkx graph\n",
    "redes=redes[['Subject ID','Subject origin','Subject residence','Mu','Regime','Average degree','Betweenness',\n",
    "             'Closeness','Load centrality','Assortativity','Clustering',\n",
    "             'Number components','Size largest component',\n",
    "             'Closeness residence','Number residence','Closeness origin','Number origin',',\n",
    "             'Graphs']]\n",
    "del redes['Graphs']\n",
    "\n",
    "redes = pd.concat([redes, pd.DataFrame(assort_atr)], axis=1)\n",
    "redes = redes.rename(columns = dict(zip(redes.columns[-len(cols_alteri)+1:],cols_alteri[:-1])))\n",
    "redes.fillna(-2,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More changes in the networks dataframe and merging \n",
    "\n",
    "Now we are going to merge the network measures dataframe with the original one _df_, that contains information about egos and alteris. We want to preserve the egos attributes, so we delete all the information about the alteris. We will also introduce the variable _FMIG2_ that measures the number of years these migrants have spent in their residence countries. Once we have made all these changes, some typos need to be corrected, and that is what we do at the end of this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Deleting the alteri column in df and merging it with the networks dataframe\n",
    "df.drop([\"alter/num\",\"alter/origin\",\"alter/residence\",\"Clos\"],axis=1,inplace=True)\n",
    "df.drop([col for col in df.columns if col[0] == \"A\"],axis=1,inplace=True)\n",
    "df.rename(columns={'sub/num':'Subject ID','sub/origin':'Subject origin','sub/residence':'Subject residence'},inplace=True)\n",
    "df = df.drop_duplicates(['Subject origin','Subject ID'])\n",
    "\n",
    "redes2= pd.merge(redes, df,how='left',on=['Subject ID','Subject origin','Subject residence'])\n",
    "redes2.drop(\"Subject ID\",axis=1,inplace=True)\n",
    "\n",
    "###Calculating FMIG2\n",
    "import datetime\n",
    "FMIG2=[0]*len(redes2)\n",
    "for i in range(len(redes2)):\n",
    "    if (redes2['FMIG'].iloc[i]<0.5): \n",
    "        FMIG2[i]=0\n",
    "    elif (redes2['FMIG'].iloc[i]>1500):\n",
    "        FMIG2[i] = 2005 - redes2['FMIG'].iloc[i]\n",
    "redes2['FMIG2']=FMIG2\n",
    "redes2.drop(\"FMIG\",axis=1,inplace=True)\n",
    "\n",
    "###Renaming columns and changing some typos\n",
    "redes2.rename(columns={'Subject origin':'Subject_origin','Subject residence':'Subject_residence'},inplace=True)\n",
    "redes2.columns = redes2.columns.str.replace(' ', '_')\n",
    "redes2.dropna(axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the final results and display a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redes2.to_csv(\"Redes_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " nx.attribute_assortativity_coefficient(graficas[k],cols_alteri[k1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(data_sub[\"alter/num\"],data_sub[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
