{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "# Sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Statsmodels\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.api import MNLogit\n",
    "\n",
    "# Just to print prettier. Uncomment to see all (not important) warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_origin</th>\n",
       "      <th>Subject_residence</th>\n",
       "      <th>Mu</th>\n",
       "      <th>Regime</th>\n",
       "      <th>Average_degree</th>\n",
       "      <th>Betweenness</th>\n",
       "      <th>Closeness</th>\n",
       "      <th>Load_centrality</th>\n",
       "      <th>Assortativity</th>\n",
       "      <th>Clustering</th>\n",
       "      <th>...</th>\n",
       "      <th>EMPLOY</th>\n",
       "      <th>EDUCATIO</th>\n",
       "      <th>ASMOKE</th>\n",
       "      <th>MOS</th>\n",
       "      <th>AOS</th>\n",
       "      <th>ACCULTUR</th>\n",
       "      <th>ALEVEL</th>\n",
       "      <th>ego_language</th>\n",
       "      <th>alter_language</th>\n",
       "      <th>FMIG2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>co</td>\n",
       "      <td>usa</td>\n",
       "      <td>-0.180268</td>\n",
       "      <td>Unclear</td>\n",
       "      <td>34.711111</td>\n",
       "      <td>0.004910</td>\n",
       "      <td>0.839868</td>\n",
       "      <td>0.004910</td>\n",
       "      <td>-0.204603</td>\n",
       "      <td>0.780457</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.31111</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>co</td>\n",
       "      <td>usa</td>\n",
       "      <td>-0.011112</td>\n",
       "      <td>Unclear</td>\n",
       "      <td>33.863636</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.828040</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>-0.024450</td>\n",
       "      <td>0.644311</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.73333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>co</td>\n",
       "      <td>usa</td>\n",
       "      <td>0.727070</td>\n",
       "      <td>Standard</td>\n",
       "      <td>4.648649</td>\n",
       "      <td>0.029086</td>\n",
       "      <td>0.203734</td>\n",
       "      <td>0.029086</td>\n",
       "      <td>-0.245212</td>\n",
       "      <td>0.686175</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.66667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>co</td>\n",
       "      <td>usa</td>\n",
       "      <td>-0.157215</td>\n",
       "      <td>Unclear</td>\n",
       "      <td>15.422222</td>\n",
       "      <td>0.016655</td>\n",
       "      <td>0.596000</td>\n",
       "      <td>0.016655</td>\n",
       "      <td>0.077990</td>\n",
       "      <td>0.651957</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.00000</td>\n",
       "      <td>0.75556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>co</td>\n",
       "      <td>usa</td>\n",
       "      <td>-0.239059</td>\n",
       "      <td>Inverted</td>\n",
       "      <td>29.244444</td>\n",
       "      <td>0.007799</td>\n",
       "      <td>0.768654</td>\n",
       "      <td>0.007799</td>\n",
       "      <td>-0.017031</td>\n",
       "      <td>0.705817</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.00000</td>\n",
       "      <td>0.97778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Subject_origin Subject_residence        Mu    Regime  Average_degree  \\\n",
       "0             co               usa -0.180268   Unclear       34.711111   \n",
       "1             co               usa -0.011112   Unclear       33.863636   \n",
       "2             co               usa  0.727070  Standard        4.648649   \n",
       "3             co               usa -0.157215   Unclear       15.422222   \n",
       "4             co               usa -0.239059  Inverted       29.244444   \n",
       "\n",
       "   Betweenness  Closeness  Load_centrality  Assortativity  Clustering  ...  \\\n",
       "0     0.004910   0.839868         0.004910      -0.204603    0.780457  ...   \n",
       "1     0.005059   0.828040         0.005059      -0.024450    0.644311  ...   \n",
       "2     0.029086   0.203734         0.029086      -0.245212    0.686175  ...   \n",
       "3     0.016655   0.596000         0.016655       0.077990    0.651957  ...   \n",
       "4     0.007799   0.768654         0.007799      -0.017031    0.705817  ...   \n",
       "\n",
       "   EMPLOY  EDUCATIO   ASMOKE  MOS  AOS  ACCULTUR ALEVEL ego_language  \\\n",
       "0    14.0   0.31111  0.00000  0.0  0.0       0.0    0.0           es   \n",
       "1    33.0   0.73333  0.00000  0.0  0.0       0.0    0.0           es   \n",
       "2    30.0   0.66667  0.00000  0.0  0.0       0.0    0.0           es   \n",
       "3     NaN  34.00000  0.75556  0.0  0.0       0.0    0.0           es   \n",
       "4     NaN  44.00000  0.97778  0.0  0.0       0.0    0.0           es   \n",
       "\n",
       "  alter_language FMIG2  \n",
       "0             es   0.0  \n",
       "1             es   0.0  \n",
       "2             es   0.0  \n",
       "3             es   0.0  \n",
       "4             es   0.0  \n",
       "\n",
       "[5 rows x 384 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('Redes_2.csv')\n",
    "# Drop Unnecessary Variables\n",
    "df.drop(['Unnamed: 0', 'Subject_num'],axis=1, inplace=True)\n",
    "# Have a look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_origin</th>\n",
       "      <th>Subject_residence</th>\n",
       "      <th>Mu</th>\n",
       "      <th>Regime</th>\n",
       "      <th>Average_degree</th>\n",
       "      <th>Betweenness</th>\n",
       "      <th>Closeness</th>\n",
       "      <th>Load_centrality</th>\n",
       "      <th>Assortativity</th>\n",
       "      <th>Clustering</th>\n",
       "      <th>...</th>\n",
       "      <th>EMPLOY</th>\n",
       "      <th>EDUCATIO</th>\n",
       "      <th>ASMOKE</th>\n",
       "      <th>MOS</th>\n",
       "      <th>AOS</th>\n",
       "      <th>ACCULTUR</th>\n",
       "      <th>ALEVEL</th>\n",
       "      <th>ego_language</th>\n",
       "      <th>alter_language</th>\n",
       "      <th>FMIG2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>473</td>\n",
       "      <td>473</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>473</td>\n",
       "      <td>473</td>\n",
       "      <td>473.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>do</td>\n",
       "      <td>sp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unclear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>154</td>\n",
       "      <td>282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.743170</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.921957</td>\n",
       "      <td>0.015368</td>\n",
       "      <td>0.687610</td>\n",
       "      <td>0.014957</td>\n",
       "      <td>-0.018617</td>\n",
       "      <td>0.639269</td>\n",
       "      <td>...</td>\n",
       "      <td>1.909032</td>\n",
       "      <td>0.831709</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.078587</td>\n",
       "      <td>0.016147</td>\n",
       "      <td>0.165787</td>\n",
       "      <td>0.197922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.139535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.524199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.589893</td>\n",
       "      <td>0.011334</td>\n",
       "      <td>0.217223</td>\n",
       "      <td>0.011679</td>\n",
       "      <td>0.213724</td>\n",
       "      <td>0.169064</td>\n",
       "      <td>...</td>\n",
       "      <td>7.332452</td>\n",
       "      <td>5.092979</td>\n",
       "      <td>2.239957</td>\n",
       "      <td>0.350677</td>\n",
       "      <td>0.119575</td>\n",
       "      <td>0.803888</td>\n",
       "      <td>0.740157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>396.184672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-294.081935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.628571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130665</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.695654</td>\n",
       "      <td>0.205819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.299994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.818182</td>\n",
       "      <td>0.006953</td>\n",
       "      <td>0.532497</td>\n",
       "      <td>0.005732</td>\n",
       "      <td>-0.139165</td>\n",
       "      <td>0.505203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.111711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.066667</td>\n",
       "      <td>0.014024</td>\n",
       "      <td>0.632030</td>\n",
       "      <td>0.013907</td>\n",
       "      <td>-0.004029</td>\n",
       "      <td>0.659861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100436</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.666667</td>\n",
       "      <td>0.020487</td>\n",
       "      <td>0.938077</td>\n",
       "      <td>0.020487</td>\n",
       "      <td>0.024567</td>\n",
       "      <td>0.770016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.302179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.974478</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows Ã— 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Subject_origin Subject_residence          Mu   Regime  Average_degree  \\\n",
       "count             473               473  473.000000      473      473.000000   \n",
       "unique             10                 2         NaN        3             NaN   \n",
       "top                do                sp         NaN  Unclear             NaN   \n",
       "freq              154               282         NaN      222             NaN   \n",
       "mean              NaN               NaN   -0.743170      NaN       23.921957   \n",
       "std               NaN               NaN   13.524199      NaN       13.589893   \n",
       "min               NaN               NaN -294.081935      NaN        2.628571   \n",
       "25%               NaN               NaN   -0.299994      NaN       12.818182   \n",
       "50%               NaN               NaN   -0.111711      NaN       19.066667   \n",
       "75%               NaN               NaN    0.100436      NaN       40.666667   \n",
       "max               NaN               NaN    2.302179      NaN       44.000000   \n",
       "\n",
       "        Betweenness   Closeness  Load_centrality  Assortativity  Clustering  \\\n",
       "count    473.000000  473.000000       473.000000     473.000000  473.000000   \n",
       "unique          NaN         NaN              NaN            NaN         NaN   \n",
       "top             NaN         NaN              NaN            NaN         NaN   \n",
       "freq            NaN         NaN              NaN            NaN         NaN   \n",
       "mean       0.015368    0.687610         0.014957      -0.018617    0.639269   \n",
       "std        0.011334    0.217223         0.011679       0.213724    0.169064   \n",
       "min        0.000000    0.130665         0.000000      -0.695654    0.205819   \n",
       "25%        0.006953    0.532497         0.005732      -0.139165    0.505203   \n",
       "50%        0.014024    0.632030         0.013907      -0.004029    0.659861   \n",
       "75%        0.020487    0.938077         0.020487       0.024567    0.770016   \n",
       "max        0.078431    1.000000         0.078431       0.974478    1.000000   \n",
       "\n",
       "        ...      EMPLOY    EDUCATIO      ASMOKE         MOS         AOS  \\\n",
       "count   ...  171.000000  171.000000  152.000000  151.000000  139.000000   \n",
       "unique  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "top     ...         NaN         NaN         NaN         NaN         NaN   \n",
       "freq    ...         NaN         NaN         NaN         NaN         NaN   \n",
       "mean    ...    1.909032    0.831709    0.276316    0.078587    0.016147   \n",
       "std     ...    7.332452    5.092979    2.239957    0.350677    0.119575   \n",
       "min     ...    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%     ...    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%     ...    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%     ...    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "max     ...   44.000000   44.000000   27.000000    3.000000    1.000000   \n",
       "\n",
       "          ACCULTUR      ALEVEL ego_language alter_language        FMIG2  \n",
       "count   139.000000  139.000000          473            473   473.000000  \n",
       "unique         NaN         NaN            2              2          NaN  \n",
       "top            NaN         NaN           es             es          NaN  \n",
       "freq           NaN         NaN          368            368          NaN  \n",
       "mean      0.165787    0.197922          NaN            NaN    83.139535  \n",
       "std       0.803888    0.740157          NaN            NaN   396.184672  \n",
       "min       0.000000    0.000000          NaN            NaN     0.000000  \n",
       "25%       0.000000    0.000000          NaN            NaN     0.000000  \n",
       "50%       0.000000    0.000000          NaN            NaN     0.000000  \n",
       "75%       0.000000    0.000000          NaN            NaN     0.000000  \n",
       "max       6.000000    5.000000          NaN            NaN  2019.000000  \n",
       "\n",
       "[11 rows x 384 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some values of `mu` are way out of range (min = -294). This is clearly from divergences in the model. We mark observations greater than 10 (in absolute value) as `nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean estimates for mu\n",
    "df['Mu'] = df['Mu'].apply(lambda x: np.nan if x < -10 else x)\n",
    "df['Mu'] = df['Mu'].apply(lambda x: np.nan if x > 10 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subject_origin         0\n",
       "Subject_residence      0\n",
       "Mu                     1\n",
       "Regime                 0\n",
       "Average_degree         0\n",
       "                    ... \n",
       "ACCULTUR             334\n",
       "ALEVEL               334\n",
       "ego_language           0\n",
       "alter_language         0\n",
       "FMIG2                  0\n",
       "Length: 384, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of nans in the data\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply remove the observation having a nan (more sophisticated approaches could be done, as replacing its value with the median `mu` of the individuals in his same class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df, hue=\"Subject_origin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Notes***\n",
    "<ul>\n",
    "    <li> Interesting sigmoid relationship closeness origin ~ mu</li>\n",
    "    <li> Presence of severe collinearities in the data (this may cause numeric problems in linear models)</li>\n",
    "    <li> The conditional distributions show clear differences in some of the variables (Number origin, Average degree, clustering)</li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group some nationalities in `others` group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are few data on several Origins\n",
    "count_origins = pd.get_dummies(df['Subject_origin']).sum()\n",
    "print(count_origins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We keep only classes with more than 50 observations</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 50 # threshold\n",
    "df['Subject_origin'] = df['Subject_origin'].apply(lambda x: 'aa-other' if (count_origins[x] < t) else x)\n",
    "pd.get_dummies(df['Subject_origin']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df, hue=\"Subject_origin\")\n",
    "plt.savefig('pair_plot.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu vs closseness origin\n",
    "sns.scatterplot(x='Mu',y='Closeness_origin', data = df,hue=\"Subject_origin\");\n",
    "plt.savefig('sigmoid_mu_clos.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting sigmoid relationship for further exploring (fitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference for interpreting results\n",
    "\n",
    "https://stats.idre.ucla.edu/stata/dae/multinomiallogistic-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('List of variables in data. The target is Subject_origin')\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: For model selection it is important to know how to justify the predictors we use. So far, I have used a somewhat ad-hoc selection. Using all predictors causes numerical errors (most likely due to collinearity). What I did was removing some until I got a model with interesting variables and no numerical errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['Closeness','Closeness_residence','Closeness_origin','Clustering','Mu','Average_degree',\n",
    "             'Assortativity', 'Betweenness']\n",
    "\n",
    "#all_predictors = df.columns[2:] # do not include Subject_residence\n",
    "g = sns.pairplot(df[predictors + [\"Subject_origin\"]] , hue=\"Subject_origin\")\n",
    "# plt.savefig('pair_plot.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Multinomial Logistic Model\n",
    "\n",
    "https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.MNLogit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the list 'predictors' as independent variables\n",
    "formula_predictors = ' + '.join(predictors)\n",
    "model = MNLogit.from_formula('Subject_origin ~ {}'.format(formula_predictors), df)\n",
    "results = model.fit(maxiter=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('pseudo r-squared = {}'.format(np.round(results.prsquared,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"<i>While the R2 index is a more familiar concept to planner who are experienced in OLS, it is not as well behaved as the rho-squared measure, for ML estimation. Those unfamiliar with rho-squared should be forewarned that its values tend to be considerably lower than those of the R2 index...For example, values of 0.2 to 0.4 for rho-squared represent EXCELLENT fit.</i>\"\n",
    "\n",
    "https://stats.stackexchange.com/questions/82105/mcfaddens-pseudo-r2-interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['ar', 'do', 'ma', 'pu', 'se']\n",
    "# Note: the reference value is 'aa-others'\n",
    "estimated_odds_ratios = pd.DataFrame(results.params[results.pvalues < 0.05].values, \n",
    "                         columns = names,\n",
    "                         index= results.params.index).apply(lambda x: np.exp(x))\n",
    "estimated_odds_ratios # i.e. pr(cat=ar) / pr(cat=aa-other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in the table above are the odds ratios for the significant predictors (p-value < 0.05). They mean that one unit increase in that variable ($x_1$) increases by that factor ($\\beta_1$) the odds ratio of the observation belonging to that class (i.e. `ar`) with respect to the reference class (`others`). $$ Pr(ar) / Pr(other) \\sim \\beta_1*x_1  $$\n",
    "\n",
    "**Note:** They can (and should) be further explored to understand the relevant factors that differentiate the nationalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.llr_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result is the chi-squared probability of getting a log-likelihood ratio statistic greater than llr. llr has a chi-squared distribution with degrees of freedom df_model. The likelihood ratio chi-square with a p-value ~ 0 tells us that our model as a whole fits significantly better than an empty model (i.e., a model with no predictors). See the following links for more details:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/82105/mcfaddens-pseudo-r2-interpretation\n",
    "\n",
    "https://www.statsmodels.org/devel/generated/statsmodels.discrete.discrete_model.DiscreteResults.prsquared.html#statsmodels.discrete.discrete_model.DiscreteResults.prsquared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTION\n",
    "\n",
    "We train and fit a powerful non-linear (and non-parametric) machine learnin classifier to the data; a Random Forest. There are many other alternatives, but tree based metods are very powerfull and there are new techniques to help identify relevant predictors.\n",
    "\n",
    "In this section, we want to test wether this model can outperform significantly other null (dummy) classifiers. If that is the case (which it is), it confirms the hypothesis that the predictors have relevant information about the nationalities of the subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Training and Test sets from data\n",
    "# and splitting the data into independent and dependent variables\n",
    "\n",
    "predictors = ['Closeness','Closeness_residence','Closeness_origin','Clustering','Mu','Average_degree',\n",
    "             'Assortativity', 'Betweenness'] # The same used in the MNL model above.\n",
    "\n",
    "y = df['Subject_origin'] # target variable\n",
    "X = df[predictors]       # independent variables\n",
    "\n",
    "test_size = 0.20 #maybe more is needed (20% is standard though)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standar Scaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform (X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and tune the model using k-cross fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = 'accuracy' #'f1_macro' # This chooses the metric to optimise during training (there are others!)\n",
    "njobs=-1                         # This the number of cores used in your cpu (-1 means \"all of them\")\n",
    "cv=5                             # the k in k-cross-fold validation\n",
    "# RANDOM FOREST\n",
    "print('\\nFitting Random Forest\\n')\n",
    "\n",
    "rfc=RandomForestClassifier(random_state=0)\n",
    "# Parameter combinations to explore\n",
    "param_grid = { \n",
    "    'n_estimators': [75, 100,300,1000],\n",
    "    'max_features': ['auto', None],\n",
    "    'min_samples_split' :[2,6, 10, 14],\n",
    "    'max_depth' : [10, 15, 30, 50,None],\n",
    "    'max_samples' : [0.5 ,0.7, None],}\n",
    "\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring = scoring,\n",
    "                  verbose=1,\n",
    "                  n_jobs=njobs,\n",
    "                  cv= cv)\n",
    "CV_rfc.fit(X_train, y_train)\n",
    "\n",
    "print('\\nRandom Forest:')\n",
    "print('Best Score: ', CV_rfc.best_score_)\n",
    "print('Best Params: ', CV_rfc.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the algorithm performance in the test set (unseen data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = CV_rfc.predict(X_test)\n",
    "print('Confusion Matrix:\\n ', confusion_matrix(y_test,y_pred),'\\n')\n",
    "print(classification_report(y_test,y_pred),'\\n')\n",
    "print('Accuracy: {0:.2f}'.format(accuracy_score(y_test, y_pred),2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare this performance with  null models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  relative prevalence of each class\n",
    "rel_prev = (y.value_counts() / len(y))\n",
    "print(rel_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform Dummy Classifier (classifies randomly with p = 1/6)\n",
    "\n",
    "# If the classifier randomly guesses: \n",
    "print('Acurracy of uniform dummy classifier: ',(((1/6) * y.value_counts()) / len(y)).sum()) # = 1/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Dummy Classifier (classifies randomly with p ~ prevalence of each class)\n",
    "print('Acurracy of stratified dummy classifier: ',(rel_prev * y.value_counts()).sum() / len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent Dummy Classifier (classifies always in the most frequent class)\n",
    "print('Acurracy of Most freq dummy classifier: ',rel_prev.max() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLEARN versions of the dummy classifiers (to double check and for convinience methods)\n",
    "\n",
    "dummy = \"stratified\"# most_frequent, stratified, uniform\n",
    "dummy_clf = DummyClassifier(strategy=dummy,random_state=0) \n",
    "\n",
    "#Mean accuracy null model (to check my numbers above)\n",
    "#dummy_score = 0\n",
    "#for _ in range(1000):\n",
    "#    dummy_clf.fit(X, y)\n",
    "#    dummy_score += dummy_clf.score(X, y)\n",
    "#dummy_score = dummy_score / 1000   \n",
    "\n",
    "# Actual accuracy of the dummy in the same train-test split as the RF model\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_score = dummy_clf.score(X_test, y_test)\n",
    "print('Mean accuracy of null ' + dummy +' model: {0:.2f}'.format(dummy_score),'\\n')\n",
    "print('Mean accuracy (in test) of RF model: {0:.2f}'.format(CV_rfc.score(X_test, y_test)),'\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix and report of the selected dummy classifier\n",
    "\n",
    "y_pred_dummy = dummy_clf.predict(X_test)\n",
    "print('Confusion Matrix:\\n\\n ',confusion_matrix(y_test,y_pred_dummy),'\\n')\n",
    "print(classification_report(y_test,y_pred_dummy),'\\n')\n",
    "print('Accuracy: {0:.2f}'.format(accuracy_score(y_test, y_pred_dummy),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for reference, the results of the RF Model\n",
    "\n",
    "y_pred = CV_rfc.predict(X_test)\n",
    "print('Confusion Matrix:\\n\\n ', confusion_matrix(y_test,y_pred),'\\n')\n",
    "print(classification_report(y_test,y_pred),'\\n')\n",
    "print('Accuracy: {0:.2f}'.format(accuracy_score(y_test, y_pred),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_report = pd.DataFrame(classification_report(y_test,dummy_clf.predict(X_test), output_dict= True))\n",
    "\n",
    "rfc_report = pd.DataFrame(classification_report(y_test,CV_rfc.predict(X_test), output_dict= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increase in prediction power (percentage with respect to null model)\n",
    "\n",
    "i.e. 100% means twice as good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table = ((rfc_report - dummy_report)*100 / dummy_report).drop('support').round(decimals=2)\n",
    "final_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This significant increases further support the claim that the predictors (based on ego-network properties) have useful information to predict the countries of origin of the individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work (ideas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Learning curves: could explore how getting more data would impact (improve) prediction results</li>\n",
    "    <li>We could train other models (neural networks, SVM, etc) but I think the point has been made</li>\n",
    "    <li>Feature engeneering. We should focus on feedback from Jose Luis to see what features are more interesting to explore. The analyses I present here focus on a somewhat arbitrary choice.</li>\n",
    "    <li>Feature importance and interpreation: Random Forest are amenable to work on the relative importance of each of the features for each of the labels---see SHAP values for example. This together with the results from the multinomial LR may help to explain the results (and build a narrative)</li>\n",
    "        \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
