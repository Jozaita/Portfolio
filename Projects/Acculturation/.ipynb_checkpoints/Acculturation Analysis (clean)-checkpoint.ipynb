{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "# Sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Statsmodels\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.api import MNLogit\n",
    "\n",
    "\n",
    "\n",
    "# Just to print prettier. Uncomment to see all (not important) warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Subject_num'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2ae61ab05ba7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Redes_2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Drop Unnecessary Variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Subject_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EDUC'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EDUC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3995\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3996\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3997\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3998\u001b[0m         )\n\u001b[1;32m   3999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3934\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3935\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3936\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3968\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3970\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3971\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5016\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5017\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5018\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5019\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5020\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Subject_num'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "df_2 = pd.read_csv('Redes_2.csv')\n",
    "# Drop Unnecessary Variables\n",
    "df_2.drop(['Unnamed: 0','Subject_num'],axis=1, inplace=True)\n",
    "df = df_2[df_2.columns[0:16]]\n",
    "df['EDUC'] = df_2['EDUC'].copy()\n",
    "df['FMIG2'] = df_2['FMIG'].copy()\n",
    "df['SEX'] = df_2['SEX'].copy()\n",
    "\n",
    "not_apply = ['Subject_origin','Subject_residence','Regime']\n",
    "diccs = [0]*len(not_apply)\n",
    "i = 0\n",
    "for col in not_apply: \n",
    "        uniques = list(df[col].unique()) \n",
    "        print(col)\n",
    "        print(uniques)\n",
    "        diccs[i] = {uniques[j]:uniques.index(uniques[j]) for j in range(len(uniques)) }\n",
    "        df[col] = df[col].map(diccs[i])\n",
    "        i+=1\n",
    "df['Subject_origin'].astype('int64')\n",
    "df['Subject_residence'].astype('int64')\n",
    "df['Regime'].astype('int64')\n",
    "df['Subject_origin'] = pd.to_numeric(df['Subject_origin'], errors='ignore').astype('int64')\n",
    "\n",
    "##Restore the FMIG problems\n",
    "for i in range(len(df)):\n",
    "    if df['FMIG2'].iloc[i] > 1750:\n",
    "        df['FMIG2'].iloc[i] -= 2020\n",
    "        df['FMIG2'].iloc[i] = abs(df['FMIG2'].iloc[i])\n",
    "\n",
    "\n",
    "# Have a look at the data\n",
    "#df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(df)\n",
    "df.groupby(by=['Subject_origin','EDUC']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some values of `mu` are way out of range (min = -294). This is clearly from divergences in the model. We mark observations greater than 10 (in absolute value) as `nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean estimates for mu\n",
    "df['Mu'] = df['Mu'].apply(lambda x: np.nan if x < -100 else x)\n",
    "df['Mu'] = df['Mu'].apply(lambda x: np.nan if x > 100 else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nans in the data\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply remove the observation having a nan (more sophisticated approaches could be done, as replacing its value with the median `mu` of the individuals in his same class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.pairplot(df, hue=\"Subject_origin\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Notes***\n",
    "<ul>\n",
    "    <li> Interesting sigmoid relationship closeness origin ~ mu</li>\n",
    "    <li> Presence of severe collinearities in the data (this may cause numeric problems in linear models)</li>\n",
    "    <li> The conditional distributions show clear differences in some of the variables (Number origin, Average degree, clustering)</li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group some nationalities in `others` group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are few data on several Origins\n",
    "count_origins = pd.get_dummies(df['Subject_origin']).sum()\n",
    "print(count_origins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We keep only classes with more than 50 observations</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 50 # threshold\n",
    "df['Subject_origin'] = df['Subject_origin'].apply(lambda x: 10 if (count_origins[x] < t) else x)\n",
    "pd.get_dummies(df['Subject_origin']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Subject_origin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc_traslation = {10:0,2:1,5:2,6:3,8:4,9:5}\n",
    "df['Subject_origin'] = df['Subject_origin'].map(dicc_traslation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns: \n",
    "    print(df[col].dtypes,col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.pairplot(df, hue=\"Subject_origin\")\n",
    "#plt.savefig('pair_plot.pdf',dpi=600)\n",
    "#plt.show()\n",
    "#df['Subject_origin'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu vs closseness origin\n",
    "#df['Subject_origin'] = pd.to_numeric(df['Subject_origin'], errors='ignore').astype('Int64')\n",
    "sns.scatterplot(x='Mu',y='Closeness_origin', data = df,hue=\"Subject_origin\");\n",
    "plt.savefig('sigmoid_mu_clos.pdf',dpi=600)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting sigmoid relationship for further exploring (fitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference for interpreting results\n",
    "\n",
    "https://stats.idre.ucla.edu/stata/dae/multinomiallogistic-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('List of variables in data. The target is Subject_origin')\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: For model selection it is important to know how to justify the predictors we use. So far, I have used a somewhat ad-hoc selection. Using all predictors causes numerical errors (most likely due to collinearity). What I did was removing some until I got a model with interesting variables and no numerical errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['Closeness','Closeness_residence','Closeness_origin','Clustering','Mu','Average_degree',\n",
    "             'Assortativity', 'Betweenness']\n",
    "\n",
    "#all_predictors = df.columns[2:] # do not include Subject_residence\n",
    "#g = sns.pairplot(df[predictors + [\"Subject_origin\"]] , hue=\"Subject_origin\")\n",
    "# plt.savefig('pair_plot.pdf',dpi=600)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Multinomial Logistic Model\n",
    "\n",
    "https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.MNLogit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the list 'predictors' as independent variables\n",
    "formula_predictors = ' + '.join(predictors)\n",
    "model = MNLogit.from_formula('Subject_origin ~ {}'.format(formula_predictors), df)\n",
    "results = model.fit(maxiter=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Subject_origin ~ {}'.format(formula_predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('pseudo r-squared = {}'.format(np.round(results.prsquared,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"<i>While the R2 index is a more familiar concept to planner who are experienced in OLS, it is not as well behaved as the rho-squared measure, for ML estimation. Those unfamiliar with rho-squared should be forewarned that its values tend to be considerably lower than those of the R2 index...For example, values of 0.2 to 0.4 for rho-squared represent EXCELLENT fit.</i>\"\n",
    "\n",
    "https://stats.stackexchange.com/questions/82105/mcfaddens-pseudo-r2-interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['ar', 'do', 'ma', 'pu', 'se']\n",
    "# Note: the reference value is 'aa-others'\n",
    "estimated_odds_ratios = pd.DataFrame(results.params[results.pvalues < 0.05].values, \n",
    "                         columns = names,\n",
    "                         index= results.params.index).apply(lambda x: np.exp(x))\n",
    "estimated_odds_ratios # i.e. pr(cat=ar) / pr(cat=aa-other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in the table above are the odds ratios for the significant predictors (p-value < 0.05). They mean that one unit increase in that variable ($x_1$) increases by that factor ($\\beta_1$) the odds ratio of the observation belonging to that class (i.e. `ar`) with respect to the reference class (`others`). $$ Pr(ar) / Pr(other) \\sim \\beta_1*x_1  $$\n",
    "\n",
    "**Note:** They can (and should) be further explored to understand the relevant factors that differentiate the nationalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.llr_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result is the chi-squared probability of getting a log-likelihood ratio statistic greater than llr. llr has a chi-squared distribution with degrees of freedom df_model. The likelihood ratio chi-square with a p-value ~ 0 tells us that our model as a whole fits significantly better than an empty model (i.e., a model with no predictors). See the following links for more details:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/82105/mcfaddens-pseudo-r2-interpretation\n",
    "\n",
    "https://www.statsmodels.org/devel/generated/statsmodels.discrete.discrete_model.DiscreteResults.prsquared.html#statsmodels.discrete.discrete_model.DiscreteResults.prsquared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTION\n",
    "\n",
    "We train and fit a powerful non-linear (and non-parametric) machine learnin classifier to the data; a Random Forest. There are many other alternatives, but tree based metods are very powerfull and there are new techniques to help identify relevant predictors.\n",
    "\n",
    "In this section, we want to test wether this model can outperform significantly other null (dummy) classifiers. If that is the case (which it is), it confirms the hypothesis that the predictors have relevant information about the nationalities of the subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Training and Test sets from data\n",
    "# and splitting the data into independent and dependent variables\n",
    "\n",
    "#predictors = ['Closeness','Closeness_residence','Closeness_origin','Clustering','Mu','Average_degree',\n",
    "#             'Assortativity', 'Betweenness','SEX'] # The same used in the MNL model above.\n",
    "\n",
    "y = df['Subject_origin'] # target variable\n",
    "X = df[predictors]       # independent variables\n",
    "\n",
    "test_size = 0.20 #maybe more is needed (20% is standard though)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standar Scaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform (X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and tune the model using k-cross fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = 'accuracy' #'f1_macro' # This chooses the metric to optimise during training (there are others!)\n",
    "njobs=-1                         # This the number of cores used in your cpu (-1 means \"all of them\")\n",
    "cv=5                             # the k in k-cross-fold validation\n",
    "# RANDOM FOREST\n",
    "print('\\nFitting Random Forest\\n')\n",
    "\n",
    "rfc=RandomForestClassifier(random_state=0)\n",
    "# Parameter combinations to explore\n",
    "param_grid = { \n",
    "    'n_estimators': [75, 100,300,1000],\n",
    "    'max_features': ['auto', None],\n",
    "    'min_samples_split' :[2,6, 10, 14],\n",
    "    'max_depth' : [10, 15, 30, 50,None],\n",
    "    'max_samples' : [0.5 ,0.7, None],}\n",
    "\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring = scoring,\n",
    "                  verbose=1,\n",
    "                  n_jobs=njobs,\n",
    "                  cv= cv)\n",
    "CV_rfc.fit(X_train, y_train)\n",
    "\n",
    "print('\\nRandom Forest:')\n",
    "print('Best Score: ', CV_rfc.best_score_)\n",
    "print('Best Params: ', CV_rfc.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the algorithm performance in the test set (unseen data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = CV_rfc.predict(X_test)\n",
    "print('Confusion Matrix:\\n ', confusion_matrix(y_test,y_pred),'\\n')\n",
    "print(classification_report(y_test,y_pred),'\\n')\n",
    "print('Accuracy: {0:.2f}'.format(accuracy_score(y_test, y_pred),2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare this performance with  null models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  relative prevalence of each class\n",
    "rel_prev = (y.value_counts() / len(y))\n",
    "print(rel_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform Dummy Classifier (classifies randomly with p = 1/6)\n",
    "\n",
    "# If the classifier randomly guesses: \n",
    "print('Acurracy of uniform dummy classifier: ',(((1/6) * y.value_counts()) / len(y)).sum()) # = 1/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Dummy Classifier (classifies randomly with p ~ prevalence of each class)\n",
    "print('Acurracy of stratified dummy classifier: ',(rel_prev * y.value_counts()).sum() / len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent Dummy Classifier (classifies always in the most frequent class)\n",
    "print('Acurracy of Most freq dummy classifier: ',rel_prev.max() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLEARN versions of the dummy classifiers (to double check and for convinience methods)\n",
    "\n",
    "dummy = \"stratified\"# most_frequent, stratified, uniform\n",
    "dummy_clf = DummyClassifier(strategy=dummy,random_state=0) \n",
    "\n",
    "#Mean accuracy null model (to check my numbers above)\n",
    "#dummy_score = 0\n",
    "#for _ in range(1000):\n",
    "#    dummy_clf.fit(X, y)\n",
    "#    dummy_score += dummy_clf.score(X, y)\n",
    "#dummy_score = dummy_score / 1000   \n",
    "\n",
    "# Actual accuracy of the dummy in the same train-test split as the RF model\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_score = dummy_clf.score(X_test, y_test)\n",
    "print('Mean accuracy of null ' + dummy +' model: {0:.2f}'.format(dummy_score),'\\n')\n",
    "print('Mean accuracy (in test) of RF model: {0:.2f}'.format(CV_rfc.score(X_test, y_test)),'\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix and report of the selected dummy classifier\n",
    "\n",
    "y_pred_dummy = dummy_clf.predict(X_test)\n",
    "print('Confusion Matrix:\\n\\n ',confusion_matrix(y_test,y_pred_dummy),'\\n')\n",
    "print(classification_report(y_test,y_pred_dummy),'\\n')\n",
    "print('Accuracy: {0:.2f}'.format(accuracy_score(y_test, y_pred_dummy),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for reference, the results of the RF Model\n",
    "\n",
    "y_pred = CV_rfc.predict(X_test)\n",
    "print('Confusion Matrix:\\n\\n ', confusion_matrix(y_test,y_pred),'\\n')\n",
    "print(classification_report(y_test,y_pred),'\\n')\n",
    "print('Accuracy: {0:.2f}'.format(accuracy_score(y_test, y_pred),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_report = pd.DataFrame(classification_report(y_test,dummy_clf.predict(X_test), output_dict= True))\n",
    "\n",
    "rfc_report = pd.DataFrame(classification_report(y_test,CV_rfc.predict(X_test), output_dict= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increase in prediction power (percentage with respect to null model)\n",
    "\n",
    "i.e. 100% means twice as good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table = ((rfc_report - dummy_report)*100 / dummy_report).drop('support').round(decimals=2)\n",
    "final_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This significant increases further support the claim that the predictors (based on ego-network properties) have useful information to predict the countries of origin of the individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work (ideas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Learning curves: could explore how getting more data would impact (improve) prediction results</li>\n",
    "    <li>We could train other models (neural networks, SVM, etc) but I think the point has been made</li>\n",
    "    <li>Feature engeneering. We should focus on feedback from Jose Luis to see what features are more interesting to explore. The analyses I present here focus on a somewhat arbitrary choice.</li>\n",
    "    <li>Feature importance and interpreation: Random Forest are amenable to work on the relative importance of each of the features for each of the labels---see SHAP values for example. This together with the results from the multinomial LR may help to explain the results (and build a narrative)</li>\n",
    "        \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shap Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "  Shap values are a tool to interpret our random forest model, in this case. They tell us some intuition about which part of the prediction belongs to each feature. \n",
    "</ul>\n",
    "<ul>\n",
    "A positive (negative) SHAP value indicates that the value (in this case, probability of belonging to a certain country) is reinforced (diminished) by the feature.  \n",
    "</ul>\n",
    "<ul>\n",
    "We will use 2 kind of plots at this moment. The first one one is a summary plot, a violin plot of the distribution of SHAP values. The colour indicates the value of the feature indicated at the left. This plot let us see the which features contribute the most (this is, they have high SHAP values). Features are ordered according to their contribution to the global prediction.\n",
    "</ul>\n",
    "<ul>\n",
    "The second kind of plot you will see several times after the summary plot is the dependence plot. They show the distribution of the SHAP values of a variable. The colormap plots another variable, the one the algorithm thinks it has more interaction with the current variable. It lets us distinguish between different regimes of the coloured variable. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the model's predictions using SHAP\n",
    "##Shap values\n",
    "import  shap\n",
    "\n",
    "shap.initjs()\n",
    "model = CV_rfc.best_estimator_\n",
    "explainer = shap.TreeExplainer(model,X_train,check_additivity=False)\n",
    "shap_values = explainer.shap_values(X_train,check_additivity=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Shap values for argentinians</u>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[3],X_train,feature_names = predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Interesting dependencies for argentinians</u>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(5, shap_values[3], X_train,feature_names=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Low average degree shows (more or less) a linear tendency prop to closeness_origin. With high average degree, there are some nonlinear contributions, meaning that people with higher closeness to its origin is in general more probable to be argentinian but, if overall someone has a high average_degree it is losing its argentinianness. Perhaps we are talking about migrants that have a high closeness to their loved ones  ( in Argentina) , but they have a highly connected network in their current countries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(3, shap_values[3], X_train,feature_names=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low average degree shows more or less a linear tendency, but high average degree shows again some nonlinear effects. Perhaps here we are watching that people with poorly grouped networks, but highly connected, lows the probability of being argentinian. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Shap values for dominicans</u> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of summary plot\n",
    "shap.summary_plot(shap_values[1],X_train,feature_names = predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Interesting dependencies for dominicans</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of dependence plot \n",
    "shap.dependence_plot(0, shap_values[1], X_train,feature_names=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closeness is irrelevant in order to determina if someone is dominican except if we reach an aprox value of 1.5. This could mean that if people are sufficient close to each other in the ego network, forming some kind of closed groups, it helps the machine to identify dominicans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(3, shap_values[1], X_train,feature_names=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is only interesting if we are talking about highly connected networks. Clustered networks with high average degree provide high SHAP values. The effect seems similar to the same graph in argentinians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(5, shap_values[1], X_train,feature_names=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average degree in the ego network is only interesting when we are talking about the most connected networks of the sample. Perhaps its some bias from our data, because they are the most connected ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>SHAP values for moroccans </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of summary plot\n",
    "shap.summary_plot(shap_values[-2],X_train,feature_names = predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Interesting dependencies for moroccans</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(0, shap_values[-2], X_train,feature_names=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract from this picture that the algorithm learns that someone is not moroccan if the elements from its network are very close to each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(2, shap_values[-2], X_train,feature_names=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that, despite a low deviation, agents that are below the average closeness_origin are more probable to be moroccan, and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(3, shap_values[-2], X_train,feature_names=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same effect, but weaker, appears in clustering. Moroccan people appear to be identified by a high value of clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(5, shap_values[-2], X_train,feature_names=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same effect happens again , the algorithm learns that moroccans usually have less densely connected networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>SHAP values for Puerto Ricans  </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[2],X_train,feature_names = predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Interesting dependencies for Puerto Ricans</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(0, shap_values[2], X_train,feature_names=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is learning that individuals with a high value of closeness are not Puerto Ricans, or it makes it less probable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(5, shap_values[2], X_train,feature_names=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm correlates a value of average_degree around the average value of the set with a higher probability of being Puerto Rican. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(1, shap_values[2], X_train,feature_names=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is learning that a closeness_residence above the average value of the set is slightly correlated with being Puerto Rican.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>SHAP values for Senegalese   </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[5],X_train,feature_names = predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Interesting dependencies for Senegalese</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(7, shap_values[5], X_train,feature_names=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the closeness above the average, we can see that there is some kind of positive correlation with being senegalese until we reach a limit value. Senegalese have close networks, but not the closest ones, perhaps this is reflecting a bias from our sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(5, shap_values[5], X_train,feature_names=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the closeness the message is that senegalese people have densely connected networks, but not the most dense in the sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(7, shap_values[5], X_train,feature_names=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm learns that people with a higher betweenness that the average is less likely to be senegalese and viceversa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Education and migration date effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the accuracy, which stays at 0.48, education is in someway interesting according to the random forest model. On the other hand, migration date does not seem to make a sufficient effect. \n",
    "Below some dependence plots have been drawn, for the nationalities where SHAP values become important vs the education level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(8, shap_values[1], X_train,feature_names=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that for people with an education level slightly above the average (4 in our graduated scale of 7), the system understands that there is a slightly learning on the possibility of being dominican according to the closeness of the network. This means that it should be some pattern related with dominicans having dense networks and a level of studies slightly above the average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(8, shap_values[2], X_train,feature_names=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of moroccans, the system learns that there is a slightly increase of the probability of being moroccan (slightly because the SHAP values hardly reach 0.1), if the level of education is above the average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(8, shap_values[3], X_train,feature_names=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of Puerto Ricans, the system learns that an education level below the average increases slightly the probability, and when the education level gets lower, the probability is increased. But these values are very scattered, they should be treated carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of nationalities that do not appear do not show correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "LIME (Local Interpretable Model-agnostic Explanations), is an algorithm that takes the decision function from the classifier (decision = f(features)). This function may be complex, but the algorithm makes a linear regression around a single prediction, weighting the importance of the coefficients with the distance to this local prediction.   \n",
    "</ul>\n",
    "<ul>\n",
    "This kind of algorithm helps us to explain single predictions.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Using LIME to interpret \n",
    "import lime\n",
    "import lime.lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=predictors, class_names=names, discretize_continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, X_test.shape[0])\n",
    "exp = explainer.explain_instance(X_test[i], CV_rfc.predict_proba, num_features=2, top_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exp.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "plt.subplot(1,5,1)\n",
    "shap.summary_plot(shap_values[1], X_train,feature_names=predictors,show = False)\n",
    "plt.subplot(1,5,2)\n",
    "shap.summary_plot(shap_values[2], X_train,feature_names=predictors,show = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_2.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[0],X_train,feature_names = predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 =df[df['Subject_origin'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_0['Closeness_origin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
