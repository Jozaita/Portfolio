{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6badee70",
   "metadata": {},
   "source": [
    "# High school network and deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c57a2",
   "metadata": {},
   "source": [
    "Our point is, considering the positive and negative networks as two different directed graphs, use a GConv network with previous autoencoder (Tutorial 12 Pytorch Geometric) to predict links from one of the networks, then compare the predicted_edges with the negative ones. Consider the positive as the training set and compare it to the negatives in the test one. \n",
    "\n",
    "* We are going to use a graph autoencoder, which is a non-supervised neural network that takes data, translate them to another representation (the one the neural network extracts from them) and then try to rebuild the original data. The representation it learns is based on the structure of the network.\n",
    "\n",
    "* We will use also a heuristic method, called PageRank method, traditionally used in link prediction, where the probability of a link depends on a variable called rank. \n",
    "\n",
    "* We compare it with a null method, a random graph. \n",
    "\n",
    "In both of the comparative methods of the GNN, negative links can only be placed where there are not positive links. \n",
    "\n",
    "The graph autoencoder generate a fixed number of links depending on built-in functions, so we are taking these number of links in order to establish comparison with other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d206f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "## Just prepare the data\n",
    "nodes = pd.read_csv(r\"Nodes_t1.csv\",sep=\";\",encoding = 'unicode_escape')\n",
    "edges = pd.read_csv(r\"Edges_t1.csv\",sep=\";\",encoding = 'unicode_escape')\n",
    "edges = edges.apply(lambda x: x - x.min(),axis = 0)\n",
    "###Erase ESO \n",
    "nodes[\"Curso\"] = nodes[\"Curso\"].astype(str).str[0].astype(\"int64\")\n",
    "del nodes[\"Unnamed: 0\"]\n",
    "edges[\"weight\"] = edges[\"weight\"].apply(lambda x:x+1)\n",
    "pos_edges = edges[edges[\"weight\"]> 3]\n",
    "neg_edges = edges[edges[\"weight\"]< 3]\n",
    "G_positive = nx.from_pandas_edgelist(pos_edges, \"from\", \"to\",create_using=nx.DiGraph,edge_attr=\"weight\")\n",
    "G_negative = nx.from_pandas_edgelist(neg_edges, \"from\", \"to\",create_using=nx.DiGraph,edge_attr=\"weight\")\n",
    "G_negative.add_nodes_from(range(nodes.index.max()+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34705cd",
   "metadata": {},
   "source": [
    "### Define the complementary network of the positive network\n",
    "We will use it for choosing the edges in the different null models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7288d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_real = []\n",
    "for elem in pos_edges[[\"from\",\"to\"]].to_numpy():\n",
    "    edges_real.append(tuple(elem))\n",
    "chosen_edges = list(nx.complete_graph(409,create_using=nx.DiGraph()).edges())\n",
    "for elem in edges_real:\n",
    "    chosen_edges.remove(elem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d0bc34",
   "metadata": {},
   "source": [
    "## Graph autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a6a20",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1fc3d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "import torch\n",
    "import torch_geometric.data as data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import negative_sampling,train_test_split_edges,to_dense_adj\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from sklearn import preprocessing\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1795f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.read_csv(r\"Nodes_t1.csv\",sep=\";\",encoding = 'unicode_escape')\n",
    "edges = pd.read_csv(r\"Edges_t1.csv\",sep=\";\",encoding = 'unicode_escape')\n",
    "edges = edges.apply(lambda x: x - x.min(),axis = 0)\n",
    "###Erase ESO \n",
    "nodes[\"Curso\"] = nodes[\"Curso\"].astype(str).str[0].astype(\"int64\")\n",
    "del nodes[\"Unnamed: 0\"]\n",
    "### Separate positive from negative networks\n",
    "pos_edges = edges[edges[\"weight\"]> 2]\n",
    "neg_edges = edges[edges[\"weight\"]< 2] \n",
    "### One hot encode and normalize node attributes\n",
    "nodes_dummy = pd.get_dummies(nodes[[\"Curso\",\"Grupo\"]])\n",
    "rng = np.random.default_rng()\n",
    "#nodes_dummy = pd.DataFrame(rng.integers(0, 2, size=(409, 10)), columns=list('ABCDEFGHIJ'))\n",
    "\n",
    "x = nodes_dummy.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "nodes_norm = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5003aabe",
   "metadata": {},
   "source": [
    "### Firstly, check for isomorphism with Networkx \n",
    "\n",
    "Networkx has a isomorphism library that comes mainly from the VF2 algorithm : https://www.researchgate.net/publication/200034365_An_Improved_Algorithm_for_Matching_Large_Graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629e2cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph of positive links is direcly isomorphic to the negative one ? False.\n"
     ]
    }
   ],
   "source": [
    "from networkx.algorithms import isomorphism\n",
    "\n",
    "\n",
    "DiGM = isomorphism.DiGraphMatcher(G_positive,G_negative)\n",
    "\n",
    "print(\"The graph of positive links is direcly isomorphic to the negative one ? {}.\".format(DiGM.is_isomorphic()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e28034",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Without including class and group information \n",
    "positive_data = data.Data(x=torch.tensor(nodes_norm.to_numpy(),dtype=torch.float32),\n",
    "                          edge_index=torch.tensor(pos_edges[[\"from\",\"to\"]].to_numpy().T))\n",
    "negative_data = data.Data(x=torch.tensor(nodes_norm.to_numpy(),dtype=torch.float32),\n",
    "                          edge_index=torch.tensor(neg_edges[[\"from\",\"to\"]].to_numpy().T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "079082e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/.local/lib/python3.8/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "data = positive_data.clone()\n",
    "data.num_nodes = len(data._store[\"x\"])\n",
    "data = train_test_split_edges(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d91946",
   "metadata": {},
   "source": [
    "### Models for the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaafd14",
   "metadata": {},
   "source": [
    "#### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f13d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(data.num_features, 128)\n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "\n",
    "    def encode(self):\n",
    "        x = self.conv1(data.x, data.train_pos_edge_index) # convolution 1\n",
    "        x = x.relu()\n",
    "        return self.conv2(x, data.train_pos_edge_index) # convolution 2\n",
    "\n",
    "    def decode(self, z, pos_edge_index, neg_edge_index): # only pos and neg edges\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1) # concatenate pos and neg edges\n",
    "        logits = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)  # dot product \n",
    "        return logits\n",
    "\n",
    "    def decode_all(self, z): \n",
    "        prob_adj = z @ z.t() # get adj NxN\n",
    "        return (prob_adj > 1-10e-10).nonzero(as_tuple=False).t() # get predicted edge_list "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2c32d",
   "metadata": {},
   "source": [
    "#### Set the parameters and move data to autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e9fb8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, positive_data = Net().to(device), positive_data.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df952510",
   "metadata": {},
   "source": [
    "#### Algorithms of training and evaluation (Tutorial PyG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "266bf880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    # returns a tensor:\n",
    "    # [1,1,1,1,...,0,0,0,0,0,..] with the number of ones is equel to the lenght of pos_edge_index\n",
    "    # and the number of zeros is equal to the length of neg_edge_index\n",
    "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(E, dtype=torch.float, device=device)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1.\n",
    "    return link_labels\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.train_pos_edge_index, #positive edges\n",
    "        num_nodes=data.num_nodes, # number of nodes\n",
    "        num_neg_samples=data.train_pos_edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    z = model.encode() #encode\n",
    "    link_logits = model.decode(z, data.train_pos_edge_index, neg_edge_index) # decode\n",
    "    \n",
    "    link_labels = get_link_labels(data.train_pos_edge_index, neg_edge_index)\n",
    "    loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    perfs = []\n",
    "    for prefix in [\"val\", \"test\"]:\n",
    "        pos_edge_index = data[f'{prefix}_pos_edge_index']\n",
    "        neg_edge_index = data[f'{prefix}_neg_edge_index']\n",
    "\n",
    "        z = model.encode() # encode train\n",
    "        link_logits = model.decode(z, pos_edge_index, neg_edge_index) # decode test or val\n",
    "        link_probs = link_logits.sigmoid() # apply sigmoid\n",
    "        \n",
    "        link_labels = get_link_labels(pos_edge_index, neg_edge_index) # get link\n",
    "        \n",
    "        perfs.append(roc_auc_score(link_labels.cpu(), link_probs.cpu())) #compute roc_auc score\n",
    "    return perfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205bf787",
   "metadata": {},
   "source": [
    "#### Training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "831973b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 0.4466, Val: 0.9353, Test: 0.9070\n",
      "Epoch: 200, Loss: 0.4378, Val: 0.9391, Test: 0.9109\n",
      "Epoch: 300, Loss: 0.4320, Val: 0.9430, Test: 0.9156\n",
      "Epoch: 400, Loss: 0.4228, Val: 0.9430, Test: 0.9156\n",
      "Epoch: 500, Loss: 0.4279, Val: 0.9452, Test: 0.9252\n",
      "Epoch: 600, Loss: 0.4293, Val: 0.9452, Test: 0.9252\n",
      "Epoch: 700, Loss: 0.4202, Val: 0.9452, Test: 0.9252\n",
      "Epoch: 800, Loss: 0.4196, Val: 0.9452, Test: 0.9252\n",
      "Epoch: 900, Loss: 0.4173, Val: 0.9452, Test: 0.9252\n",
      "Epoch: 1000, Loss: 0.4114, Val: 0.9452, Test: 0.9252\n",
      "Epoch: 1100, Loss: 0.4102, Val: 0.9452, Test: 0.9252\n",
      "Epoch: 1200, Loss: 0.4168, Val: 0.9452, Test: 0.9252\n",
      "Epoch: 1300, Loss: 0.4067, Val: 0.9452, Test: 0.9252\n",
      "Epoch: 1400, Loss: 0.4048, Val: 0.9452, Test: 0.9252\n",
      "Epoch: 1500, Loss: 0.4021, Val: 0.9452, Test: 0.9252\n",
      "Epoch: 1600, Loss: 0.3992, Val: 0.9452, Test: 0.9252\n",
      "Epoch: 1700, Loss: 0.4132, Val: 0.9452, Test: 0.9252\n",
      "Epoch: 1800, Loss: 0.4080, Val: 0.9452, Test: 0.9252\n",
      "Epoch: 1900, Loss: 0.4038, Val: 0.9452, Test: 0.9252\n",
      "Epoch: 2000, Loss: 0.4057, Val: 0.9452, Test: 0.9252\n"
     ]
    }
   ],
   "source": [
    "best_val_perf = test_perf = 0\n",
    "for epoch in range(1, 2001):\n",
    "    train_loss = train()\n",
    "    val_perf, tmp_test_perf = test()\n",
    "    if val_perf > best_val_perf:\n",
    "        best_val_perf = val_perf\n",
    "        test_perf = tmp_test_perf\n",
    "    log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    if epoch % 100 == 0:\n",
    "        print(log.format(epoch, train_loss, best_val_perf, test_perf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d13d7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.encode()\n",
    "final_edge_index_1 = model.decode_all(z)\n",
    "#Remove self loops\n",
    "bool_mask = final_edge_index_1[0] != final_edge_index_1[1]\n",
    "simulated_edges_1 = torch.empty((2,int(sum(bool_mask))))\n",
    "for item in range(final_edge_index_1.size()[0]):\n",
    "    simulated_edges_1[item] = final_edge_index_1[item][bool_mask]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "782adc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coincidences = to_dense_adj(negative_data[\"edge_index\"]).squeeze()*to_dense_adj(final_edge_index_1).squeeze()\n",
    "coin_GNN_neg = coincidences.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169cef43",
   "metadata": {},
   "source": [
    "### Link prediction with PageRank \n",
    "\n",
    "From the paper of _Alain Barrat_ Anxo recommended (_New Insights and Methods forPredicting Face-to-Face Contacts_), it can be checked the  _Hybrid Rooted PageRank_. We implement it in the following: \n",
    "\n",
    "*  With probability $\\alpha$ jump to root node _r_.\n",
    "*  With probability $1−\\alpha$:\n",
    "    *  Choose Network $N_{i}∈N$ with respect toprobability distribution _P_.\n",
    "    *  If there exist no outgoing edges then :\n",
    "    * Jump to root node _r_\n",
    "    *  Else:\n",
    "        From the current node c jump to a neighbornselected with a probability $w(c,n)∑c→dw(c,d)$, i. e.,proportional to the weight $w(c,n)$ of the $e(c,n)$\n",
    "\n",
    "But we will include modifications on this analysis, as _Barrat et al_ use two networks in order to extract a single social network, while we are trying to deduce one from the other. We will implement PageRank on one of them and predict the links of the other one based on this quantity. When we are calculating link prediction, we calculate that the probability of the link is :\n",
    "\\begin{equation}\n",
    "  p_{link(i,j)} =\\dfrac{1}{1+e^{-(rank_{i}-rank_{j})}}\n",
    "\\end{equation}\n",
    "\n",
    "Considering that the rank ordering is a some kind of classification of dominance of the node, according to the original paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c176db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "def HR_pagerank(alpha,G):\n",
    "    N_rounds = 1000\n",
    "    rank = [0]*len(G.nodes())\n",
    "    for rounds in range(N_rounds):\n",
    "        for node in range(409):\n",
    "            a = rd.uniform(0,1)\n",
    "            site = list(G.nodes())[node]\n",
    "            targets = list(G.nodes())\n",
    "            targets.remove(site)\n",
    "            if a > alpha:\n",
    "                target = rd.choice(targets)\n",
    "                if target in list(G.neighbors(site)):\n",
    "                    c = rd.uniform(0,1)\n",
    "                    weight_target = G[site][target][\"weight\"]\n",
    "                    weight=nx.get_edge_attributes(G,'weight')\n",
    "                    av_weights = 0\n",
    "                    for n in list(G.neighbors(site)):\n",
    "                        av_weights += weight[(site,n)]\n",
    "                    av_weights /= len(list(G.neighbors(site)))\n",
    "                    if c<((weight_target)/(av_weights)):\n",
    "                        site = target\n",
    "                        rank[site] +=1\n",
    "    rank = [item/N_rounds for item in rank]\n",
    "    return rank\n",
    "\n",
    "def create_link(G,rank,chosen_edges):\n",
    "    index_pair = rd.choice(chosen_edges)\n",
    "    rd_pair = [rank[item] for item in index_pair]\n",
    "    p_rank = 1/(1 + np.exp(-(rd_pair[0]-rd_pair[1])))\n",
    "    if (rd.uniform(0,1) < p_rank) : \n",
    "        G.add_edge(index_pair[0],index_pair[1])\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e4d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive_rank = HR_pagerank(0.15,G_positive)\n",
    "positive_rank = nx.algorithms.pagerank(G_positive,0.15)\n",
    "G_simulated = nx.DiGraph()\n",
    "while len(G_simulated.edges())< final_edge_index_1.size()[1]:\n",
    "    create_link(G_simulated,positive_rank,chosen_edges)\n",
    "\n",
    "coincidences = to_dense_adj(negative_data[\"edge_index\"]).squeeze()*torch.tensor(nx.adjacency_matrix(G_simulated).todense())\n",
    "coin_rank_pos = coincidences.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de98345",
   "metadata": {},
   "source": [
    "### Randomly created network\n",
    "\n",
    "We compare the results from the GNN and the PageRank with a randomly created network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8cf840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd \n",
    "coincidences_total = 0\n",
    "for sim in range(10):\n",
    "    chosen_edges_2 = chosen_edges.copy()\n",
    "    G_random = nx.DiGraph()\n",
    "    G_random.add_nodes_from(range(409))\n",
    "    for trial in range(final_edge_index_1.size()[1]):\n",
    "        rd_sample = rd.choice(chosen_edges_2)\n",
    "        G_random.add_edge(rd_sample[0],rd_sample[1]) \n",
    "        chosen_edges_2.remove(rd_sample)\n",
    "    coincidences_random = len([(u,v) for (u,v) in G_random.edges() if G_negative.has_edge(u,v)])\n",
    "    coincidences_total += coincidences_random\n",
    "coin_random = coincidences_total/10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a41c1fd",
   "metadata": {},
   "source": [
    "Proportions between the random network score and the amount of edges chosen divided by the total number of edges are similar, so the random network finds a correct proportion of the real links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4db8fd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13904382470119522, 0.14082220968853795)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coin_random/len(neg_edges),final_edge_index_1.size()[1]/(409*408-len(pos_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ca8d9",
   "metadata": {},
   "source": [
    "### Results compared in coincidences with the original networks\n",
    "\n",
    "We write $+/-$ as the prediction power of the negative network depending on the positive one and $-/+$ to design the other way around. Results are expresed in terms of the number of links the method is able to reproduce in the original networks. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccfa41e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Random network           </td><td style=\"text-align: right;\">0.139</td></tr>\n",
       "<tr><td>PageRank heuristics      </td><td style=\"text-align: right;\">0.134</td></tr>\n",
       "<tr><td>GCN with random info     </td><td style=\"text-align: right;\">0.406</td></tr>\n",
       "<tr><td>GCN with class/group info</td><td style=\"text-align: right;\">0.41 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "table = [[\"Random network\",f\"{coin_random/len(neg_edges):.3f}\"],\n",
    "         [\"PageRank heuristics\",f\"{coin_rank_pos/len(neg_edges):.3f}\"],\n",
    "         [\"GCN with random info\",\"0.406\"],\n",
    "         [\"GCN with class/group info\",f\"{coin_GNN_neg/len(neg_edges):.3f}\"]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0ba46c",
   "metadata": {},
   "source": [
    "The quantities may diverge a little bit because the autoencoder generate a different number of links depending on the realization, but it is clear that Graph Convolutional Networks outperform the heuristics used in link prediction. It is difficult to say something about the structural relationship between both networks, but there is some kind of relationship, as there is a difference in scoring for all the methods.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e9c8ee",
   "metadata": {},
   "source": [
    "**Work to be done** \n",
    "\n",
    "1) Check the structural balance theory, computing global equilibria in both networks, in order to generate ensembles. \n",
    "\n",
    "2) Graph neural networks can also be used to predict labeling in edges, it could be used to proof structural balance theory from other perspective. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dffe4e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[409, 9], edge_index=[2, 7302])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b436e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
