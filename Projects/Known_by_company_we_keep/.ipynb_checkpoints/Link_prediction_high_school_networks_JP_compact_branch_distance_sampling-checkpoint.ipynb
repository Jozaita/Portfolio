{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6badee70",
   "metadata": {},
   "source": [
    "# High school network and deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c57a2",
   "metadata": {},
   "source": [
    "Our point is, considering the positive and negative networks as two different directed graphs, use a GConv network with previous autoencoder (Tutorial 12 Pytorch Geometric) to predict links from one of the networks, then compare the predicted_edges with the negative ones. Consider the positive as the training set and compare it to the negatives in the test one. \n",
    "\n",
    "* We are going to use a graph autoencoder, which is a non-supervised neural network that takes data, translate them to another representation (the one the neural network extracts from them) and then try to rebuild the original data. The representation it learns is based on the structure of the network.\n",
    "\n",
    "* We will use also a heuristic method, called PageRank method, traditionally used in link prediction, where the probability of a link depends on a variable called rank. \n",
    "\n",
    "* We compare it with a null method, a random graph. \n",
    "\n",
    "In both of the comparative methods of the GNN, negative links can only be placed where there are not positive links. \n",
    "\n",
    "The graph autoencoder generate a fixed number of links depending on built-in functions, so we are taking these number of links in order to establish comparison with other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4d72f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  In this variation we are going to remove a whole class and let the neural network rebuild it. We are considering classes as main components of the whole. This is done just in the positive network. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ca0dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "import torch\n",
    "import torch_geometric.data as data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import negative_sampling,train_test_split_edges,to_dense_adj\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from sklearn import preprocessing\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d206f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "## Just prepare the data\n",
    "nodes = pd.read_csv(r\"Nodes_t1.csv\",sep=\";\",encoding = 'unicode_escape')\n",
    "edges = pd.read_csv(r\"Edges_t1.csv\",sep=\";\",encoding = 'unicode_escape')\n",
    "edges = edges.apply(lambda x: x - x.min(),axis = 0)\n",
    "###Erase ESO \n",
    "nodes[\"Curso\"] = nodes[\"Curso\"].astype(str).str[0].astype(\"int64\")\n",
    "del nodes[\"Unnamed: 0\"]\n",
    "edges[\"weight\"] = edges[\"weight\"].apply(lambda x:x+1)\n",
    "pos_edges = edges[edges[\"weight\"]> 3]\n",
    "neg_edges = edges[edges[\"weight\"]< 3]\n",
    "G_positive = nx.from_pandas_edgelist(pos_edges, \"from\", \"to\",create_using=nx.DiGraph,edge_attr=\"weight\")\n",
    "G_negative = nx.from_pandas_edgelist(neg_edges, \"from\", \"to\",create_using=nx.DiGraph,edge_attr=\"weight\")\n",
    "G_negative.add_nodes_from(range(nodes.index.max()+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26210f0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    We need to remove from the data the edges of a whole class and label them as <i> y_true</i>.\n",
    "    We remove the edges that come from a target course, such that the intraclass links are removed and also the \n",
    "    links that have source in class. \n",
    "</div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c2ad85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29443/1727794151.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  edges_class_saved = torch.Tensor([edges_class[\"from\"].to_numpy(),edges_class[\"to\"].to_numpy()])\n"
     ]
    }
   ],
   "source": [
    "curso = 2 \n",
    "grupo = \"C\"\n",
    "\n",
    "nodes_class = list(nodes[(nodes[\"Curso\"] == curso)&(nodes[\"Grupo\"] == grupo)].index)\n",
    "edges_class = edges[(edges[\"from\"].isin(nodes_class))]\n",
    "edges_class_saved = torch.Tensor([edges_class[\"from\"].to_numpy(),edges_class[\"to\"].to_numpy()])\n",
    "edges.drop(edges_class.index,inplace = True)\n",
    "edges_class_saved = edges_class_saved.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d0bc34",
   "metadata": {},
   "source": [
    "## Graph autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a6a20",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1795f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.read_csv(r\"Nodes_t1.csv\",sep=\";\",encoding = 'unicode_escape')\n",
    "edges = pd.read_csv(r\"Edges_t1.csv\",sep=\";\",encoding = 'unicode_escape')\n",
    "edges = edges.apply(lambda x: x - x.min(),axis = 0)\n",
    "###Erase ESO \n",
    "nodes[\"Curso\"] = nodes[\"Curso\"].astype(str).str[0].astype(\"int64\")\n",
    "del nodes[\"Unnamed: 0\"]\n",
    "### Separate positive from negative networks\n",
    "pos_edges = edges[edges[\"weight\"]> 2]\n",
    "neg_edges = edges[edges[\"weight\"]< 2] \n",
    "### One hot encode and normalize node attributes\n",
    "nodes_dummy = pd.get_dummies(nodes[[\"Curso\",\"Grupo\"]])\n",
    "rng = np.random.default_rng()\n",
    "#nodes_dummy = pd.DataFrame(rng.integers(0, 2, size=(409, 10)), columns=list('ABCDEFGHIJ'))\n",
    "\n",
    "x = nodes_dummy.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "nodes_norm = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5003aabe",
   "metadata": {},
   "source": [
    "### Firstly, check for isomorphism with Networkx \n",
    "\n",
    "Networkx has a isomorphism library that comes mainly from the VF2 algorithm : https://www.researchgate.net/publication/200034365_An_Improved_Algorithm_for_Matching_Large_Graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629e2cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph of positive links is direcly isomorphic to the negative one ? False.\n"
     ]
    }
   ],
   "source": [
    "from networkx.algorithms import isomorphism\n",
    "\n",
    "\n",
    "DiGM = isomorphism.DiGraphMatcher(G_positive,G_negative)\n",
    "\n",
    "print(\"The graph of positive links is direcly isomorphic to the negative one ? {}.\".format(DiGM.is_isomorphic()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e28034",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Without including class and group information \n",
    "positive_data = data.Data(x=torch.tensor(nodes_norm.to_numpy(),dtype=torch.float32),\n",
    "                          edge_index=torch.tensor(pos_edges[[\"from\",\"to\"]].to_numpy().T))\n",
    "negative_data = data.Data(x=torch.tensor(nodes_norm.to_numpy(),dtype=torch.float32),\n",
    "                          edge_index=torch.tensor(neg_edges[[\"from\",\"to\"]].to_numpy().T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "079082e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/.local/lib/python3.8/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "data = positive_data.clone()\n",
    "data.num_nodes = len(data._store[\"x\"])\n",
    "data = train_test_split_edges(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d91946",
   "metadata": {},
   "source": [
    "### Models for the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaafd14",
   "metadata": {},
   "source": [
    "#### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f13d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(data.num_features, 128)\n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "\n",
    "    def encode(self):\n",
    "        x = self.conv1(data.x, data.train_pos_edge_index) # convolution 1\n",
    "        x = x.relu()\n",
    "        return self.conv2(x, data.train_pos_edge_index) # convolution 2\n",
    "\n",
    "    def decode(self, z, pos_edge_index, neg_edge_index): # only pos and neg edges\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1) # concatenate pos and neg edges\n",
    "        logits = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)  # dot product \n",
    "        return logits\n",
    "\n",
    "    def decode_all(self, z): \n",
    "        prob_adj = z @ z.t() # get adj NxN\n",
    "        return (prob_adj > 1-10e-10).nonzero(as_tuple=False).t() # get predicted edge_list "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2c32d",
   "metadata": {},
   "source": [
    "#### Set the parameters and move data to autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e9fb8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, positive_data = Net().to(device), positive_data.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df952510",
   "metadata": {},
   "source": [
    "#### Algorithms of training and evaluation (Tutorial PyG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "266bf880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    # returns a tensor:\n",
    "    # [1,1,1,1,...,0,0,0,0,0,..] with the number of ones is equel to the lenght of pos_edge_index\n",
    "    # and the number of zeros is equal to the length of neg_edge_index\n",
    "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(E, dtype=torch.float, device=device)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1.\n",
    "    return link_labels\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.train_pos_edge_index, #positive edges\n",
    "        num_nodes=data.num_nodes, # number of nodes\n",
    "        num_neg_samples=data.train_pos_edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    z = model.encode() #encode\n",
    "    link_logits = model.decode(z, data.train_pos_edge_index, neg_edge_index) # decode\n",
    "    \n",
    "    link_labels = get_link_labels(data.train_pos_edge_index, neg_edge_index)\n",
    "    loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    perfs = []\n",
    "    for prefix in [\"val\", \"test\"]:\n",
    "        pos_edge_index = data[f'{prefix}_pos_edge_index']\n",
    "        neg_edge_index = data[f'{prefix}_neg_edge_index']\n",
    "\n",
    "        z = model.encode() # encode train\n",
    "        link_logits = model.decode(z, pos_edge_index, neg_edge_index) # decode test or val\n",
    "        link_probs = link_logits.sigmoid() # apply sigmoid\n",
    "        \n",
    "        link_labels = get_link_labels(pos_edge_index, neg_edge_index) # get link\n",
    "        \n",
    "        perfs.append(roc_auc_score(link_labels.cpu(), link_probs.cpu())) #compute roc_auc score\n",
    "    return perfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205bf787",
   "metadata": {},
   "source": [
    "#### Training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831973b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 0.4508, Val: 0.9357, Test: 0.9071\n",
      "Epoch: 200, Loss: 0.4392, Val: 0.9391, Test: 0.9082\n",
      "Epoch: 300, Loss: 0.4373, Val: 0.9391, Test: 0.9082\n",
      "Epoch: 400, Loss: 0.4247, Val: 0.9391, Test: 0.9082\n",
      "Epoch: 500, Loss: 0.4319, Val: 0.9391, Test: 0.9082\n"
     ]
    }
   ],
   "source": [
    "best_val_perf = test_perf = 0\n",
    "for epoch in range(1, 2001):\n",
    "    train_loss = train()\n",
    "    val_perf, tmp_test_perf = test()\n",
    "    if val_perf > best_val_perf:\n",
    "        best_val_perf = val_perf\n",
    "        test_perf = tmp_test_perf\n",
    "    log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    if epoch % 100 == 0:\n",
    "        print(log.format(epoch, train_loss, best_val_perf, test_perf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521e83b9",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "    We will produce the links, and from them extract the ones referred to the particular course we are trying to rebuild. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13d7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.encode()\n",
    "final_edge_index_1 = model.decode_all(z)\n",
    "#Remove self loops\n",
    "bool_mask = final_edge_index_1[0] != final_edge_index_1[1]\n",
    "simulated_edges_1 = torch.empty((2,int(sum(bool_mask))))\n",
    "for item in range(final_edge_index_1.size()[0]):\n",
    "    simulated_edges_1[item] = final_edge_index_1[item][bool_mask]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f46fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_edge_index_1\n",
    "#edges_class_saved\n",
    "#final_edge_index_1[0]\n",
    "a = ((simulated_edges_1[0] <= max(nodes_class))&(simulated_edges_1[0] > min(nodes_class)))\n",
    "final_edge_index_f = torch.Tensor([final_edge_index_1[0][a].numpy(),final_edge_index_1[1][a].numpy()])\n",
    "final_edge_index_f = final_edge_index_f.type(torch.LongTensor)\n",
    "print(len(final_edge_index_f[0]),len(edges_class_saved[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48905ecc",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "   Compute the coincidences. To do this, we check if each of the predicted links was in the original data. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782adc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coin = 0 \n",
    "for item in zip(*final_edge_index_f):\n",
    "     coin += (item in list(zip(*edges_class_saved)))\n",
    "print(\"The amount of links that were in the data is the {} of the total\".format(round(coin/len(final_edge_index_f[0]),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc36ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "coin,len(final_edge_index_1[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
