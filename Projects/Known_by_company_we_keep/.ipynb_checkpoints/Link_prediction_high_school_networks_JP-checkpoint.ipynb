{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6badee70",
   "metadata": {},
   "source": [
    "# High school network and deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c57a2",
   "metadata": {},
   "source": [
    "Our point is, considering the positive and negative networks as two different directed graphs, use a GConv network with previous autoencoder (Tutorial 12 Pytorch Geometric) to predict links from one of the networks, then compare the predicted_edges with the negative ones. Consider the positive as the training set and compare it to the negatives in the test one. \n",
    "\n",
    "* We are going to use a graph autoencoder, which is a non-supervised neural network that takes data, translate them to another representation (the one the neural network extracts from them) and then try to rebuild the original data. The representation it learns is based on the structure of the network.\n",
    "\n",
    "* We will use also a heuristic method, called PageRank method, traditionally used in link prediction, where the probability of a link depends on a variable called rank. \n",
    "\n",
    "* We compare it with a null method, a random graph. \n",
    "\n",
    "The graph autoencoder generate a fixed number of links depending on built-in functions, so we are taking these number of links in order to establish comparison with other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d206f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "## Just prepare the data\n",
    "nodes = pd.read_csv(r\"Nodes_t1.csv\",sep=\";\",encoding = 'unicode_escape')\n",
    "edges = pd.read_csv(r\"Edges_t1.csv\",sep=\";\",encoding = 'unicode_escape')\n",
    "edges = edges.apply(lambda x: x - x.min(),axis = 0)\n",
    "###Erase ESO \n",
    "nodes[\"Curso\"] = nodes[\"Curso\"].astype(str).str[0].astype(\"int64\")\n",
    "del nodes[\"Unnamed: 0\"]\n",
    "edges[\"weight\"] = edges[\"weight\"].apply(lambda x:x+1)\n",
    "pos_edges = edges[edges[\"weight\"]> 2]\n",
    "neg_edges = edges[edges[\"weight\"]< 2]\n",
    "G_positive = nx.from_pandas_edgelist(pos_edges, \"from\", \"to\",create_using=nx.DiGraph,edge_attr=\"weight\")\n",
    "G_negative = nx.from_pandas_edgelist(neg_edges, \"from\", \"to\",create_using=nx.DiGraph,edge_attr=\"weight\")\n",
    "G_negative.add_nodes_from(range(nodes.index.max()+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ceef4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from networkx.algorithms import isomorphism\n",
    "\n",
    "DiGM = isomorphism.DiGraphMatcher(G_positive,G_negative)\n",
    "\n",
    "DiGM.is_isomorphic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d0bc34",
   "metadata": {},
   "source": [
    "## Graph autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a6a20",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1fc3d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "import torch\n",
    "import torch_geometric.data as data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import negative_sampling,train_test_split_edges,to_dense_adj\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from sklearn import preprocessing\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1795f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.read_csv(r\"Nodes_t1.csv\",sep=\";\",encoding = 'unicode_escape')\n",
    "edges = pd.read_csv(r\"Edges_t1.csv\",sep=\";\",encoding = 'unicode_escape')\n",
    "edges = edges.apply(lambda x: x - x.min(),axis = 0)\n",
    "###Erase ESO \n",
    "nodes[\"Curso\"] = nodes[\"Curso\"].astype(str).str[0].astype(\"int64\")\n",
    "del nodes[\"Unnamed: 0\"]\n",
    "### Separate positive from negative networks\n",
    "pos_edges = edges[edges[\"weight\"]> 2]\n",
    "neg_edges = edges[edges[\"weight\"]< 2] \n",
    "### One hot encode and normalize node attributes\n",
    "nodes_dummy = pd.get_dummies(nodes[[\"Curso\",\"Grupo\"]])\n",
    "rng = np.random.default_rng()\n",
    "#nodes_dummy = pd.DataFrame(rng.integers(0, 2, size=(409, 10)), columns=list('ABCDEFGHIJ'))\n",
    "\n",
    "x = nodes_dummy.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "nodes_norm = pd.DataFrame(x_scaled)\n",
    "\n",
    "#x = nodes_dummy_2.values #returns a numpy array\n",
    "#min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#x_scaled = min_max_scaler.fit_transform(x)\n",
    "#nodes_norm = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5003aabe",
   "metadata": {},
   "source": [
    "### Firstly, check for isomorphism with Networkx \n",
    "\n",
    "Networkx has a isomorphism library that comes mainly from the VF2 algorithm : https://www.researchgate.net/publication/200034365_An_Improved_Algorithm_for_Matching_Large_Graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629e2cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph of positive links is direcly isomorphic to the negative one ? False.\n"
     ]
    }
   ],
   "source": [
    "from networkx.algorithms import isomorphism\n",
    "\n",
    "\n",
    "DiGM = isomorphism.DiGraphMatcher(G_positive,G_negative)\n",
    "\n",
    "print(\"The graph of positive links is direcly isomorphic to the negative one ? {}.\".format(DiGM.is_isomorphic()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e28034",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Without including class and group information \n",
    "positive_data = data.Data(x=torch.tensor(nodes_norm.to_numpy(),dtype=torch.float32),\n",
    "                          edge_index=torch.tensor(pos_edges[[\"from\",\"to\"]].to_numpy().T))\n",
    "negative_data = data.Data(x=torch.tensor(nodes_norm.to_numpy(),dtype=torch.float32),\n",
    "                          edge_index=torch.tensor(neg_edges[[\"from\",\"to\"]].to_numpy().T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "079082e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/.local/lib/python3.8/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "data = positive_data.clone()\n",
    "data.num_nodes = len(data._store[\"x\"])\n",
    "data = train_test_split_edges(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d91946",
   "metadata": {},
   "source": [
    "### Models for the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaafd14",
   "metadata": {},
   "source": [
    "#### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f13d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(data.num_features, 128)\n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "\n",
    "    def encode(self):\n",
    "        x = self.conv1(data.x, data.train_pos_edge_index) # convolution 1\n",
    "        x = x.relu()\n",
    "        return self.conv2(x, data.train_pos_edge_index) # convolution 2\n",
    "\n",
    "    def decode(self, z, pos_edge_index, neg_edge_index): # only pos and neg edges\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1) # concatenate pos and neg edges\n",
    "        logits = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)  # dot product \n",
    "        return logits\n",
    "\n",
    "    def decode_all(self, z): \n",
    "        prob_adj = z @ z.t() # get adj NxN\n",
    "        return (prob_adj > 1-10e-10).nonzero(as_tuple=False).t() # get predicted edge_list "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2c32d",
   "metadata": {},
   "source": [
    "#### Set the parameters and move data to autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e9fb8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, positive_data = Net().to(device), positive_data.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df952510",
   "metadata": {},
   "source": [
    "#### Algorithms of training and evaluation (Tutorial PyG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "266bf880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    # returns a tensor:\n",
    "    # [1,1,1,1,...,0,0,0,0,0,..] with the number of ones is equel to the lenght of pos_edge_index\n",
    "    # and the number of zeros is equal to the length of neg_edge_index\n",
    "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(E, dtype=torch.float, device=device)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1.\n",
    "    return link_labels\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.train_pos_edge_index, #positive edges\n",
    "        num_nodes=data.num_nodes, # number of nodes\n",
    "        num_neg_samples=data.train_pos_edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    z = model.encode() #encode\n",
    "    link_logits = model.decode(z, data.train_pos_edge_index, neg_edge_index) # decode\n",
    "    \n",
    "    link_labels = get_link_labels(data.train_pos_edge_index, neg_edge_index)\n",
    "    loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    perfs = []\n",
    "    for prefix in [\"val\", \"test\"]:\n",
    "        pos_edge_index = data[f'{prefix}_pos_edge_index']\n",
    "        neg_edge_index = data[f'{prefix}_neg_edge_index']\n",
    "\n",
    "        z = model.encode() # encode train\n",
    "        link_logits = model.decode(z, pos_edge_index, neg_edge_index) # decode test or val\n",
    "        link_probs = link_logits.sigmoid() # apply sigmoid\n",
    "        \n",
    "        link_labels = get_link_labels(pos_edge_index, neg_edge_index) # get link\n",
    "        \n",
    "        perfs.append(roc_auc_score(link_labels.cpu(), link_probs.cpu())) #compute roc_auc score\n",
    "    return perfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205bf787",
   "metadata": {},
   "source": [
    "#### Training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "831973b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 0.4560, Val: 0.9138, Test: 0.9211\n",
      "Epoch: 200, Loss: 0.4478, Val: 0.9259, Test: 0.9194\n",
      "Epoch: 300, Loss: 0.4354, Val: 0.9288, Test: 0.9192\n",
      "Epoch: 400, Loss: 0.4280, Val: 0.9306, Test: 0.9251\n",
      "Epoch: 500, Loss: 0.4237, Val: 0.9327, Test: 0.9253\n",
      "Epoch: 600, Loss: 0.4166, Val: 0.9333, Test: 0.9291\n",
      "Epoch: 700, Loss: 0.4142, Val: 0.9351, Test: 0.9302\n",
      "Epoch: 800, Loss: 0.4293, Val: 0.9351, Test: 0.9302\n",
      "Epoch: 900, Loss: 0.4141, Val: 0.9351, Test: 0.9302\n",
      "Epoch: 1000, Loss: 0.4238, Val: 0.9358, Test: 0.9296\n",
      "Epoch: 1100, Loss: 0.4091, Val: 0.9358, Test: 0.9296\n",
      "Epoch: 1200, Loss: 0.4052, Val: 0.9367, Test: 0.9326\n",
      "Epoch: 1300, Loss: 0.4129, Val: 0.9370, Test: 0.9321\n",
      "Epoch: 1400, Loss: 0.4081, Val: 0.9377, Test: 0.9340\n",
      "Epoch: 1500, Loss: 0.4104, Val: 0.9386, Test: 0.9341\n",
      "Epoch: 1600, Loss: 0.4087, Val: 0.9386, Test: 0.9341\n",
      "Epoch: 1700, Loss: 0.4083, Val: 0.9395, Test: 0.9318\n",
      "Epoch: 1800, Loss: 0.4136, Val: 0.9403, Test: 0.9353\n",
      "Epoch: 1900, Loss: 0.3966, Val: 0.9404, Test: 0.9280\n",
      "Epoch: 2000, Loss: 0.4118, Val: 0.9404, Test: 0.9280\n"
     ]
    }
   ],
   "source": [
    "best_val_perf = test_perf = 0\n",
    "for epoch in range(1, 2001):\n",
    "    train_loss = train()\n",
    "    val_perf, tmp_test_perf = test()\n",
    "    if val_perf > best_val_perf:\n",
    "        best_val_perf = val_perf\n",
    "        test_perf = tmp_test_perf\n",
    "    log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    if epoch % 100 == 0:\n",
    "        print(log.format(epoch, train_loss, best_val_perf, test_perf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d13d7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.encode()\n",
    "final_edge_index_1 = model.decode_all(z)\n",
    "#Remove self loops\n",
    "bool_mask = final_edge_index_1[0] != final_edge_index_1[1]\n",
    "simulated_edges_1 = torch.empty((2,int(sum(bool_mask))))\n",
    "for item in range(final_edge_index_1.size()[0]):\n",
    "    simulated_edges_1[item] = final_edge_index_1[item][bool_mask]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "782adc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of available links are 166872\n",
      "The positive (negative) network has 7302 (1255) links \n",
      "The total amount of generated links are 23655, and  567 of them are in the negative network \n",
      "This is a 45.18 % of the total links in the negative network \n",
      "The probability of predicting correctly a link is 2.40 % in the case of the neural network\n"
     ]
    }
   ],
   "source": [
    "coincidences = to_dense_adj(negative_data[\"edge_index\"]).squeeze()*to_dense_adj(final_edge_index_1).squeeze()\n",
    "pos_edges = positive_data.edge_index.size()[1]\n",
    "neg_edges = negative_data.edge_index.size()[1]\n",
    "metric = coincidences.sum()/negative_data.edge_index.size()[1]\n",
    "print(\"The total number of available links are {}\".format(409*408))\n",
    "print(\"The positive (negative) network has {} ({}) links \".format(pos_edges,neg_edges))\n",
    "print(\"The total amount of generated links are {}, and {:4d} of them are in the negative network \".format(final_edge_index_1.size()[1],int(coincidences.sum())))\n",
    "print(\"This is a {:.2f} % of the total links in the negative network \".format(metric*100))\n",
    "print(\"The probability of predicting correctly a link is {:.2f} % in the case of the neural network\".format(100*int(coincidences.sum())/final_edge_index_1.size()[1]))\n",
    "coin_GNN_neg = coincidences.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaf420d",
   "metadata": {},
   "source": [
    "### The other way around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a203517d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/.local/lib/python3.8/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = negative_data.clone()\n",
    "data.num_nodes = len(data._store[\"x\"])\n",
    "data = train_test_split_edges(data)\n",
    "\n",
    "model, positive_data = Net().to(device), positive_data.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bb6372a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 0.4412, Val: 0.8431, Test: 0.9697\n",
      "Epoch: 200, Loss: 0.4005, Val: 0.8526, Test: 0.9303\n",
      "Epoch: 300, Loss: 0.4302, Val: 0.8526, Test: 0.9303\n",
      "Epoch: 400, Loss: 0.3904, Val: 0.8526, Test: 0.9303\n",
      "Epoch: 500, Loss: 0.4002, Val: 0.8526, Test: 0.9303\n",
      "Epoch: 600, Loss: 0.4161, Val: 0.8526, Test: 0.9303\n",
      "Epoch: 700, Loss: 0.3948, Val: 0.8526, Test: 0.9303\n",
      "Epoch: 800, Loss: 0.4014, Val: 0.8526, Test: 0.9303\n",
      "Epoch: 900, Loss: 0.4099, Val: 0.8526, Test: 0.9303\n",
      "Epoch: 1000, Loss: 0.3942, Val: 0.8526, Test: 0.9303\n",
      "Epoch: 1100, Loss: 0.3924, Val: 0.8526, Test: 0.9303\n",
      "Epoch: 1200, Loss: 0.3869, Val: 0.8526, Test: 0.9303\n",
      "Epoch: 1300, Loss: 0.3765, Val: 0.8526, Test: 0.9303\n",
      "Epoch: 1400, Loss: 0.4001, Val: 0.8526, Test: 0.9303\n",
      "Epoch: 1500, Loss: 0.3921, Val: 0.8526, Test: 0.9303\n",
      "Epoch: 1600, Loss: 0.3790, Val: 0.8526, Test: 0.9303\n",
      "Epoch: 1700, Loss: 0.3858, Val: 0.8526, Test: 0.9303\n",
      "Epoch: 1800, Loss: 0.3863, Val: 0.8526, Test: 0.9303\n",
      "Epoch: 1900, Loss: 0.4025, Val: 0.8526, Test: 0.9303\n",
      "Epoch: 2000, Loss: 0.3913, Val: 0.8526, Test: 0.9303\n"
     ]
    }
   ],
   "source": [
    "best_val_perf = test_perf = 0\n",
    "for epoch in range(1, 2001):\n",
    "    train_loss = train()\n",
    "    val_perf, tmp_test_perf = test()\n",
    "    if val_perf > best_val_perf:\n",
    "        best_val_perf = val_perf\n",
    "        test_perf = tmp_test_perf\n",
    "    log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    if epoch % 100 == 0:\n",
    "        print(log.format(epoch, train_loss, best_val_perf, test_perf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e764d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.encode()\n",
    "final_edge_index_2 = model.decode_all(z)\n",
    "#Remove self loops\n",
    "bool_mask = final_edge_index_2[0] != final_edge_index_2[1]\n",
    "simulated_edges_2 = torch.empty((2,int(sum(bool_mask))))\n",
    "for item in range(final_edge_index_2.size()[0]):\n",
    "    simulated_edges_2[item] = final_edge_index_2[item][bool_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a839df2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of available links are 166872\n",
      "The positive (negative) network has 7302 (1255) links \n",
      "The total amount of generated links are 11855, and 2289 of them are in the positive network \n",
      "This is a 31.35 % of the total links in the positive network \n",
      "The probability of predicting correctly a link 19.31 % in the neural network\n"
     ]
    }
   ],
   "source": [
    "coincidences = to_dense_adj(positive_data[\"edge_index\"]).squeeze()*to_dense_adj(final_edge_index_2).squeeze()\n",
    "pos_edges = positive_data.edge_index.size()[1]\n",
    "neg_edges = negative_data.edge_index.size()[1]\n",
    "metric = coincidences.sum()/positive_data.edge_index.size()[1]\n",
    "print(\"The total number of available links are {}\".format(409*408))\n",
    "print(\"The positive (negative) network has {} ({}) links \".format(pos_edges,neg_edges))\n",
    "print(\"The total amount of generated links are {}, and {:4d} of them are in the positive network \".format(final_edge_index_2.size()[1],int(coincidences.sum())))\n",
    "print(\"This is a {:.2f} % of the total links in the positive network \".format(metric*100))\n",
    "print(\"The probability of predicting correctly a link {:.2f} % in the neural network\".format(100*int(coincidences.sum())/final_edge_index_2.size()[1]))\n",
    "coin_GNN_pos = coincidences.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169cef43",
   "metadata": {},
   "source": [
    "### Link prediction with PageRank \n",
    "\n",
    "From the paper of _Alain Barrat_ Anxo recommended (_New Insights and Methods forPredicting Face-to-Face Contacts_), it can be checked the  _Hybrid Rooted PageRank_. We implement it in the following: \n",
    "\n",
    "*  With probability $\\alpha$ jump to root node _r_.\n",
    "*  With probability $1−\\alpha$:\n",
    "    *  Choose Network $N_{i}∈N$ with respect toprobability distribution _P_.\n",
    "    *  If there exist no outgoing edges then :\n",
    "    * Jump to root node _r_\n",
    "    *  Else:\n",
    "        From the current node c jump to a neighbornselected with a probability $w(c,n)∑c→dw(c,d)$, i. e.,proportional to the weight $w(c,n)$ of the $e(c,n)$\n",
    "\n",
    "But we will include modifications on this analysis, as _Barrat et al_ use two networks in order to extract a single social network, while we are trying to deduce one from the other. We will implement PageRank on one of them and predict the links of the other one based on this quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c176db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "def HR_pagerank(alpha,G):\n",
    "#alpha = 0.1\n",
    "    N_rounds = 10000\n",
    "    rank = [0]*len(G.nodes())\n",
    "    for rounds in range(N_rounds):\n",
    "        for node in range(409):\n",
    "            a = rd.uniform(0,1)\n",
    "            site = list(G.nodes())[node]\n",
    "            targets = list(G.nodes())\n",
    "            targets.remove(site)\n",
    "            #print(site,targets)\n",
    "            if a > alpha:\n",
    "                target = rd.choice(targets)\n",
    "                if target in list(G.neighbors(site)):\n",
    "                    c = rd.uniform(0,1)\n",
    "                    weight_target = G[site][target][\"weight\"]\n",
    "                    weight=nx.get_edge_attributes(G,'weight')\n",
    "                    av_weights = 0\n",
    "                    for n in list(G.neighbors(site)):\n",
    "                        av_weights += weight[(site,n)]\n",
    "                    av_weights /= len(list(G.neighbors(site)))\n",
    "                    if c<((weight_target)/(av_weights)):\n",
    "                        site = target\n",
    "                        rank[site] +=1\n",
    "    rank = [item/N_rounds for item in rank]\n",
    "    return rank\n",
    "\n",
    "def create_link(G,rank):\n",
    "    index_pair = rd.sample(range(len(rank)),2)\n",
    "    rd_pair = [rank[item] for item in index_pair]\n",
    "    p_rank = 1/(1 + np.exp(-(rd_pair[0]-rd_pair[1])))\n",
    "    if rd.uniform(0,1) < p_rank: \n",
    "        G.add_edge(index_pair[0],index_pair[1])\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8e4d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_rank = HR_pagerank(0.15,G_positive)\n",
    "G_simulated = nx.DiGraph()\n",
    "while len(G_simulated.edges())< final_edge_index_2.size()[1]:\n",
    "    create_link(G_simulated,positive_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d524abc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of available links are 166872\n",
      "The positive (negative) network has 7302 (1255) links \n",
      "The total amount of generated links are 11855, and  102 of them are in the negative network \n",
      "This is a 1.40 % of the total links in the negative network \n",
      "The probability of predicting correctly a link at random is 0.75 % versus a 0.86 % of the heuristics\n"
     ]
    }
   ],
   "source": [
    "coincidences = to_dense_adj(negative_data[\"edge_index\"]).squeeze()*torch.tensor(nx.adjacency_matrix(G_simulated).todense())\n",
    "pos_edges = positive_data.edge_index.size()[1]\n",
    "neg_edges = negative_data.edge_index.size()[1]\n",
    "metric = coincidences.sum()/positive_data.edge_index.size()[1]\n",
    "print(\"The total number of available links are {}\".format(409*408))\n",
    "print(\"The positive (negative) network has {} ({}) links \".format(pos_edges,neg_edges))\n",
    "print(\"The total amount of generated links are {}, and {:4d} of them are in the negative network \".format(final_edge_index_2.size()[1],int(coincidences.sum())))\n",
    "print(\"This is a {:.2f} % of the total links in the negative network \".format(metric*100))\n",
    "print(\"The probability of predicting correctly a link at random is {:.2f} % versus a {:.2f} % of the heuristics\".format(\n",
    "     neg_edges*100/(409*408),100*int(coincidences.sum())/final_edge_index_2.size()[1]))\n",
    "coin_rank_pos = coincidences.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "663de677",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_rank = HR_pagerank(0.15,G_negative)\n",
    "G_simulated = nx.DiGraph()\n",
    "while len(G_simulated.edges())< final_edge_index_1.size()[1]:\n",
    "    create_link(G_simulated,negative_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbf9fc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of available links are 166872\n",
      "The positive (negative) network has 7302 (1255) links \n",
      "The total amount of generated links are 11855, and 1087 of them are in the negative network \n",
      "This is a 14.89 % of the total links in the negative network \n",
      "The probability of predicting correctly a link at random is 4.38 % versus a 4.60 % of the heuristics\n"
     ]
    }
   ],
   "source": [
    "coincidences = to_dense_adj(positive_data[\"edge_index\"]).squeeze()*torch.tensor(nx.adjacency_matrix(G_simulated).todense())\n",
    "pos_edges = positive_data.edge_index.size()[1]\n",
    "neg_edges = negative_data.edge_index.size()[1]\n",
    "metric = coincidences.sum()/positive_data.edge_index.size()[1]\n",
    "print(\"The total number of available links are {}\".format(409*408))\n",
    "print(\"The positive (negative) network has {} ({}) links \".format(pos_edges,neg_edges))\n",
    "print(\"The total amount of generated links are {}, and {:4d} of them are in the negative network \".format(final_edge_index_2.size()[1],int(coincidences.sum())))\n",
    "print(\"This is a {:.2f} % of the total links in the negative network \".format(metric*100))\n",
    "print(\"The probability of predicting correctly a link at random is {:.2f} % versus a {:.2f} % of the heuristics\".format(\n",
    "     pos_edges*100/(409*408),100*int(coincidences.sum())/final_edge_index_1.size()[1]))\n",
    "coin_rank_neg = coincidences.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de98345",
   "metadata": {},
   "source": [
    "### Randomly created network\n",
    "\n",
    "We compare the results from the GNN and the PageRank with a randomly created network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8cf840b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is an average of 506.99 coincidences.\n"
     ]
    }
   ],
   "source": [
    "import random as rd \n",
    "coincidences_total = 0\n",
    "for sim in range(100):\n",
    "    G_random = nx.DiGraph()\n",
    "    G_random.add_nodes_from(range(409))\n",
    "    for trial in range(final_edge_index_2.size()[1]):\n",
    "        rd_sample = rd.sample(range(409),2)\n",
    "        G_random.add_edge(rd_sample[0],rd_sample[1]) \n",
    "\n",
    "    coincidences_random = len([(u,v) for (u,v) in G_random.edges() if G_positive.has_edge(u,v)])\n",
    "    coincidences_total += coincidences_random\n",
    "print(\"There is an average of {:.2f} coincidences.\".format(coincidences_total/100))\n",
    "coin_random_pos = coincidences_total/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cdf20c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is an average of 25.16 coincidences.\n"
     ]
    }
   ],
   "source": [
    "import random as rd \n",
    "coincidences_total = 0\n",
    "for sim in range(100):\n",
    "    G_random = nx.DiGraph()\n",
    "    G_random.add_nodes_from(range(409))\n",
    "    for trial in range(final_edge_index_2.size()[1]):\n",
    "        rd_sample = rd.sample(range(409),2)\n",
    "        G_random.add_edge(rd_sample[0],rd_sample[1]) \n",
    "\n",
    "    coincidences_random = len([(u,v) for (u,v) in G_random.edges() if G_negative.has_edge(u,v)])\n",
    "    coincidences_total += coincidences_random\n",
    "print(\"There is an average of {:.2f} coincidences.\".format(coincidences_total/100))\n",
    "coin_random_neg = coincidences_total/100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ca8d9",
   "metadata": {},
   "source": [
    "### Results compared in coincidences with the original networks\n",
    "\n",
    "We write $+/-$ as the prediction power of the negative network depending on the positive one and $-/+$ to design the other way around. Results are expresed in terms of the number of links the method is able to reproduce in the original networks. \n",
    "\n",
    "|  | +/- | -/+ |\n",
    "| --- | --- | --- |\n",
    "| **Random network** | 0.031 | 0.105 |\n",
    "| --- | --- | --- |\n",
    "| **PageRank heuristics** | 0.106 | 0.167 |\n",
    "| --- | --- | --- |\n",
    "| **GCN**  | 0.391 | 0.177 |\n",
    "| --- | --- | --- |\n",
    "| **GCN with class/group info** | 0.426 | 0.305 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0ba46c",
   "metadata": {},
   "source": [
    "The quantities may diverge a little bit because the autoencoder generate a different number of links depending on the realization, but it is clear that Graph Convolutional Networks outperform the heuristics used in link prediction. It is difficult to say something about the structural relationship between both networks, but there is some kind of relationship, as there is a difference in scoring for all the methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e9c8ee",
   "metadata": {},
   "source": [
    "**Work to be done** \n",
    "\n",
    "1) Check the structural balance theory, computing global equilibria in both networks, in order to generate ensembles. \n",
    "\n",
    "2) Graph neural networks can also be used to predict labeling in edges, it could be used to proof structural balance theory from other perspective. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
