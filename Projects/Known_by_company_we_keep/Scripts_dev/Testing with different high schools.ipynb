{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bc7b170",
   "metadata": {},
   "source": [
    "## Build the Data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffbadf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random as rd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch_geometric.data as data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import negative_sampling,train_test_split_edges,to_dense_adj\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from sklearn import preprocessing\n",
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c572ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Clean and normalize data\n",
    "###Cole para adestrar\n",
    "nodes_h1 = pd.read_csv(r\"Nodes_t1.csv\",sep=\",\",encoding = 'unicode_escape')\n",
    "edges_h1 = pd.read_csv(r\"Edges_t1.csv\",sep=\";\",encoding = 'unicode_escape')\n",
    "nodes_h1.drop(\"ID\",axis=1,inplace=True)\n",
    "##cole para testear\n",
    "nodes_h2 = pd.read_csv(r\"Nodes_t4.csv\",sep=\",\",encoding = 'unicode_escape')\n",
    "edges_h2 = pd.read_csv(r\"Edges_t4.csv\",sep=\",\",encoding = 'unicode_escape')\n",
    "nodes_h2 = nodes_h2[nodes_h1.columns]\n",
    "nodes_h1[\"Escuela\"] = \"h1\"\n",
    "nodes_h2[\"Escuela\"] = \"h2\"\n",
    "edges_h1[\"Escuela\"] = \"h1\"\n",
    "edges_h2[\"Escuela\"] = \"h2\"\n",
    "#edges_h2.drop(\"relacion\",axis=1,inplace = True)\n",
    "edges_h2.columns = edges_h1.columns\n",
    "edges_h1[[\"from\",\"to\"]] = edges_h1[[\"from\",\"to\"]].apply(lambda x:x-edges_h1[\"from\"].min())\n",
    "edges_h2[[\"from\",\"to\"]] = edges_h2[[\"from\",\"to\"]].apply(lambda x:x+1+edges_h1[\"from\"].max()-edges_h2[\"from\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea698a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(645, 646)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_edges[1][\"from\"].max(),datasets_edges[2][\"from\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f70be119",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_datasets = 6\n",
    "datasets_nodes = [0]*len_datasets\n",
    "datasets_edges = [0]*len_datasets\n",
    "for i in range(0,len_datasets):\n",
    "    datasets_nodes[i] = pd.read_csv(r\"Nodes_t\"+str(i+1)+\".csv\",sep=\",\",encoding = 'unicode_escape')\n",
    "    datasets_edges[i] = pd.read_csv(r\"Edges_t\"+str(i+1)+\".csv\",sep=\",\",encoding = 'unicode_escape')\n",
    "    datasets_nodes[i].drop(\"ID\",axis=1,inplace=True)\n",
    "    if i == 0:\n",
    "        datasets_edges[i][[\"from\",\"to\"]] = datasets_edges[i][[\"from\",\"to\"]].apply(lambda x:x-datasets_edges[i][\"from\"].min())\n",
    "    else:\n",
    "        datasets_edges[i][[\"from\",\"to\"]] = datasets_edges[i][[\"from\",\"to\"]].apply(lambda x:x+1+datasets_edges[i-1][\"from\"].max()-datasets_edges[i][\"from\"].min())\n",
    "    \n",
    "edges_total = pd.concat(datasets_edges,axis=0).reset_index().drop(\"index\",axis=1)\n",
    "edges_total = edges_total[edges_total[\"weight\"] != 1].reset_index().drop(\"index\",axis=1)\n",
    "edges_total[\"weight\"] = edges_total[\"weight\"].apply(lambda x: np.sign(x)).replace({-1:0})\n",
    "nodes_total = pd.concat(datasets_nodes,axis=0).reset_index().drop(\"index\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70333e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_total = pd.concat([edges_h1,edges_h2],axis=0).reset_index().drop(\"index\",axis=1)\n",
    "edges_total = edges_total[edges_total[\"weight\"] != 1].reset_index().drop(\"index\",axis=1)\n",
    "edges_total[\"weight\"] = edges_total[\"weight\"].apply(lambda x: np.sign(x)).replace({-1:0})\n",
    "nodes_total = pd.concat([nodes_h1,nodes_h2],axis=0).reset_index().drop(\"index\",axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842da7ba",
   "metadata": {},
   "source": [
    "### Build the data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33b0ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_dummy = pd.get_dummies(nodes_total,drop_first=True)\n",
    "x = nodes_dummy.values \n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "nodes_norm = pd.DataFrame(x_scaled,columns = nodes_dummy.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3919349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2754, 19], edge_index=[2, 28307], edge_attr=[28307, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data = data.Data(x=torch.tensor(nodes_norm.to_numpy(),dtype=torch.float32),\n",
    "                          edge_index=torch.tensor(edges_total[[\"from\",\"to\"]].to_numpy().T),\n",
    "                          edge_attr=torch.tensor((pd.get_dummies(edges_total[[\"weight\"]]).to_numpy())))\n",
    "total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e081fb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327 2345 82\n",
      "True\n",
      "train set\t [88, 138, 368, 216, 309, 251, 37, 370, 16, 140]\n",
      "test set \t [1565, 440, 2578, 1551, 1690, 2634, 1842, 1093, 2456, 2500]\n",
      "val set  \t [45, 233, 265, 82, 134, 25, 123, 371, 167, 391]\n"
     ]
    }
   ],
   "source": [
    "node_list = list(range(len(nodes_total)))\n",
    "train_val_set = node_list[0:len(nodes_h1)]\n",
    "rd.shuffle(train_val_set)\n",
    "train_set = train_val_set[0:round(len(train_val_set)*0.8)]\n",
    "val_set = train_val_set[len(train_set):len(train_val_set)]\n",
    "test_set = node_list[len(nodes_h1):len(nodes_h1)+len(nodes_h2)]\n",
    "rd.shuffle(test_set)\n",
    "print(len(train_set),len(test_set),len(val_set))\n",
    "print(len(train_set)+len(test_set)+len(val_set) == len(nodes_total))\n",
    "\n",
    "print(\"train set\\t\",train_set[:10])\n",
    "print(\"test set \\t\",test_set[:10])\n",
    "print(\"val set  \\t\",val_set[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5174e8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mask \t tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1])\n",
      "test mask  \t tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "val mask   \t tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# build test train val masks\n",
    "\n",
    "train_mask = torch.zeros(len(nodes_total),dtype=torch.long, device=device)\n",
    "for i in train_set:\n",
    "    train_mask[i] = 1.\n",
    "\n",
    "test_mask = torch.zeros(len(nodes_total),dtype=torch.long, device=device)\n",
    "for i in test_set:\n",
    "    test_mask[i] = 1.\n",
    "    \n",
    "val_mask = torch.zeros(len(nodes_total),dtype=torch.long, device=device)\n",
    "for i in val_set:\n",
    "    val_mask[i] = 1.\n",
    "    \n",
    "print(\"train mask \\t\",train_mask[0:15])\n",
    "print(\"test mask  \\t\",test_mask[0:15])\n",
    "print(\"val mask   \\t\",val_mask[0:15]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93b09dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2754, 19], edge_index=[2, 28307], edge_attr=[28307, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a876e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "befor\t\t Data(x=[2754, 19], edge_index=[2, 28307], edge_attr=[28307, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"befor\\t\\t\",total_data)\n",
    "total_data.x = None\n",
    "total_data.edge_attr = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0819cf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after\t\t Data(edge_index=[2, 28307], train_mask=[2754], test_mask=[2754], val_mask=[2754])\n"
     ]
    }
   ],
   "source": [
    "# add masks\n",
    "total_data.train_mask = train_mask\n",
    "total_data.test_mask = test_mask\n",
    "total_data.val_mask = val_mask\n",
    "\n",
    "print(\"after\\t\\t\",total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf65ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Node2Vec(total_data.edge_index, embedding_dim=128, walk_length=40,\n",
    "             context_size=20, walks_per_node=5,\n",
    "             num_negative_samples=1, p=1, q=1, sparse=True).to(device)\n",
    "\n",
    "loader = model.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d549f573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.8907\n",
      "Epoch: 20, Loss: 0.7447\n",
      "Epoch: 30, Loss: 0.7285\n",
      "Epoch: 40, Loss: 0.7244\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_335895/1101141748.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m#acc = test()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_335895/1101141748.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_rw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_rw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    z = model()\n",
    "    acc = model.test(z[total_data.train_mask], total_data.y[total_data.train_mask],\n",
    "                     z[total_data.test_mask], total_data.y[total_data.test_mask],\n",
    "                     max_iter=10)\n",
    "    return acc\n",
    "\n",
    "\n",
    "for epoch in range(1, 121):\n",
    "    loss = train()\n",
    "    #acc = test()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13fdc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = data.Data(x=torch.tensor(nodes_norm.to_numpy(),dtype=torch.float32),\n",
    "                          edge_index=torch.tensor(edges_total[[\"from\",\"to\"]].to_numpy().T),\n",
    "                          edge_attr=torch.tensor((pd.get_dummies(edges_total[\"weight\"]).to_numpy())))\n",
    "total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2278fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model()\n",
    "# from tensor to numpy\n",
    "emb_128 = z.detach().cpu().numpy()\n",
    "\n",
    "# convert edge attributes from categorical to numerical\n",
    "edge_attr_cat = total_data.edge_attr.numpy()\n",
    "print(\"Categorical edge attributes:\\n\",edge_attr_cat[:3])\n",
    "\n",
    "edge_attr = []\n",
    "for i in edge_attr_cat:\n",
    "    edge_attr.append(np.nonzero(i)[0][0])\n",
    "\n",
    "print(\"\\n\\nNumerical edge attributes:\\n\",edge_attr[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65504ca5",
   "metadata": {},
   "source": [
    "## Compute edge embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ba474",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_embedding = []\n",
    "for u,v in total_data.edge_index.t():\n",
    "    edge_embedding.append(np.maximum(emb_128[u],emb_128[v]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08222d45",
   "metadata": {},
   "source": [
    "### Make dataframe from embedding and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a455f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_h1 = len(edges_total[edges_total[\"Escuela\"] == \"h1\"])\n",
    "emb_df_h1 = pd.concat([pd.DataFrame(edge_embedding[:len_h1]),pd.DataFrame(edge_attr[:len_h1],columns=[\"label\"])],axis=1)\n",
    "\n",
    "emb_df_h2 = pd.concat([pd.DataFrame(edge_embedding[len_h1:]),pd.DataFrame(edge_attr[len_h1:],columns=[\"label\"])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8bb9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df_h1 = emb_df_h1.sample(frac=1)\n",
    "emb_df_h2 = emb_df_h2.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f7620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "sc = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e92ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = emb_df_h1.drop(\"label\",axis=1)\n",
    "y_train = emb_df_h1[\"label\"]\n",
    "X_test = emb_df_h2.drop(\"label\",axis=1)\n",
    "y_test = emb_df_h2[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf0f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.fit(X_train)\n",
    "emb_x_train = sc.transform(X_train)\n",
    "emb_y_train = y_train\n",
    "emb_x_test = sc.transform(X_test)\n",
    "emb_y_test = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5f38b",
   "metadata": {},
   "source": [
    "### Oversample minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde46cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c9807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = SMOTE(random_state=0,sampling_strategy=\"minority\")\n",
    "emb_x_resampled, emb_y_resampled = ros.fit_resample(emb_x_train, emb_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04a6bf",
   "metadata": {},
   "source": [
    "### Training and prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fa712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af91c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=7,random_state=10,class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(emb_x_resampled,emb_y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478d81d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "fig= plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "sns.heatmap(np.round(confusion_matrix(emb_y_test,np.around(clf.predict(emb_x_test)))/len(emb_y_test)*100,2),\n",
    "            annot=True,ax=ax,cmap=\"coolwarm\")\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"Current\")\n",
    "print(\"\\n\")\n",
    "print(classification_report(emb_y_test,clf.predict(emb_x_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5defae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cfed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128,activation=\"relu\",input_shape=(emb_x_train.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64,activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(32,activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(8,activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=10e-5),\n",
    "         loss=\"binary_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "model_history = model.fit(emb_x_resampled,emb_y_resampled,validation_data=(emb_x_test,emb_y_test),epochs=250,verbose=1,batch_size=128,\n",
    "                         #callbacks=[tf.keras.callbacks.EarlyStopping(patience=50)])\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7443460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "fig= plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "sns.heatmap(np.round(confusion_matrix(emb_y_test,np.around(model.predict(emb_x_test)))/len(emb_y_test)*100,2),\n",
    "            annot=True,ax=ax,cmap=\"coolwarm\")\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"Current\")\n",
    "print(\"\\n\")\n",
    "print(classification_report(emb_y_test,np.around(model.predict(emb_x_test))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa2180",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "pd.DataFrame(model_history.history)[[\"loss\",\"val_loss\"]].plot(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd62376",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "pd.DataFrame(model_history.history)[[\"accuracy\",\"val_accuracy\"]].plot(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "# fit and transform using PCA\n",
    "pca = PCA(n_components=2)\n",
    "edge_emb2d = pca.fit_transform(edge_embedding)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(dict(edge_att=edge_attr))\n",
    "colors = {0:\"red\",1:\"blue\"}\n",
    "plt.title(\"edge embedding in 2D\")\n",
    "plt.scatter(edge_emb2d[:,0],edge_emb2d[:,1],c=df.edge_att.map(colors),alpha=0.2)\n",
    "plt.show()\n",
    "\n",
    "# not so good but we are using PCA to reduce the dim from 128 to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d913ecf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emb_128' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_343888/2372982034.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memb_128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'emb_128' is not defined"
     ]
    }
   ],
   "source": [
    "emb_128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf93265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
