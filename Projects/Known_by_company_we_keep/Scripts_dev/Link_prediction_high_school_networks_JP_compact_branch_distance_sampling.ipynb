{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6badee70",
   "metadata": {},
   "source": [
    "# High school network and deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c57a2",
   "metadata": {},
   "source": [
    "Our point is, considering the positive and negative networks as two different directed graphs, use a GConv network with previous autoencoder (Tutorial 12 Pytorch Geometric) to predict links from one of the networks, then compare the predicted_edges with the negative ones. Consider the positive as the training set and compare it to the negatives in the test one. \n",
    "\n",
    "* We are going to use a graph autoencoder, which is a non-supervised neural network that takes data, translate them to another representation (the one the neural network extracts from them) and then try to rebuild the original data. The representation it learns is based on the structure of the network.\n",
    "\n",
    "* We will use also a heuristic method, called PageRank method, traditionally used in link prediction, where the probability of a link depends on a variable called rank. \n",
    "\n",
    "* We compare it with a null method, a random graph. \n",
    "\n",
    "In both of the comparative methods of the GNN, negative links can only be placed where there are not positive links. \n",
    "\n",
    "The graph autoencoder generate a fixed number of links depending on built-in functions, so we are taking these number of links in order to establish comparison with other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4d72f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  In this variation we are going to remove a whole class and let the neural network rebuild it. We are considering classes as main components of the whole. This is done just in the positive network. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ca0dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "import torch\n",
    "import torch_geometric.data as data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import negative_sampling,train_test_split_edges,to_dense_adj\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from sklearn import preprocessing\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d206f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "## Just prepare the data\n",
    "nodes = pd.read_csv(r\"Nodes_t1.csv\",sep=\";\",encoding = 'unicode_escape')\n",
    "edges = pd.read_csv(r\"Edges_t1.csv\",sep=\";\",encoding = 'unicode_escape')\n",
    "edges = edges.apply(lambda x: x - x.min(),axis = 0)\n",
    "###Erase ESO \n",
    "nodes[\"Curso\"] = nodes[\"Curso\"].astype(str).str[0].astype(\"int64\")\n",
    "del nodes[\"Unnamed: 0\"]\n",
    "edges[\"weight\"] = edges[\"weight\"].apply(lambda x:x+1)\n",
    "pos_edges = edges[edges[\"weight\"]> 3]\n",
    "neg_edges = edges[edges[\"weight\"]< 3]\n",
    "G_positive = nx.from_pandas_edgelist(pos_edges, \"from\", \"to\",create_using=nx.DiGraph,edge_attr=\"weight\")\n",
    "G_negative = nx.from_pandas_edgelist(neg_edges, \"from\", \"to\",create_using=nx.DiGraph,edge_attr=\"weight\")\n",
    "G_negative.add_nodes_from(range(nodes.index.max()+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26210f0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    We need to remove from the data the edges at a certain distance of a random selected student. We then eliminate all the edges that come from that student, at then from the contacts of that student, up to a a cutoff.\n",
    "</div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c2ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "selected_student = [rd.choice(edges[\"from\"].unique())]\n",
    "d_cutoff = 3\n",
    "d = 0\n",
    "edges_saved = []\n",
    "while d < d_cutoff:\n",
    "    for student in selected_student:\n",
    "        selected_edges = edges[[\"from\",\"to\"]][edges[\"from\"] == student]\n",
    "        nei = []\n",
    "        for row in selected_edges.iterrows():\n",
    "            nei.append(row[1][\"to\"])\n",
    "            edges_saved.append([row[1][\"from\"],row[1][\"to\"]])\n",
    "    selected_student = nei\n",
    "    d += 1\n",
    "edges_saved = torch.Tensor(list(zip(*edges_saved))).type(torch.LongTensor)\n",
    "#list(set(list(zip(*edges_saved))) & set(edges[[\"from\",\"to\"]].to_numpy()))\n",
    "saved_df = pd.DataFrame({\"from\":edges_saved[0],\"to\":edges_saved[1]})\n",
    "edges = pd.concat([edges[[\"from\",\"to\"]],saved_df]).drop_duplicates(keep=False)\n",
    "nodes_class = list(saved_df[\"from\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d0bc34",
   "metadata": {},
   "source": [
    "## Graph autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a6a20",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1795f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.read_csv(r\"Nodes_t1.csv\",sep=\";\",encoding = 'unicode_escape')\n",
    "edges = pd.read_csv(r\"Edges_t1.csv\",sep=\";\",encoding = 'unicode_escape')\n",
    "edges = edges.apply(lambda x: x - x.min(),axis = 0)\n",
    "###Erase ESO \n",
    "nodes[\"Curso\"] = nodes[\"Curso\"].astype(str).str[0].astype(\"int64\")\n",
    "del nodes[\"Unnamed: 0\"]\n",
    "### Separate positive from negative networks\n",
    "pos_edges = edges[edges[\"weight\"]> 2]\n",
    "neg_edges = edges[edges[\"weight\"]< 2] \n",
    "### One hot encode and normalize node attributes\n",
    "nodes_dummy = pd.get_dummies(nodes[[\"Curso\",\"Grupo\"]])\n",
    "rng = np.random.default_rng()\n",
    "#nodes_dummy = pd.DataFrame(rng.integers(0, 2, size=(409, 10)), columns=list('ABCDEFGHIJ'))\n",
    "\n",
    "x = nodes_dummy.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "nodes_norm = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44e28034",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Without including class and group information \n",
    "positive_data = data.Data(x=torch.tensor(nodes_norm.to_numpy(),dtype=torch.float32),\n",
    "                          edge_index=torch.tensor(pos_edges[[\"from\",\"to\"]].to_numpy().T))\n",
    "negative_data = data.Data(x=torch.tensor(nodes_norm.to_numpy(),dtype=torch.float32),\n",
    "                          edge_index=torch.tensor(neg_edges[[\"from\",\"to\"]].to_numpy().T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079082e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/.local/lib/python3.8/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "data = positive_data.clone()\n",
    "data.num_nodes = len(data._store[\"x\"])\n",
    "data = train_test_split_edges(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d91946",
   "metadata": {},
   "source": [
    "### Models for the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaafd14",
   "metadata": {},
   "source": [
    "#### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f13d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(data.num_features, 128)\n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "\n",
    "    def encode(self):\n",
    "        x = self.conv1(data.x, data.train_pos_edge_index) # convolution 1\n",
    "        x = x.relu()\n",
    "        return self.conv2(x, data.train_pos_edge_index) # convolution 2\n",
    "\n",
    "    def decode(self, z, pos_edge_index, neg_edge_index): # only pos and neg edges\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1) # concatenate pos and neg edges\n",
    "        logits = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)  # dot product \n",
    "        return logits\n",
    "\n",
    "    def decode_all(self, z): \n",
    "        prob_adj = z @ z.t() # get adj NxN\n",
    "        return (prob_adj > 1-10e-10).nonzero(as_tuple=False).t() # get predicted edge_list "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2c32d",
   "metadata": {},
   "source": [
    "#### Set the parameters and move data to autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e9fb8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, positive_data = Net().to(device), positive_data.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df952510",
   "metadata": {},
   "source": [
    "#### Algorithms of training and evaluation (Tutorial PyG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "266bf880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    # returns a tensor:\n",
    "    # [1,1,1,1,...,0,0,0,0,0,..] with the number of ones is equel to the lenght of pos_edge_index\n",
    "    # and the number of zeros is equal to the length of neg_edge_index\n",
    "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(E, dtype=torch.float, device=device)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1.\n",
    "    return link_labels\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.train_pos_edge_index, #positive edges\n",
    "        num_nodes=data.num_nodes, # number of nodes\n",
    "        num_neg_samples=data.train_pos_edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    z = model.encode() #encode\n",
    "    link_logits = model.decode(z, data.train_pos_edge_index, neg_edge_index) # decode\n",
    "    \n",
    "    link_labels = get_link_labels(data.train_pos_edge_index, neg_edge_index)\n",
    "    loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    perfs = []\n",
    "    for prefix in [\"val\", \"test\"]:\n",
    "        pos_edge_index = data[f'{prefix}_pos_edge_index']\n",
    "        neg_edge_index = data[f'{prefix}_neg_edge_index']\n",
    "\n",
    "        z = model.encode() # encode train\n",
    "        link_logits = model.decode(z, pos_edge_index, neg_edge_index) # decode test or val\n",
    "        link_probs = link_logits.sigmoid() # apply sigmoid\n",
    "        \n",
    "        link_labels = get_link_labels(pos_edge_index, neg_edge_index) # get link\n",
    "        \n",
    "        perfs.append(roc_auc_score(link_labels.cpu(), link_probs.cpu())) #compute roc_auc score\n",
    "    return perfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205bf787",
   "metadata": {},
   "source": [
    "#### Training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "831973b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 0.4471, Val: 0.9050, Test: 0.9161\n",
      "Epoch: 200, Loss: 0.4390, Val: 0.9134, Test: 0.9239\n",
      "Epoch: 300, Loss: 0.4256, Val: 0.9134, Test: 0.9239\n",
      "Epoch: 400, Loss: 0.4325, Val: 0.9157, Test: 0.9291\n",
      "Epoch: 500, Loss: 0.4188, Val: 0.9178, Test: 0.9299\n",
      "Epoch: 600, Loss: 0.4285, Val: 0.9178, Test: 0.9299\n",
      "Epoch: 700, Loss: 0.4113, Val: 0.9178, Test: 0.9299\n",
      "Epoch: 800, Loss: 0.4110, Val: 0.9206, Test: 0.9297\n",
      "Epoch: 900, Loss: 0.4120, Val: 0.9239, Test: 0.9323\n",
      "Epoch: 1000, Loss: 0.4217, Val: 0.9240, Test: 0.9317\n",
      "Epoch: 1100, Loss: 0.4100, Val: 0.9250, Test: 0.9307\n",
      "Epoch: 1200, Loss: 0.4030, Val: 0.9252, Test: 0.9324\n",
      "Epoch: 1300, Loss: 0.4108, Val: 0.9259, Test: 0.9353\n",
      "Epoch: 1400, Loss: 0.4020, Val: 0.9266, Test: 0.9335\n",
      "Epoch: 1500, Loss: 0.4150, Val: 0.9271, Test: 0.9330\n",
      "Epoch: 1600, Loss: 0.3999, Val: 0.9282, Test: 0.9361\n",
      "Epoch: 1700, Loss: 0.4052, Val: 0.9293, Test: 0.9382\n",
      "Epoch: 1800, Loss: 0.4003, Val: 0.9293, Test: 0.9382\n",
      "Epoch: 1900, Loss: 0.4067, Val: 0.9305, Test: 0.9399\n",
      "Epoch: 2000, Loss: 0.3973, Val: 0.9319, Test: 0.9386\n"
     ]
    }
   ],
   "source": [
    "best_val_perf = test_perf = 0\n",
    "for epoch in range(1, 2001):\n",
    "    train_loss = train()\n",
    "    val_perf, tmp_test_perf = test()\n",
    "    if val_perf > best_val_perf:\n",
    "        best_val_perf = val_perf\n",
    "        test_perf = tmp_test_perf\n",
    "    log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    if epoch % 100 == 0:\n",
    "        print(log.format(epoch, train_loss, best_val_perf, test_perf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521e83b9",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "    We will produce the links, and from them extract the ones referred to the particular course we are trying to rebuild. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d13d7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.encode()\n",
    "final_edge_index_1 = model.decode_all(z)\n",
    "#Remove self loops\n",
    "bool_mask = final_edge_index_1[0] != final_edge_index_1[1]\n",
    "simulated_edges_1 = torch.empty((2,int(sum(bool_mask))))\n",
    "for item in range(final_edge_index_1.size()[0]):\n",
    "    simulated_edges_1[item] = final_edge_index_1[item][bool_mask]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f46fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_saved_compare = list(zip(*edges_saved))\n",
    "edges_saved_compare = set(list(map(lambda item: (int(item[0]),int(item[1])),edges_saved_compare)))\n",
    "\n",
    "simulated_edges_compare = list(zip(*simulated_edges_1))\n",
    "simulated_edges_compare = set(list(map(lambda item: (int(item[0]),int(item[1])),simulated_edges_compare)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cc40a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of links that were in the data is the 0.73 of the total\n"
     ]
    }
   ],
   "source": [
    "precision = round(len(edges_saved_compare.intersection(simulated_edges_compare))/len(edges_saved[0]),2)\n",
    "print(\"The amount of links that were in the data is the {} of the total\".format(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48905ecc",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "   Compute the coincidences. To do this, we check if each of the predicted links was in the original data. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "598e30f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1259, 23)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_links = 0\n",
    "for item in simulated_edges_compare: \n",
    "    if item[0] in nodes_class: \n",
    "        n_links += 1\n",
    "n_links,len(nodes_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "415a101a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We generated 22550 links, 1259 of them are in the cut region \n",
      "We cut a total of 497 links\n",
      "From 1259, 362 of 497 were found in the simulated links. \n"
     ]
    }
   ],
   "source": [
    "print(f\"We generated {len(simulated_edges_compare)} links, {n_links} of them are in the cut region \")\n",
    "print(f\"We cut a total of {len(edges_saved_compare)} links\")\n",
    "print(f\"From {n_links}, {int(precision*len(edges_saved_compare))} of {len(edges_saved_compare)} were found in the simulated links. \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
