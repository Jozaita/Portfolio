{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bc7b170",
   "metadata": {},
   "source": [
    "## Build the Data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffbadf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random as rd\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch_geometric.data as data\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import negative_sampling,train_test_split_edges,to_dense_adj\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn import preprocessing\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_auc_score,classification_report,confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from itertools import product\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8e41d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_edges(nodes,edges):\n",
    "    class_classi = []\n",
    "    for edge in edges.iterrows():\n",
    "        try:\n",
    "            agent_from = nodes.iloc[list(nodes[\"ID\"]).index(edge[1][\"from\"])]\n",
    "            agent_to = nodes.iloc[list(nodes[\"ID\"]).index(edge[1][\"to\"])]\n",
    "            if (agent_from[\"Curso\"] == agent_to[\"Curso\"]):\n",
    "                    class_classi.append(str(agent_from[\"Curso\"]))\n",
    "            else:\n",
    "                    class_classi.append(\"Intergroup\")\n",
    "        except:\n",
    "            class_classi.append(\"Missing\")\n",
    "    return class_classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08ecd95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_values(df_edges,df_nodes):\n",
    "    \n",
    "    if df_edges[\"to\"].nunique() > df_edges[\"from\"].nunique():\n",
    "        new_vect = np.arange(df_edges[\"to\"].nunique())\n",
    "        ###\n",
    "        translation =  dict(zip(list(df_edges.sort_values(by=\"to\")[\"to\"].unique()),new_vect))\n",
    "    else:\n",
    "        new_vect = np.arange(df_edges[\"from\"].nunique())\n",
    "        ###\n",
    "        translation =  dict(zip(list(df_edges.sort_values(by=\"from\")[\"from\"].unique()),new_vect))\n",
    "        \n",
    "        \n",
    "    df_edges[[\"from\",\"to\"]] = df_edges[[\"from\",\"to\"]].replace(translation)\n",
    "    df_nodes[\"ID\"] = df_nodes[\"ID\"].replace(translation)\n",
    "    return df_edges,df_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6958ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "    len_datasets = 13\n",
    "    datasets_nodes = [0]*len_datasets\n",
    "    datasets_edges = [0]*len_datasets\n",
    "    datasets_edges_train = [0]*len_datasets\n",
    "    datasets_edges_test = [0]*len_datasets\n",
    "    graficas_train = [0]*len_datasets\n",
    "    graficas_test = [0]*len_datasets\n",
    "    data_list_train = [0]*len_datasets\n",
    "    data_list_test = [0]*len_datasets\n",
    "    data_list_train = [0]*len_datasets\n",
    "    data_list_test = [0]*len_datasets\n",
    "    graficas = [0]*len_datasets\n",
    "    for i in range(0,len_datasets):\n",
    "        datasets_nodes[i] = pd.read_csv(r\"Coles/Nodes/Nodes_t\"+str(i+1)+\".csv\",sep=\",\",encoding = 'unicode_escape')\n",
    "        datasets_edges[i] = pd.read_csv(r\"Coles/Edges/Edges_t\"+str(i+1)+\".csv\",sep=\",\",encoding = 'unicode_escape')\n",
    "        #datasets_edges[i][[\"from\",\"to\"]] = datasets_edges[i][[\"from\",\"to\"]].apply(lambda x:x-min(datasets_edges[i][\"to\"].min(),datasets_edges[i][\"from\"].min()))    \n",
    "        datasets_edges[i][\"Escuela\"] = i\n",
    "        datasets_edges[i][\"weight\"] = datasets_edges[i][\"weight\"].apply(lambda x: np.sign(x)).replace({-1:0}).reset_index().drop(\"index\",axis=1)\n",
    "        datasets_edges[i],datasets_nodes[i] = rearrange_values(datasets_edges[i],datasets_nodes[i])\n",
    "        datasets_edges[i][\"class_classif\"] = f_edges(datasets_nodes[i],datasets_edges[i])\n",
    "        datasets_edges_train[i], datasets_edges_test[i] = train_test_split(datasets_edges[i], test_size=0.2)\n",
    "        graficas_train[i] = nx.from_pandas_edgelist(datasets_edges_train[i],source=\"from\",target=\"to\",create_using=nx.DiGraph())\n",
    "        graficas_test[i] = nx.from_pandas_edgelist(datasets_edges_test[i],source=\"from\",target=\"to\",create_using=nx.DiGraph())\n",
    "        data_list_train[i] = data.Data(edge_index = torch.tensor(datasets_edges_train[i][[\"from\",\"to\"]].to_numpy().T))\n",
    "        data_list_test[i] = data.Data(edge_index = torch.tensor(datasets_edges_test[i][[\"from\",\"to\"]].to_numpy().T))\n",
    "\n",
    "    data_loader_train = DataLoader(data_list_train, batch_size=1)\n",
    "    data_loader_test = DataLoader(data_list_test,batch_size = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a19ed3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3514"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([409,\n",
    "238,\n",
    "534,\n",
    "232,\n",
    "512,\n",
    "156,\n",
    "110,\n",
    "223,\n",
    "106,\n",
    "80,\n",
    "209,\n",
    "319,\n",
    "386])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd44c346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409\n",
      "238\n",
      "534\n",
      "232\n",
      "512\n",
      "156\n",
      "110\n",
      "223\n",
      "106\n",
      "80\n",
      "209\n",
      "319\n",
      "386\n"
     ]
    }
   ],
   "source": [
    "for data in datasets_nodes:\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "147abbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_DataLoader__initialized',\n",
       " '_DataLoader__multiprocessing_context',\n",
       " '_IterableDataset_len_called',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_auto_collation',\n",
       " '_dataset_kind',\n",
       " '_get_iterator',\n",
       " '_index_sampler',\n",
       " '_is_protocol',\n",
       " '_iterator',\n",
       " 'batch_sampler',\n",
       " 'batch_size',\n",
       " 'check_worker_number_rationality',\n",
       " 'collate_fn',\n",
       " 'dataset',\n",
       " 'drop_last',\n",
       " 'exclude_keys',\n",
       " 'follow_batch',\n",
       " 'generator',\n",
       " 'multiprocessing_context',\n",
       " 'num_workers',\n",
       " 'persistent_workers',\n",
       " 'pin_memory',\n",
       " 'prefetch_factor',\n",
       " 'sampler',\n",
       " 'timeout',\n",
       " 'worker_init_fn']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(data_loader_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac78143",
   "metadata": {},
   "source": [
    "### Final choices for the hyperparameters\n",
    "The parameters that are not described are set as in the code below.\n",
    "\n",
    " <u> Configurations without best friends </u> <br>\n",
    "    -  DeepWalk _(p=1,q=1)_ <br>\n",
    "    - BFS _(p=1,q=10)_ <br>\n",
    "    - DFS _(p=10,q=1)_ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4afb853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(data_loader):\n",
    "\n",
    "    tolerance = 1e-3\n",
    "    total_embeddings = pd.DataFrame()\n",
    "    i = 0\n",
    "    for data in data_loader.dataset:\n",
    "        print(\"Dataset: {}\".format(i))\n",
    "        model = Node2Vec(data.edge_index, embedding_dim=128, walk_length=30,\n",
    "                 context_size=10, walks_per_node=10,\n",
    "                 num_negative_samples=1, p=1, q=4, sparse=True).to(device)\n",
    "        loader = model.loader(batch_size=64, shuffle=True, num_workers=4)\n",
    "        optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "\n",
    "        #####\n",
    "        def train():\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for pos_rw, neg_rw in loader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            return total_loss / len(loader)\n",
    "\n",
    "        pre_value_loss,curr_value_loss = 100,0\n",
    "        epoch = 0\n",
    "        while (abs(pre_value_loss - curr_value_loss) > tolerance):\n",
    "            loss = train()\n",
    "            epoch += 1\n",
    "            pre_value_loss = curr_value_loss\n",
    "            curr_value_loss = loss\n",
    "            #if epoch%5 == 0:\n",
    "            #    print(f'Epoch: {epoch:02d}, with loss: {loss:.4f}')\n",
    "        print(f'The Node2vec algorithm converged at epoch: {epoch:02d}, with loss: {loss:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "            ####\n",
    "\n",
    "        z = model()\n",
    "        # from tensor to numpy\n",
    "        emb_128 = z.detach().cpu().numpy()\n",
    "\n",
    "        edge_embedding = []\n",
    "        for u,v in data.edge_index.t():\n",
    "            edge_embedding.append(np.maximum(emb_128[u],emb_128[v]))\n",
    "        total_embeddings_temp = pd.DataFrame(edge_embedding)\n",
    "        total_embeddings_temp[\"Escuela\"] = datasets_edges[i][\"Escuela\"]\n",
    "        total_embeddings_temp[\"weight\"] = datasets_edges[i][\"weight\"]\n",
    "        total_embeddings_temp[\"class_classif\"] = datasets_edges[i][\"class_classif\"]\n",
    "        total_embeddings = pd.concat([total_embeddings,total_embeddings_temp],axis=0)\n",
    "        i += 1\n",
    "        \n",
    "        return total_embeddings\n",
    "    \n",
    "#total_embeddings.to_csv(\"total_embeddings_with_bf_p\"+str(model.p)+\"q\"+str(model.q)+\"_courses_ftest.csv\",index=\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cb3160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a probar a crear los embeddings por separado. Embeddings de train y embeddings de test\n",
    "\n",
    "#Tiene que ser una pipeline de un único archivo : \n",
    "\n",
    "# Dados los dataframes listos : \n",
    "\n",
    "    # * Define train y test acorde a un criterio \n",
    "    # * Crea los embeddings para cada uno \n",
    "    # * Entrena la red neuronal y obtiene un val_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bb423ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 0\n",
      "The Node2vec algorithm converged at epoch: 65, with loss: 0.9233\n",
      "Dataset: 0\n",
      "The Node2vec algorithm converged at epoch: 58, with loss: 0.8473\n"
     ]
    }
   ],
   "source": [
    "#####Define train y test split\n",
    "len_datasets = 13\n",
    "datasets_nodes = [0]*len_datasets\n",
    "datasets_edges = [0]*len_datasets\n",
    "datasets_edges_train = [0]*len_datasets\n",
    "datasets_edges_test = [0]*len_datasets\n",
    "graficas_train = [0]*len_datasets\n",
    "graficas_test = [0]*len_datasets\n",
    "data_list_train = [0]*len_datasets\n",
    "data_list_test = [0]*len_datasets\n",
    "data_list_train = [0]*len_datasets\n",
    "data_list_test = [0]*len_datasets\n",
    "graficas = [0]*len_datasets\n",
    "for i in range(0,len_datasets):\n",
    "    datasets_nodes[i] = pd.read_csv(r\"Coles/Nodes/Nodes_t\"+str(i+1)+\".csv\",sep=\",\",encoding = 'unicode_escape')\n",
    "    datasets_edges[i] = pd.read_csv(r\"Coles/Edges/Edges_t\"+str(i+1)+\".csv\",sep=\",\",encoding = 'unicode_escape')\n",
    "    #datasets_edges[i][[\"from\",\"to\"]] = datasets_edges[i][[\"from\",\"to\"]].apply(lambda x:x-min(datasets_edges[i][\"to\"].min(),datasets_edges[i][\"from\"].min()))    \n",
    "    datasets_edges[i][\"Escuela\"] = i\n",
    "    datasets_edges[i][\"weight\"] = datasets_edges[i][\"weight\"].apply(lambda x: np.sign(x)).replace({-1:0}).reset_index().drop(\"index\",axis=1)\n",
    "    datasets_edges[i],datasets_nodes[i] = rearrange_values(datasets_edges[i],datasets_nodes[i])\n",
    "    datasets_edges[i][\"class_classif\"] = f_edges(datasets_nodes[i],datasets_edges[i])\n",
    "    datasets_edges_train[i], datasets_edges_test[i] = train_test_split(datasets_edges[i], test_size=0.2)\n",
    "    graficas_train[i] = nx.from_pandas_edgelist(datasets_edges_train[i],source=\"from\",target=\"to\",create_using=nx.DiGraph())\n",
    "    graficas_test[i] = nx.from_pandas_edgelist(datasets_edges_test[i],source=\"from\",target=\"to\",create_using=nx.DiGraph())\n",
    "    data_list_train[i] = data.Data(edge_index = torch.tensor(datasets_edges_train[i][[\"from\",\"to\"]].to_numpy().T))\n",
    "    data_list_test[i] = data.Data(edge_index = torch.tensor(datasets_edges_test[i][[\"from\",\"to\"]].to_numpy().T))\n",
    "\n",
    "data_loader_train = DataLoader(data_list_train, batch_size=1)\n",
    "data_loader_test = DataLoader(data_list_test,batch_size = 1 )\n",
    "\n",
    "#Crea los embeddings para dicho train y test_split\n",
    "\n",
    "\n",
    "embeddings_train = create_embedding(data_loader_train)\n",
    "embeddings_test = create_embedding(data_loader_test)\n",
    "\n",
    "##Entrena la red neuronal para cada caso particular "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4a406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NN(embeddings_train,embeddings_test):\n",
    "\n",
    "    unique_courses = len(total_embeddings.loc[(total_embeddings[\"class_classif\"]!=\"Missing\")&(total_embeddings[\"class_classif\"]!=\"Intergroup\"),[\"Escuela\",\"class_classif\"]].value_counts())\n",
    "    unique_schools = total_embeddings[\"Escuela\"].nunique()\n",
    "    factor = 1\n",
    "    n_sim = unique_courses*factor #\n",
    "\n",
    "\n",
    "    acc_clf_auc = np.zeros((n_sim))\n",
    "    acc_ann_auc = np.zeros((n_sim))\n",
    "    for i in tqdm(range(n_sim)):\n",
    "        #tr_label = i%unique_schools\n",
    "        #X = total_embeddings[total_embeddings[\"Escuela\"] == tr_label].drop([\"Escuela\",\"weight\",'class_classif'],axis=1).values\n",
    "        #y = total_embeddings[total_embeddings[\"Escuela\"] == tr_label][\"weight\"].values\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        sc = MinMaxScaler()\n",
    "        sc.fit(X_train)\n",
    "        emb_x_train = sc.transform(X_train)\n",
    "        emb_y_train = y_train\n",
    "        emb_x_test = sc.transform(X_test)\n",
    "        emb_y_test = y_test\n",
    "        ros = SMOTE(random_state=0,sampling_strategy=\"minority\")\n",
    "        emb_x_resampled, emb_y_resampled = ros.fit_resample(emb_x_train, emb_y_train)\n",
    "        clf = RandomForestClassifier(max_depth=7,class_weight=\"balanced\")\n",
    "        clf.fit(emb_x_resampled,emb_y_resampled)\n",
    "        acc_clf_auc[int(i)] = roc_auc_score(emb_y_test,clf.predict(emb_x_test))\n",
    "        #######\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128,activation=\"relu\",input_shape=(emb_x_train.shape[1],)),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(64,activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(32,activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(8,activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=10e-5),\n",
    "                 loss=\"binary_crossentropy\",\n",
    "                     metrics=[\"AUC\"])\n",
    "        model_history = model.fit(emb_x_resampled,emb_y_resampled,epochs=250,verbose=0,batch_size=128,\n",
    "                                 #callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"auc\",patience=50,)])\n",
    "                                 )\n",
    "        #######º\n",
    "        acc_ann_auc[int(i)] = roc_auc_score(emb_y_test,model.predict(emb_x_test))\n",
    "        \n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
