{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6badee70",
   "metadata": {},
   "source": [
    "# High school network and deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c57a2",
   "metadata": {},
   "source": [
    "Our point is, considering the positive and negative networks as two different directed graphs, use a GConv network with previous autoencoder (Tutorial 12 Pytorch Geometric) to predict links from one of the networks, then compare the predicted_edges with the negative ones. Consider the positive as the training set and compare it to the negatives in the test one. \n",
    "\n",
    "* We are going to use a graph autoencoder, which is a non-supervised neural network that takes data, translate them to another representation (the one the neural network extracts from them) and then try to rebuild the original data. The representation it learns is based on the structure of the network.\n",
    "\n",
    "* We will use also a heuristic method, called PageRank method, traditionally used in link prediction, where the probability of a link depends on a variable called rank. \n",
    "\n",
    "* We compare it with a null method, a random graph. \n",
    "\n",
    "In both of the comparative methods of the GNN, negative links can only be placed where there are not positive links. \n",
    "\n",
    "The graph autoencoder generate a fixed number of links depending on built-in functions, so we are taking these number of links in order to establish comparison with other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c87626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57cccb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.read_csv(\"Renacimiento_info_completo_1.csv\",encoding=\"iso8859_7\",delimiter=\";\")\n",
    "edges = pd.read_csv(\"Renacimiento_edges_completo_1.csv\",encoding=\"iso8859_7\",delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d0b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[\"Curso\"] = nodes[\"Curso\"].apply(lambda x: x.split(\"ÎŠ\")[0])\n",
    "edges.drop(\"relacion\",axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b12305",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_range = dict(nodes[\"ID\"])\n",
    "clean_range = {value:key for key,value in clean_range.items()}\n",
    "nodes[\"ID\"] = nodes[\"ID\"].map(clean_range)\n",
    "edges[\"from\"] = edges[\"from\"].map(clean_range)\n",
    "edges[\"to\"] = edges[\"to\"].map(clean_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d206f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "pos_edges = edges[edges[\"peso\"]> 0]\n",
    "neg_edges = edges[edges[\"peso\"]< 0]\n",
    "G_positive = nx.from_pandas_edgelist(pos_edges, \"from\", \"to\",create_using=nx.DiGraph,edge_attr=\"peso\")\n",
    "G_negative = nx.from_pandas_edgelist(neg_edges, \"from\", \"to\",create_using=nx.DiGraph,edge_attr=\"peso\")\n",
    "G_negative.add_nodes_from(range(nodes.index.max()+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34705cd",
   "metadata": {},
   "source": [
    "### Define the complementary network of the positive network\n",
    "We will use it for choosing the edges in the different null models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7288d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_real = []\n",
    "for elem in pos_edges[[\"from\",\"to\"]].to_numpy():\n",
    "    edges_real.append(tuple(elem))\n",
    "chosen_edges = list(nx.complete_graph(238,create_using=nx.DiGraph()).edges())\n",
    "for elem in edges_real:\n",
    "    chosen_edges.remove(elem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d0bc34",
   "metadata": {},
   "source": [
    "## Graph autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a6a20",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1fc3d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "import torch\n",
    "import torch_geometric.data as data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import negative_sampling,train_test_split_edges,to_dense_adj\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from sklearn import preprocessing\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1795f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### One hot encode and normalize node attributes\n",
    "nodes_dummy = pd.get_dummies(nodes,['Curso', 'Grupo', 'Sexo', 'Procedencia', 'Repetidor'])\n",
    "rng = np.random.default_rng()\n",
    "#nodes_dummy = pd.DataFrame(rng.integers(0, 2, size=(409, 10)), columns=list('ABCDEFGHIJ'))\n",
    "\n",
    "x = nodes_dummy.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "nodes_norm = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5003aabe",
   "metadata": {},
   "source": [
    "### Firstly, check for isomorphism with Networkx \n",
    "\n",
    "Networkx has a isomorphism library that comes mainly from the VF2 algorithm : https://www.researchgate.net/publication/200034365_An_Improved_Algorithm_for_Matching_Large_Graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "629e2cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph of positive links is direcly isomorphic to the negative one ? False.\n"
     ]
    }
   ],
   "source": [
    "from networkx.algorithms import isomorphism\n",
    "\n",
    "\n",
    "DiGM = isomorphism.DiGraphMatcher(G_positive,G_negative)\n",
    "\n",
    "print(\"The graph of positive links is direcly isomorphic to the negative one ? {}.\".format(DiGM.is_isomorphic()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44e28034",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Without including class and group information \n",
    "positive_data = data.Data(x=torch.tensor(nodes_norm.to_numpy(),dtype=torch.float32),\n",
    "                          edge_index=torch.tensor(pos_edges[[\"from\",\"to\"]].to_numpy().T))\n",
    "negative_data = data.Data(x=torch.tensor(nodes_norm.to_numpy(),dtype=torch.float32),\n",
    "                          edge_index=torch.tensor(neg_edges[[\"from\",\"to\"]].to_numpy().T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b014e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[238, 36], edge_index=[2, 3397]),\n",
       " Data(x=[238, 36], edge_index=[2, 358]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_data,negative_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "079082e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/.local/lib/python3.8/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "data = positive_data.clone()\n",
    "data.num_nodes = len(data._store[\"x\"])\n",
    "data = train_test_split_edges(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d91946",
   "metadata": {},
   "source": [
    "### Models for the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaafd14",
   "metadata": {},
   "source": [
    "#### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f13d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(data.num_features, 128)\n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "\n",
    "    def encode(self):\n",
    "        x = self.conv1(data.x, data.train_pos_edge_index) # convolution 1\n",
    "        x = x.relu()\n",
    "        return self.conv2(x, data.train_pos_edge_index) # convolution 2\n",
    "\n",
    "    def decode(self, z, pos_edge_index, neg_edge_index): # only pos and neg edges\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1) # concatenate pos and neg edges\n",
    "        logits = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)  # dot product \n",
    "        return logits\n",
    "\n",
    "    def decode_all(self, z): \n",
    "        prob_adj = z @ z.t() # get adj NxN\n",
    "        return (prob_adj > 1-10e-10).nonzero(as_tuple=False).t() # get predicted edge_list "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2c32d",
   "metadata": {},
   "source": [
    "#### Set the parameters and move data to autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e9fb8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, positive_data = Net().to(device), positive_data.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df952510",
   "metadata": {},
   "source": [
    "#### Algorithms of training and evaluation (Tutorial PyG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "266bf880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    # returns a tensor:\n",
    "    # [1,1,1,1,...,0,0,0,0,0,..] with the number of ones is equel to the lenght of pos_edge_index\n",
    "    # and the number of zeros is equal to the length of neg_edge_index\n",
    "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(E, dtype=torch.float, device=device)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1.\n",
    "    return link_labels\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.train_pos_edge_index, #positive edges\n",
    "        num_nodes=data.num_nodes, # number of nodes\n",
    "        num_neg_samples=data.train_pos_edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    z = model.encode() #encode\n",
    "    link_logits = model.decode(z, data.train_pos_edge_index, neg_edge_index) # decode\n",
    "    \n",
    "    link_labels = get_link_labels(data.train_pos_edge_index, neg_edge_index)\n",
    "    loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    perfs = []\n",
    "    for prefix in [\"val\", \"test\"]:\n",
    "        pos_edge_index = data[f'{prefix}_pos_edge_index']\n",
    "        neg_edge_index = data[f'{prefix}_neg_edge_index']\n",
    "\n",
    "        z = model.encode() # encode train\n",
    "        link_logits = model.decode(z, pos_edge_index, neg_edge_index) # decode test or val\n",
    "        link_probs = link_logits.sigmoid() # apply sigmoid\n",
    "        \n",
    "        link_labels = get_link_labels(pos_edge_index, neg_edge_index) # get link\n",
    "        \n",
    "        perfs.append(roc_auc_score(link_labels.cpu(), link_probs.cpu())) #compute roc_auc score\n",
    "    return perfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205bf787",
   "metadata": {},
   "source": [
    "#### Training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "831973b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 0.3782, Val: 0.9710, Test: 0.9582\n",
      "Epoch: 200, Loss: 0.3900, Val: 0.9752, Test: 0.9565\n",
      "Epoch: 300, Loss: 0.3776, Val: 0.9768, Test: 0.9540\n",
      "Epoch: 400, Loss: 0.3855, Val: 0.9768, Test: 0.9540\n",
      "Epoch: 500, Loss: 0.3635, Val: 0.9768, Test: 0.9540\n",
      "Epoch: 600, Loss: 0.3790, Val: 0.9768, Test: 0.9540\n",
      "Epoch: 700, Loss: 0.3625, Val: 0.9768, Test: 0.9540\n",
      "Epoch: 800, Loss: 0.3712, Val: 0.9768, Test: 0.9540\n",
      "Epoch: 900, Loss: 0.3599, Val: 0.9768, Test: 0.9540\n",
      "Epoch: 1000, Loss: 0.3720, Val: 0.9768, Test: 0.9540\n",
      "Epoch: 1100, Loss: 0.3589, Val: 0.9768, Test: 0.9540\n",
      "Epoch: 1200, Loss: 0.3738, Val: 0.9768, Test: 0.9540\n",
      "Epoch: 1300, Loss: 0.3630, Val: 0.9768, Test: 0.9540\n",
      "Epoch: 1400, Loss: 0.3724, Val: 0.9768, Test: 0.9540\n",
      "Epoch: 1500, Loss: 0.3630, Val: 0.9768, Test: 0.9540\n",
      "Epoch: 1600, Loss: 0.3559, Val: 0.9768, Test: 0.9540\n",
      "Epoch: 1700, Loss: 0.3610, Val: 0.9768, Test: 0.9540\n",
      "Epoch: 1800, Loss: 0.3622, Val: 0.9768, Test: 0.9540\n",
      "Epoch: 1900, Loss: 0.3554, Val: 0.9768, Test: 0.9540\n",
      "Epoch: 2000, Loss: 0.3455, Val: 0.9768, Test: 0.9540\n"
     ]
    }
   ],
   "source": [
    "best_val_perf = test_perf = 0\n",
    "for epoch in range(1, 2001):\n",
    "    train_loss = train()\n",
    "    val_perf, tmp_test_perf = test()\n",
    "    if val_perf > best_val_perf:\n",
    "        best_val_perf = val_perf\n",
    "        test_perf = tmp_test_perf\n",
    "    log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    if epoch % 100 == 0:\n",
    "        print(log.format(epoch, train_loss, best_val_perf, test_perf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d13d7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.encode()\n",
    "final_edge_index_1 = model.decode_all(z)\n",
    "#Remove self loops\n",
    "bool_mask = final_edge_index_1[0] != final_edge_index_1[1]\n",
    "simulated_edges_1 = torch.empty((2,int(sum(bool_mask))))\n",
    "for item in range(final_edge_index_1.size()[0]):\n",
    "    simulated_edges_1[item] = final_edge_index_1[item][bool_mask]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61711fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_data_compare = list(zip(*negative_data[\"edge_index\"]))\n",
    "negative_data_compare = set(list(map(lambda item: (int(item[0]),int(item[1])),negative_data_compare)))\n",
    "\n",
    "simulated_edges_compare = list(zip(*simulated_edges_1))\n",
    "simulated_edges_compare = set(list(map(lambda item: (int(item[0]),int(item[1])),simulated_edges_compare)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30df67bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of links that were in the data is the 0.5 of the total\n"
     ]
    }
   ],
   "source": [
    "precision = round(len(negative_data_compare.intersection(simulated_edges_compare))/len(negative_data_compare),2)\n",
    "print(\"The amount of links that were in the data is the {} of the total\".format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cf998bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6742"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_class = list(range(len(nodes)))\n",
    "n_links = 0\n",
    "for item in simulated_edges_compare: \n",
    "    if item[0] in nodes_class: \n",
    "        n_links += 1\n",
    "    else:\n",
    "        print(item[0])\n",
    "n_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c931357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We generated 6742 links, 6742 of them are in the cut region \n",
      "We cut a total of 358 links\n",
      "From 6742, 179 of 358 were found in the simulated links. \n"
     ]
    }
   ],
   "source": [
    "print(f\"We generated {len(simulated_edges_compare)} links, {n_links} of them are in the cut region \")\n",
    "print(f\"We cut a total of {len(negative_data_compare)} links\")\n",
    "print(f\"From {n_links}, {int(precision*len(negative_data_compare))} of {len(negative_data_compare)} were found in the simulated links. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169cef43",
   "metadata": {},
   "source": [
    "### Link prediction with PageRank \n",
    "\n",
    "From the paper of _Alain Barrat_ Anxo recommended (_New Insights and Methods forPredicting Face-to-Face Contacts_), it can be checked the  _Hybrid Rooted PageRank_. We implement it in the following: \n",
    "\n",
    "*  With probability $\\alpha$ jump to root node _r_.\n",
    "*  With probability $1âˆ’\\alpha$:\n",
    "    *  Choose Network $N_{i}âˆˆN$ with respect toprobability distribution _P_.\n",
    "    *  If there exist no outgoing edges then :\n",
    "    * Jump to root node _r_\n",
    "    *  Else:\n",
    "        From the current node c jump to a neighbornselected with a probability $w(c,n)âˆ‘câ†’dw(c,d)$, i. e.,proportional to the weight $w(c,n)$ of the $e(c,n)$\n",
    "\n",
    "But we will include modifications on this analysis, as _Barrat et al_ use two networks in order to extract a single social network, while we are trying to deduce one from the other. We will implement PageRank on one of them and predict the links of the other one based on this quantity. When we are calculating link prediction, we calculate that the probability of the link is :\n",
    "\\begin{equation}\n",
    "  p_{link(i,j)} =\\dfrac{1}{1+e^{-(rank_{i}-rank_{j})}}\n",
    "\\end{equation}\n",
    "\n",
    "Considering that the rank ordering is a some kind of classification of dominance of the node, according to the original paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c176db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "def HR_pagerank(alpha,G):\n",
    "    N_rounds = 1000\n",
    "    rank = [0]*len(G.nodes())\n",
    "    for rounds in range(N_rounds):\n",
    "        for node in range(len(nodes)):\n",
    "            a = rd.uniform(0,1)\n",
    "            site = list(G.nodes())[node]\n",
    "            targets = list(G.nodes())\n",
    "            targets.remove(site)\n",
    "            if a > alpha:\n",
    "                target = rd.choice(targets)\n",
    "                if target in list(G.neighbors(site)):\n",
    "                    c = rd.uniform(0,1)\n",
    "                    weight_target = G[site][target][\"peso\"]\n",
    "                    weight=nx.get_edge_attributes(G,'peso')\n",
    "                    av_weights = 0\n",
    "                    for n in list(G.neighbors(site)):\n",
    "                        av_weights += weight[(site,n)]\n",
    "                    av_weights /= len(list(G.neighbors(site)))\n",
    "                    if c<((weight_target)/(av_weights)):\n",
    "                        site = target\n",
    "                        rank[site] +=1\n",
    "    rank = [item/N_rounds for item in rank]\n",
    "    return rank\n",
    "\n",
    "def create_link(G,rank,chosen_edges):\n",
    "    index_pair = rd.choice(chosen_edges)\n",
    "    rd_pair = [rank[item] for item in index_pair]\n",
    "    p_rank = 1/(1 + np.exp(-(rd_pair[0]-rd_pair[1])))\n",
    "    if (rd.uniform(0,1) < p_rank) : \n",
    "        G.add_edge(index_pair[0],index_pair[1])\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8e4d1ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_81385/3759499407.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpositive_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHR_pagerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG_positive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#positive_rank = nx.algorithms.pagerank(G_positive,0.15)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mG_simulated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_simulated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m \u001b[0mfinal_edge_index_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcreate_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_simulated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpositive_rank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchosen_edges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_81385/1073979856.py\u001b[0m in \u001b[0;36mHR_pagerank\u001b[0;34m(alpha, G)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0msite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "positive_rank = HR_pagerank(0.15,G_positive)\n",
    "#positive_rank = nx.algorithms.pagerank(G_positive,0.15)\n",
    "G_simulated = nx.DiGraph()\n",
    "while len(G_simulated.edges())< final_edge_index_1.size()[1]:\n",
    "    create_link(G_simulated,positive_rank,chosen_edges)\n",
    "\n",
    "coincidences = to_dense_adj(negative_data[\"edge_index\"]).squeeze()*torch.tensor(nx.adjacency_matrix(G_simulated).todense())\n",
    "coin_rank_pos = coincidences.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de98345",
   "metadata": {},
   "source": [
    "### Randomly created network\n",
    "\n",
    "We compare the results from the GNN and the PageRank with a randomly created network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd \n",
    "coincidences_total = 0\n",
    "for sim in range(10):\n",
    "    chosen_edges_2 = chosen_edges.copy()\n",
    "    G_random = nx.DiGraph()\n",
    "    G_random.add_nodes_from(range(len(nodes)))\n",
    "    for trial in range(final_edge_index_1.size()[1]):\n",
    "        rd_sample = rd.choice(chosen_edges_2)\n",
    "        G_random.add_edge(rd_sample[0],rd_sample[1]) \n",
    "        chosen_edges_2.remove(rd_sample)\n",
    "    coincidences_random = len([(u,v) for (u,v) in G_random.edges() if G_negative.has_edge(u,v)])\n",
    "    coincidences_total += coincidences_random\n",
    "coin_random = coincidences_total/10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a41c1fd",
   "metadata": {},
   "source": [
    "Proportions between the random network score and the amount of edges chosen divided by the total number of edges are similar, so the random network finds a correct proportion of the real links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db8fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_random/len(neg_edges),final_edge_index_1.size()[1]/(len(nodes)*(len(nodes)-1)-len(pos_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e9c8ee",
   "metadata": {},
   "source": [
    "**Work to be done** \n",
    "\n",
    "1) Check the structural balance theory, computing global equilibria in both networks, in order to generate ensembles. \n",
    "\n",
    "2) Graph neural networks can also be used to predict labeling in edges, it could be used to proof structural balance theory from other perspective. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bc1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_saved_compare = list(G_random.edges())\n",
    "edges_saved_compare = set(list(map(lambda item: (int(item[0]),int(item[1])),edges_saved_compare)))\n",
    "\n",
    "simulated_edges_compare = list(zip(*simulated_edges_1))\n",
    "simulated_edges_compare = set(list(map(lambda item: (int(item[0]),int(item[1])),simulated_edges_compare)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd092e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = round(len(edges_saved_compare.intersection(simulated_edges_compare))/len(simulated_edges_compare),2)\n",
    "print(\"The amount of links that were in the data is the {} of the total\".format(precision))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
